import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.datasets import imdb
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import train_test_split
from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

from transformers import BertTokenizer
from transformers import TFBertForSequenceClassification

# preprocessing
(X_train, y_train),(X_test, y_test) = imdb.load_data()
index_to_word = {}
for word, index in imdb.get_word_index().items():
    index_to_word[index+3] = word

for index, word in enumerate(['', '', '']):
    index_to_word[index] = word

def mk_shape(lists):
    try:
        sum(lists)
        return [lists]
    except TypeError:
        return lists

X_train = mk_shape(X_train)
X_test = mk_shape(X_test)

def return_sent(text):
    sent_list = []
    for i in text:
        sent_list.append(index_to_word[i])
    return ' '.join(sent_list)

#불용어 처리가 되어있지 않은 원문

def raw_review(text):
    raw_reviews = []
    try:
        for sents in text:
            raw_review = []
            rs = return_sent(sents)
            raw_review.append(rs)
            raw_reviews.append(raw_review)
    #index number 88585 missing
    except KeyError:
        pass
    return raw_reviews

reviews_imdb_tr = raw_review(X_train)
reviews_imdb_te = raw_review(X_test)

def remove_stop_word(reviews):
    stops = set(stopwords.words('english'))
    non_stops = []
    for review in reviews:
        text = str(review[0]).split()
        text = [word for word in text if not word in stops]
        sent = ' '.join(text)
        non_stops.append(sent)
    return non_stops

non_stop_imdb_tr = remove_stop_word(reviews_imdb_tr)
non_stop_imdb_te = remove_stop_word(reviews_imdb_te)

def remove_punctuation(non_stops):
    result = []
    tokenizer = RegexpTokenizer(r'[a-zA-z]+')
    for non_stop in non_stops:
        non_sp_sent = tokenizer.tokenize(non_stop)
        non_sp_sent = " ".join(non_sp_sent)
        result.append(non_sp_sent)
    return result

non_punc_imdb_tr = remove_punctuation(non_stop_imdb_tr)
non_punc_imdb_te = remove_punctuation(non_stop_imdb_te)

def letter_lower(non_capl):
    result = []
    for sent in non_capl:
        result.append(sent.lower())
    return result
X_train_f = remove_punctuation(non_punc_imdb_tr)
X_test_f = remove_punctuation(non_punc_imdb_te)

## Tokenize
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

X_train_tok = []
for sent in X_train_f:
    t = bert_tokenizer.encode_plus(sent, add_special_tokens=True,max_length=512, padding='longest', return_attention_mask=True)
    X_train_tok.append(t)

X_test_tok = []
for sent in X_test_f:
    t = bert_tokenizer.encode_plus(sent, add_special_tokens=True,max_length=512, padding='longest', return_attention_mask=True)
    X_test_tok.append(t)

X_tr_input_ids = []
X_tr_attention_mask = []
for sent in X_train_tok:
    X_tr_input_ids.append(sent['input_ids'])
    X_tr_attention_mask.append(sent['attention_mask'])

X_te_input_ids = []
X_te_attention_mask = []
for sent in X_test_tok:
    X_te_input_ids.append(sent['input_ids'])
    X_te_attention_mask.append(sent['attention_mask'])

X_tr_input_ids = np.asarray(X_tr_input_ids)
X_tr_attention_mask = np.asarray(X_tr_attention_mask)
