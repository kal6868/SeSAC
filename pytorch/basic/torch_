{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[torch]신경망 구성_정답.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### super()로 기반 클래스 초기화"
      ],
      "metadata": {
        "id": "5N4o72aSPm9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4E-zbrOPmJt",
        "outputId": "0bf6bc3c-bfd0-43d1-919a-b846e7ae41e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "서울 소프트웨어 아카데미\n"
          ]
        }
      ],
      "source": [
        "class Person:\n",
        "    def __init__(self):\n",
        "      self.hello = '안녕하세요'\n",
        "\n",
        "class Student(Person):\n",
        "    def __init__(self):\n",
        "      self.school = '서울 소프트웨어 아카데미'\n",
        "\n",
        "dp = Student()\n",
        "print(dp.school)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Person:\n",
        "    def __init__(self):\n",
        "      self.hello = '안녕하세요'\n",
        "\n",
        "class Student(Person):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.school = '서울 소프트웨어 아카데미'\n",
        "\n",
        "dp = Student()\n",
        "print(dp.school)\n",
        "print(dp.hello)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XYkpy7HQXT1",
        "outputId": "295367ab-6c83-420f-e858-39591f265d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "서울 소프트웨어 아카데미\n",
            "안녕하세요\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 클래스를 초기화하지 않아도 되는 경우"
      ],
      "metadata": {
        "id": "FEyIW3LHSC-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#상속받은 클래스에 __init__ 함수가 없으면 super()는 생략가능함, __init__ 함수는 한번만 실행되기 때문"
      ],
      "metadata": {
        "id": "PH4oRtg9RcQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Person:\n",
        "  def __init__(self):\n",
        "    self.hello = '안녕하세요'\n",
        "\n",
        "class Student(Person):\n",
        "  pass\n",
        "\n",
        "dp = Student()\n",
        "print(dp.hello)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF6jeb9HR8ri",
        "outputId": "0c74f66b-159a-48ab-e067-1d2c64257e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인공 신경망"
      ],
      "metadata": {
        "id": "k83X-DOjSaIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 셋 다운로드"
      ],
      "metadata": {
        "id": "mLVrSXyFSaGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache-dir gdown --pre\n",
        "print(\"Downloading...\")\n",
        "!gdown --id {\"1NDg2_ccmxkWDu2e_2hsngYI5pzY4qWkW\"} -O reg.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwQAxAyxSVcf",
        "outputId": "6946d762-f77f-4e74-bd79-24ae6156c60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NDg2_ccmxkWDu2e_2hsngYI5pzY4qWkW\n",
            "To: /content/reg.csv\n",
            "100% 138k/138k [00:00<00:00, 53.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#!pip install torchsummary\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "pS_ph6qASy2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Regressor(nn.Module):\n",
        "\n",
        "  #사용한 층들을 정의\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(13, 50)\n",
        "    self.fc2 = nn.Linear(50, 30)\n",
        "    self.fc3 = nn.Linear(30, 1)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = F.relu(self.fc3(x))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "7DzEDTrCTlqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Regressor()"
      ],
      "metadata": {
        "id": "YAob2XOOVbmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joFsvJ0pVde8",
        "outputId": "d7faace3-1a90-4b77-b1e4-fd82ff56a5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regressor(\n",
            "  (fc1): Linear(in_features=13, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=30, bias=True)\n",
            "  (fc3): Linear(in_features=30, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkMYGA2AVvpR",
        "outputId": "c46ec28b-46d4-492f-c99d-a6425112d64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 13])\n",
            "torch.Size([50])\n",
            "torch.Size([30, 50])\n",
            "torch.Size([30])\n",
            "torch.Size([1, 30])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, ':',param.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7qoIwmZV_1h",
        "outputId": "1adbd3a9-68f7-4666-b157-75c9fde36cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight : torch.Size([50, 13])\n",
            "fc1.bias : torch.Size([50])\n",
            "fc2.weight : torch.Size([30, 50])\n",
            "fc2.bias : torch.Size([30])\n",
            "fc3.weight : torch.Size([1, 30])\n",
            "fc3.bias : torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgKoj55KWdHI",
        "outputId": "bc116843-0a97-4535-d04d-48feb226258e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.2463, -0.1579, -0.1209, -0.2504,  0.0633, -0.1008, -0.2177,  0.1293,\n",
              "         -0.1254, -0.0541, -0.2319,  0.1380, -0.2518],\n",
              "        [-0.1855, -0.2718, -0.1308,  0.0639, -0.0909, -0.0118, -0.2295,  0.2536,\n",
              "          0.2749,  0.2167,  0.0294,  0.2134, -0.1419],\n",
              "        [ 0.0143, -0.1546,  0.1869, -0.1772,  0.0127,  0.0843, -0.1287, -0.0585,\n",
              "          0.0159,  0.0716, -0.2051, -0.0878, -0.1890],\n",
              "        [-0.1560,  0.0370,  0.0710, -0.0104,  0.2393, -0.2350,  0.0829,  0.2309,\n",
              "         -0.2042,  0.0640,  0.0395,  0.1049, -0.0519],\n",
              "        [-0.0771,  0.0511, -0.0078, -0.2246,  0.1171, -0.0874,  0.0037,  0.1415,\n",
              "         -0.1945, -0.0757,  0.1255, -0.2100,  0.2295],\n",
              "        [-0.1326, -0.2242, -0.2175,  0.1553,  0.1847,  0.1355, -0.0021, -0.1481,\n",
              "          0.2431, -0.2773,  0.1339,  0.0539, -0.0950],\n",
              "        [ 0.2596, -0.2703,  0.0291,  0.0526,  0.0236,  0.0787,  0.1283, -0.0100,\n",
              "         -0.1723, -0.1354, -0.1264, -0.0424,  0.1587],\n",
              "        [ 0.0323,  0.1887, -0.1381, -0.1476,  0.0808,  0.2187,  0.1500, -0.1876,\n",
              "         -0.2315, -0.1404, -0.1366,  0.2118, -0.0920],\n",
              "        [ 0.1965, -0.0530, -0.0476, -0.1518, -0.2333, -0.0620,  0.2765,  0.2470,\n",
              "         -0.0044, -0.1793, -0.0971, -0.2672, -0.2358],\n",
              "        [-0.2563, -0.2269,  0.2047,  0.1380, -0.0141,  0.0287, -0.1404,  0.0897,\n",
              "          0.2193,  0.0158, -0.0274, -0.1802, -0.1149],\n",
              "        [-0.2745,  0.0380,  0.1048, -0.0286, -0.0757,  0.0387, -0.2702,  0.2469,\n",
              "         -0.0040,  0.2123,  0.1521,  0.2502, -0.0157],\n",
              "        [-0.2217, -0.1664, -0.1952,  0.0577,  0.1435,  0.2033,  0.2178, -0.1409,\n",
              "         -0.1389,  0.2624, -0.2136, -0.1537,  0.1848],\n",
              "        [ 0.0470,  0.2642,  0.0305, -0.0423, -0.2541,  0.0488,  0.1345,  0.2437,\n",
              "          0.0976, -0.0880,  0.2131, -0.2146,  0.1556],\n",
              "        [-0.2064,  0.1929,  0.1530,  0.0883,  0.0309, -0.2246, -0.2561, -0.0656,\n",
              "         -0.1182,  0.1902, -0.2566,  0.0810,  0.0585],\n",
              "        [ 0.0925,  0.2289,  0.0115,  0.1068, -0.0773, -0.1000, -0.0131, -0.1939,\n",
              "          0.0043,  0.1413,  0.1724, -0.2615,  0.0919],\n",
              "        [-0.0546,  0.0213,  0.1952,  0.1669, -0.0583, -0.0770, -0.2122,  0.1714,\n",
              "          0.0076,  0.1668,  0.1390,  0.0009, -0.1131],\n",
              "        [ 0.2638,  0.1008,  0.2248,  0.2478,  0.0741, -0.0750,  0.0420,  0.0039,\n",
              "          0.1742, -0.0454, -0.1713, -0.2173, -0.1886],\n",
              "        [-0.2014,  0.1120, -0.1424,  0.2557,  0.0029, -0.0970, -0.2258,  0.1943,\n",
              "          0.1445, -0.1365,  0.2108,  0.2305, -0.2302],\n",
              "        [ 0.1623,  0.0339,  0.2442, -0.2201, -0.1900,  0.1889,  0.2229,  0.1100,\n",
              "         -0.0024,  0.2214, -0.1476, -0.0489,  0.0139],\n",
              "        [ 0.0471,  0.0617, -0.0784,  0.0625, -0.0648,  0.2679, -0.1816,  0.2630,\n",
              "         -0.0316,  0.0946, -0.1233,  0.2584, -0.1989],\n",
              "        [-0.2682,  0.2096,  0.1967, -0.0477, -0.2475,  0.1459, -0.0042, -0.0676,\n",
              "         -0.1920,  0.1850, -0.0104,  0.2708,  0.1053],\n",
              "        [ 0.2752, -0.0614,  0.1129,  0.1296,  0.1220,  0.2539,  0.0256, -0.0875,\n",
              "          0.0559,  0.0142, -0.0498, -0.0177,  0.2227],\n",
              "        [ 0.2284, -0.1511,  0.1242, -0.2191,  0.0489, -0.1119,  0.2511,  0.1114,\n",
              "          0.0977, -0.0584, -0.1480,  0.2765,  0.2348],\n",
              "        [-0.2250,  0.0441,  0.2035,  0.1343,  0.1913,  0.2450, -0.2500,  0.2349,\n",
              "          0.0857,  0.2641, -0.2561, -0.2053,  0.2360],\n",
              "        [-0.0463, -0.2378,  0.0119, -0.1009, -0.0872, -0.1491, -0.2246,  0.1125,\n",
              "         -0.0006,  0.2405,  0.1587,  0.0520,  0.1924],\n",
              "        [ 0.2242,  0.2486,  0.0875,  0.2558, -0.1898,  0.0639, -0.1591,  0.0836,\n",
              "         -0.1233, -0.1004, -0.0835, -0.2644,  0.1288],\n",
              "        [ 0.1956,  0.1638, -0.0442,  0.1329, -0.1489,  0.0881, -0.1960,  0.0661,\n",
              "          0.0451, -0.2741,  0.2450,  0.2128,  0.0849],\n",
              "        [ 0.0064, -0.1661,  0.2082,  0.2486,  0.1439, -0.0798,  0.2413, -0.0443,\n",
              "          0.2490,  0.1463,  0.0968,  0.1482,  0.1785],\n",
              "        [ 0.0162,  0.2320,  0.2189, -0.2322,  0.2732,  0.0555,  0.1021,  0.0726,\n",
              "         -0.0620,  0.1007, -0.0558,  0.1863,  0.2536],\n",
              "        [ 0.0412, -0.1902, -0.2757,  0.2334, -0.2153,  0.1537, -0.0226, -0.2623,\n",
              "         -0.0864,  0.2480, -0.0571,  0.0055, -0.1132],\n",
              "        [-0.0420, -0.0918,  0.0483, -0.1727, -0.0101, -0.0224, -0.0510, -0.1222,\n",
              "          0.1439, -0.2396,  0.2593,  0.2558, -0.2668],\n",
              "        [ 0.0043, -0.0507, -0.0147, -0.1021,  0.0222, -0.2306,  0.0702,  0.2658,\n",
              "          0.0463,  0.0402, -0.2303, -0.0114, -0.2002],\n",
              "        [ 0.1375, -0.0404, -0.0260,  0.0599, -0.2106,  0.2218, -0.1261, -0.1510,\n",
              "          0.2289,  0.1852, -0.0080, -0.2702,  0.0359],\n",
              "        [-0.0710, -0.0411, -0.0926,  0.1151,  0.0999, -0.0566, -0.2070,  0.1847,\n",
              "         -0.2428,  0.1308, -0.0914,  0.0573, -0.2070],\n",
              "        [-0.1114, -0.0995, -0.0631, -0.2210, -0.1052, -0.1356, -0.0507,  0.1183,\n",
              "          0.0822, -0.0690, -0.2232,  0.0907,  0.0510],\n",
              "        [ 0.0051,  0.1075, -0.2353,  0.0756,  0.0235,  0.0542, -0.0906, -0.1569,\n",
              "          0.0982,  0.0990,  0.2606,  0.1653,  0.2679],\n",
              "        [-0.0723, -0.2119,  0.2663,  0.0139, -0.0804, -0.1561, -0.0837,  0.0474,\n",
              "          0.0912,  0.0766, -0.2478,  0.1807, -0.1139],\n",
              "        [-0.1800, -0.1236, -0.0482, -0.0574, -0.0537,  0.1206,  0.0857,  0.2003,\n",
              "         -0.1275, -0.2745, -0.2077,  0.1677, -0.0327],\n",
              "        [ 0.0560,  0.2650, -0.0431, -0.1577,  0.0598,  0.2720, -0.0652,  0.1955,\n",
              "          0.1167, -0.1354,  0.2635,  0.2015, -0.0027],\n",
              "        [-0.0755,  0.2124, -0.1998, -0.0131, -0.2752,  0.2753,  0.2538,  0.2383,\n",
              "         -0.2215, -0.0175, -0.0932, -0.1278, -0.0052],\n",
              "        [ 0.2702, -0.2654, -0.1366, -0.2011, -0.2555, -0.0461, -0.0531,  0.1390,\n",
              "          0.0095,  0.0596,  0.2577,  0.1007,  0.0710],\n",
              "        [ 0.2045,  0.1759, -0.1837, -0.2067, -0.1609,  0.1819, -0.2656, -0.1203,\n",
              "         -0.1851, -0.1221, -0.0349, -0.2660,  0.0135],\n",
              "        [ 0.1273, -0.1331,  0.0568, -0.1827,  0.0236,  0.0783, -0.2134, -0.0146,\n",
              "         -0.2361, -0.2422, -0.1015, -0.1585,  0.2310],\n",
              "        [ 0.2221,  0.1443,  0.2738,  0.2241,  0.2345,  0.1061,  0.1984, -0.1934,\n",
              "         -0.2276, -0.2406,  0.1429,  0.2679,  0.2683],\n",
              "        [ 0.2551,  0.2364, -0.2575,  0.2653,  0.0510,  0.2094, -0.0046, -0.2573,\n",
              "         -0.0066, -0.0233, -0.1561,  0.1853,  0.2022],\n",
              "        [ 0.1656,  0.1875, -0.2721, -0.1192,  0.2462,  0.2587, -0.0119, -0.0759,\n",
              "          0.1617, -0.1098,  0.1481, -0.1590,  0.1817],\n",
              "        [ 0.0750,  0.0355,  0.0908, -0.1630, -0.1081, -0.2567, -0.2768, -0.1176,\n",
              "         -0.1945,  0.1059, -0.1008, -0.0266, -0.2590],\n",
              "        [-0.1891, -0.2683, -0.0537, -0.1327, -0.1425,  0.1673,  0.2350, -0.0663,\n",
              "         -0.1272,  0.0688, -0.2230, -0.1595,  0.0815],\n",
              "        [-0.0844, -0.2094,  0.0827, -0.1585,  0.1028, -0.1579,  0.0133,  0.0813,\n",
              "         -0.0963, -0.0054, -0.1547,  0.2481, -0.0665],\n",
              "        [-0.1654, -0.1568, -0.1690, -0.0370,  0.2180, -0.0290, -0.0226,  0.2561,\n",
              "          0.1348, -0.1318,  0.0336,  0.2052, -0.2692]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hocfGzcwW7Wp",
        "outputId": "5ad7a362-8278-41f9-8690-17c768f86021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-2.2233e-01, -1.6600e-01, -6.5809e-02, -1.6955e-01,  1.0783e-01,\n",
              "        -4.3968e-02, -3.7570e-02, -1.7260e-01,  1.6121e-01,  1.0077e-01,\n",
              "         1.0412e-01, -2.0343e-01, -1.0680e-01, -1.6583e-01,  1.3365e-01,\n",
              "         2.2284e-01,  2.7268e-01,  4.0170e-02,  1.1130e-01, -2.4735e-01,\n",
              "        -2.6200e-01,  6.5449e-02,  1.9589e-01,  2.4584e-01, -2.2654e-02,\n",
              "         1.0866e-01,  3.5362e-02,  1.1582e-01, -2.1478e-04,  5.6454e-02,\n",
              "        -7.0718e-02,  7.5547e-02, -1.9025e-02, -1.3505e-01, -1.7986e-01,\n",
              "        -2.6912e-02,  1.2675e-01, -2.1759e-01,  2.7380e-01,  2.5231e-01,\n",
              "         2.3548e-01,  2.7265e-01,  9.5892e-02,  2.2457e-01, -5.0343e-02,\n",
              "        -1.5614e-01,  1.0345e-01,  1.2724e-01,  2.0949e-01, -1.1224e-01],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (10, 13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfsuZ-mzXFj9",
        "outputId": "2dd37b41-cd37-4daf-e0f5-2af807693cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 10, 50]             700\n",
            "            Linear-2               [-1, 10, 30]           1,530\n",
            "           Dropout-3               [-1, 10, 30]               0\n",
            "            Linear-4                [-1, 10, 1]              31\n",
            "================================================================\n",
            "Total params: 2,261\n",
            "Trainable params: 2,261\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.02\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 교차 검증"
      ],
      "metadata": {
        "id": "QNtVChAXXt0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # 데이터프레임 형태를 다룰 수 있는 라이브러리\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split # 전체 데이터를 학습 데이터와 평가 데이터로 나눈다.\n",
        "\n",
        "# ANN\n",
        "import torch\n",
        "from torch import nn, optim # torch 내의 세부적인 기능을 불러온다. (신경망 기술, 손실함수, 최적화 방법 등)\n",
        "from torch.utils.data import DataLoader, Dataset # 데이터를 모델에 사용할 수 있도록 정리해 주는 라이브러리\n",
        "import torch.nn.functional as F # torch 내의 세부적인 기능을 불러온다. (신경망 기술 등)\n",
        "\n",
        "# Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Loss\n",
        "from sklearn.metrics import mean_squared_error # Regression 문제의 평가를 위해 MSE(Mean Squared Error)를 불러온다.\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt # 시각화 도구"
      ],
      "metadata": {
        "id": "E4tjKBPOXaOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./reg.csv', index_col=[0])"
      ],
      "metadata": {
        "id": "d86Yt-jiZqU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터를 넘파이 배열로 만들기\n",
        "X = df.drop('Price', axis=1).to_numpy() #데이터프레임에서 타겟값(price)를 제외하고 넘파이 배열로 만들기\n",
        "Y = df['Price'].to_numpy().reshape((-1,1)) # 데이터프레임 형태의 타겟값을 넘파이 배열로 만들기"
      ],
      "metadata": {
        "id": "Ry1n18yudks_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUSSRmPZd-qv",
        "outputId": "1bfd016c-4e83-4f01-a62b-b614a4fdd2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 13) (506, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorData(Dataset):\n",
        "  def __init__(self,x_data,y_data):\n",
        "    self.x_data = torch.FloatTensor(x_data)\n",
        "    self.y_data = torch.FloatTensor(y_data)\n",
        "    self.len = self.y_data.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "Heyjm82YeDEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.7)\n",
        "trainset = TensorData(X_train, Y_train)\n",
        "testset = TensorData(X_test, Y_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "oF6yzdzFeHTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Regressor(nn.Module):\n",
        "\n",
        "  #사용한 층들을 정의\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(13, 50)\n",
        "    self.fc2 = nn.Linear(50, 30)\n",
        "    self.fc3 = nn.Linear(30, 1)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = F.relu(self.fc3(x))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "W_uKIvujfuLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=3, shuffle=True)"
      ],
      "metadata": {
        "id": "28giBM0TgcHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "EAgmFMVugx1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(dataloader):\n",
        "    \n",
        "    predictions = torch.tensor([], dtype=torch.float) # 예측값을 저장하는 텐서\n",
        "    actual = torch.tensor([], dtype=torch.float) # 실제값을 저장하는 텐서\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        model.eval() # 평가를 할 때에는 .eval() 반드시 사용해야 한다.\n",
        "        for data in dataloader:\n",
        "            inputs, values = data\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            predictions = torch.cat((predictions, outputs), 0) # cat을 통해 예측값을 누적\n",
        "            actual = torch.cat((actual, values), 0) # cat을 통해 실제값을 누적\n",
        "    \n",
        "    predictions = predictions.numpy() # 넘파이 배열로 변경\n",
        "    actual = actual.numpy() # 넘파이 배열로 변경\n",
        "    rmse = np.sqrt(mean_squared_error(predictions, actual)) # sklearn을 이용하여 RMSE 계산\n",
        "    model.train()\n",
        "    return rmse  \n",
        "\n",
        "# 평가 시 .eval()을 사용해야 하는 이유\n",
        "# 이번 예시에서는 상관없으나 평가 시에는 정규화 기술을 배제하여 온전한 모델로 평가를 해야한다. 따라서 .eval()을 사용한다.\n",
        "# 즉, 드랍아웃이나 배치 정규화 등과 같이 학습 시에만 사용하는 기술들이 적용 된 모델은 평가 시에는 비활성화 해야하며 학습 시 .train()을 사용한다."
      ],
      "metadata": {
        "id": "yiMJAbaPg2U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_loss = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(trainset)):\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx) # index 생성\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx) # index 생성\n",
        "\n",
        "    # sampler를 이용한 DataLoader 정의\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, \n",
        "                                              sampler=train_subsampler) \n",
        "    valloader = torch.utils.data.DataLoader(trainset, batch_size=32, \n",
        "                                            sampler=val_subsampler)\n",
        "    \n",
        "    # 모델\n",
        "    model = Regressor()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
        "\n",
        "    for epoch in range(400): # 400번 학습을 진행한다.\n",
        "        for data in trainloader: # 무작위로 섞인 32개 데이터가 있는 배치가 하나 씩 들어온다.\n",
        "            inputs, values = data # data에는 X, Y가 들어있다.\n",
        "            optimizer.zero_grad() # 최적화 초기화\n",
        "\n",
        "            outputs = model(inputs) # 모델에 입력값 대입 후 예측값 산출\n",
        "            loss = criterion(outputs, values) # 손실 함수 계산\n",
        "            loss.backward() # 손실 함수 기준으로 역전파 설정 \n",
        "\n",
        "            optimizer.step() # 역전파를 진행하고 가중치 업데이트\n",
        "\n",
        "            train_rmse = evaluation(trainloader) # 학습 데이터의 RMSE\n",
        "            val_rmse = evaluation(valloader)\n",
        "            print(\"k-fold\", fold,\" Train Loss: %.4f, Validation Loss: %.4f\" %(train_rmse, val_rmse)) \n",
        "            validation_loss.append(val_rmse)\n",
        "\n",
        "validation_loss = np.array(validation_loss)\n",
        "mean = np.mean(validation_loss)\n",
        "std = np.std(validation_loss)\n",
        "print(\"Validation Score: %.4f, ± %.4f\" %(mean, std))     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfUznDXTijCI",
        "outputId": "bd6559d2-17a7-4fcb-9b94-a95f48047d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k-fold 0  Train Loss: 0.4326, Validation Loss: 0.4211\n",
            "k-fold 0  Train Loss: 0.4326, Validation Loss: 0.4211\n",
            "k-fold 0  Train Loss: 0.4326, Validation Loss: 0.4211\n",
            "k-fold 0  Train Loss: 0.4326, Validation Loss: 0.4211\n",
            "k-fold 0  Train Loss: 0.4326, Validation Loss: 0.4211\n",
            "k-fold 0  Train Loss: 0.4326, Validation Loss: 0.4211\n",
            "k-fold 0  Train Loss: 0.4315, Validation Loss: 0.4205\n",
            "k-fold 0  Train Loss: 0.4240, Validation Loss: 0.4152\n",
            "k-fold 0  Train Loss: 0.4120, Validation Loss: 0.4049\n",
            "k-fold 0  Train Loss: 0.3973, Validation Loss: 0.3906\n",
            "k-fold 0  Train Loss: 0.3793, Validation Loss: 0.3728\n",
            "k-fold 0  Train Loss: 0.3610, Validation Loss: 0.3549\n",
            "k-fold 0  Train Loss: 0.3425, Validation Loss: 0.3366\n",
            "k-fold 0  Train Loss: 0.3235, Validation Loss: 0.3183\n",
            "k-fold 0  Train Loss: 0.3048, Validation Loss: 0.3003\n",
            "k-fold 0  Train Loss: 0.2869, Validation Loss: 0.2834\n",
            "k-fold 0  Train Loss: 0.2696, Validation Loss: 0.2671\n",
            "k-fold 0  Train Loss: 0.2535, Validation Loss: 0.2522\n",
            "k-fold 0  Train Loss: 0.2386, Validation Loss: 0.2387\n",
            "k-fold 0  Train Loss: 0.2250, Validation Loss: 0.2266\n",
            "k-fold 0  Train Loss: 0.2141, Validation Loss: 0.2173\n",
            "k-fold 0  Train Loss: 0.2053, Validation Loss: 0.2102\n",
            "k-fold 0  Train Loss: 0.1986, Validation Loss: 0.2054\n",
            "k-fold 0  Train Loss: 0.1944, Validation Loss: 0.2028\n",
            "k-fold 0  Train Loss: 0.1920, Validation Loss: 0.2019\n",
            "k-fold 0  Train Loss: 0.1910, Validation Loss: 0.2020\n",
            "k-fold 0  Train Loss: 0.1908, Validation Loss: 0.2028\n",
            "k-fold 0  Train Loss: 0.1909, Validation Loss: 0.2033\n",
            "k-fold 0  Train Loss: 0.1907, Validation Loss: 0.2033\n",
            "k-fold 0  Train Loss: 0.1902, Validation Loss: 0.2026\n",
            "k-fold 0  Train Loss: 0.1896, Validation Loss: 0.2018\n",
            "k-fold 0  Train Loss: 0.1886, Validation Loss: 0.2004\n",
            "k-fold 0  Train Loss: 0.1877, Validation Loss: 0.1991\n",
            "k-fold 0  Train Loss: 0.1868, Validation Loss: 0.1976\n",
            "k-fold 0  Train Loss: 0.1861, Validation Loss: 0.1962\n",
            "k-fold 0  Train Loss: 0.1857, Validation Loss: 0.1951\n",
            "k-fold 0  Train Loss: 0.1855, Validation Loss: 0.1943\n",
            "k-fold 0  Train Loss: 0.1858, Validation Loss: 0.1939\n",
            "k-fold 0  Train Loss: 0.1863, Validation Loss: 0.1938\n",
            "k-fold 0  Train Loss: 0.1871, Validation Loss: 0.1940\n",
            "k-fold 0  Train Loss: 0.1880, Validation Loss: 0.1943\n",
            "k-fold 0  Train Loss: 0.1887, Validation Loss: 0.1946\n",
            "k-fold 0  Train Loss: 0.1897, Validation Loss: 0.1951\n",
            "k-fold 0  Train Loss: 0.1913, Validation Loss: 0.1963\n",
            "k-fold 0  Train Loss: 0.1926, Validation Loss: 0.1973\n",
            "k-fold 0  Train Loss: 0.1938, Validation Loss: 0.1982\n",
            "k-fold 0  Train Loss: 0.1942, Validation Loss: 0.1986\n",
            "k-fold 0  Train Loss: 0.1938, Validation Loss: 0.1981\n",
            "k-fold 0  Train Loss: 0.1929, Validation Loss: 0.1974\n",
            "k-fold 0  Train Loss: 0.1920, Validation Loss: 0.1965\n",
            "k-fold 0  Train Loss: 0.1907, Validation Loss: 0.1953\n",
            "k-fold 0  Train Loss: 0.1910, Validation Loss: 0.1956\n",
            "k-fold 0  Train Loss: 0.1908, Validation Loss: 0.1954\n",
            "k-fold 0  Train Loss: 0.1904, Validation Loss: 0.1950\n",
            "k-fold 0  Train Loss: 0.1902, Validation Loss: 0.1948\n",
            "k-fold 0  Train Loss: 0.1903, Validation Loss: 0.1949\n",
            "k-fold 0  Train Loss: 0.1902, Validation Loss: 0.1947\n",
            "k-fold 0  Train Loss: 0.1898, Validation Loss: 0.1943\n",
            "k-fold 0  Train Loss: 0.1894, Validation Loss: 0.1939\n",
            "k-fold 0  Train Loss: 0.1889, Validation Loss: 0.1935\n",
            "k-fold 0  Train Loss: 0.1883, Validation Loss: 0.1928\n",
            "k-fold 0  Train Loss: 0.1875, Validation Loss: 0.1922\n",
            "k-fold 0  Train Loss: 0.1866, Validation Loss: 0.1913\n",
            "k-fold 0  Train Loss: 0.1856, Validation Loss: 0.1904\n",
            "k-fold 0  Train Loss: 0.1848, Validation Loss: 0.1897\n",
            "k-fold 0  Train Loss: 0.1837, Validation Loss: 0.1887\n",
            "k-fold 0  Train Loss: 0.1828, Validation Loss: 0.1878\n",
            "k-fold 0  Train Loss: 0.1813, Validation Loss: 0.1864\n",
            "k-fold 0  Train Loss: 0.1799, Validation Loss: 0.1850\n",
            "k-fold 0  Train Loss: 0.1787, Validation Loss: 0.1839\n",
            "k-fold 0  Train Loss: 0.1772, Validation Loss: 0.1824\n",
            "k-fold 0  Train Loss: 0.1750, Validation Loss: 0.1804\n",
            "k-fold 0  Train Loss: 0.1732, Validation Loss: 0.1788\n",
            "k-fold 0  Train Loss: 0.1718, Validation Loss: 0.1774\n",
            "k-fold 0  Train Loss: 0.1707, Validation Loss: 0.1763\n",
            "k-fold 0  Train Loss: 0.1700, Validation Loss: 0.1756\n",
            "k-fold 0  Train Loss: 0.1694, Validation Loss: 0.1751\n",
            "k-fold 0  Train Loss: 0.1689, Validation Loss: 0.1745\n",
            "k-fold 0  Train Loss: 0.1685, Validation Loss: 0.1741\n",
            "k-fold 0  Train Loss: 0.1682, Validation Loss: 0.1736\n",
            "k-fold 0  Train Loss: 0.1680, Validation Loss: 0.1733\n",
            "k-fold 0  Train Loss: 0.1678, Validation Loss: 0.1730\n",
            "k-fold 0  Train Loss: 0.1679, Validation Loss: 0.1730\n",
            "k-fold 0  Train Loss: 0.1681, Validation Loss: 0.1732\n",
            "k-fold 0  Train Loss: 0.1681, Validation Loss: 0.1731\n",
            "k-fold 0  Train Loss: 0.1683, Validation Loss: 0.1732\n",
            "k-fold 0  Train Loss: 0.1688, Validation Loss: 0.1736\n",
            "k-fold 0  Train Loss: 0.1703, Validation Loss: 0.1752\n",
            "k-fold 0  Train Loss: 0.1716, Validation Loss: 0.1765\n",
            "k-fold 0  Train Loss: 0.1732, Validation Loss: 0.1780\n",
            "k-fold 0  Train Loss: 0.1747, Validation Loss: 0.1796\n",
            "k-fold 0  Train Loss: 0.1746, Validation Loss: 0.1795\n",
            "k-fold 0  Train Loss: 0.1742, Validation Loss: 0.1791\n",
            "k-fold 0  Train Loss: 0.1735, Validation Loss: 0.1784\n",
            "k-fold 0  Train Loss: 0.1729, Validation Loss: 0.1778\n",
            "k-fold 0  Train Loss: 0.1718, Validation Loss: 0.1766\n",
            "k-fold 0  Train Loss: 0.1709, Validation Loss: 0.1758\n",
            "k-fold 0  Train Loss: 0.1702, Validation Loss: 0.1750\n",
            "k-fold 0  Train Loss: 0.1692, Validation Loss: 0.1739\n",
            "k-fold 0  Train Loss: 0.1681, Validation Loss: 0.1728\n",
            "k-fold 0  Train Loss: 0.1670, Validation Loss: 0.1717\n",
            "k-fold 0  Train Loss: 0.1661, Validation Loss: 0.1708\n",
            "k-fold 0  Train Loss: 0.1655, Validation Loss: 0.1703\n",
            "k-fold 0  Train Loss: 0.1653, Validation Loss: 0.1701\n",
            "k-fold 0  Train Loss: 0.1651, Validation Loss: 0.1700\n",
            "k-fold 0  Train Loss: 0.1650, Validation Loss: 0.1699\n",
            "k-fold 0  Train Loss: 0.1650, Validation Loss: 0.1699\n",
            "k-fold 0  Train Loss: 0.1659, Validation Loss: 0.1708\n",
            "k-fold 0  Train Loss: 0.1673, Validation Loss: 0.1722\n",
            "k-fold 0  Train Loss: 0.1684, Validation Loss: 0.1733\n",
            "k-fold 0  Train Loss: 0.1697, Validation Loss: 0.1746\n",
            "k-fold 0  Train Loss: 0.1708, Validation Loss: 0.1758\n",
            "k-fold 0  Train Loss: 0.1719, Validation Loss: 0.1768\n",
            "k-fold 0  Train Loss: 0.1726, Validation Loss: 0.1776\n",
            "k-fold 0  Train Loss: 0.1729, Validation Loss: 0.1779\n",
            "k-fold 0  Train Loss: 0.1745, Validation Loss: 0.1795\n",
            "k-fold 0  Train Loss: 0.1755, Validation Loss: 0.1806\n",
            "k-fold 0  Train Loss: 0.1762, Validation Loss: 0.1812\n",
            "k-fold 0  Train Loss: 0.1764, Validation Loss: 0.1813\n",
            "k-fold 0  Train Loss: 0.1762, Validation Loss: 0.1812\n",
            "k-fold 0  Train Loss: 0.1761, Validation Loss: 0.1811\n",
            "k-fold 0  Train Loss: 0.1756, Validation Loss: 0.1806\n",
            "k-fold 0  Train Loss: 0.1746, Validation Loss: 0.1796\n",
            "k-fold 0  Train Loss: 0.1747, Validation Loss: 0.1796\n",
            "k-fold 0  Train Loss: 0.1744, Validation Loss: 0.1793\n",
            "k-fold 0  Train Loss: 0.1732, Validation Loss: 0.1781\n",
            "k-fold 0  Train Loss: 0.1719, Validation Loss: 0.1766\n",
            "k-fold 0  Train Loss: 0.1705, Validation Loss: 0.1752\n",
            "k-fold 0  Train Loss: 0.1685, Validation Loss: 0.1731\n",
            "k-fold 0  Train Loss: 0.1669, Validation Loss: 0.1714\n",
            "k-fold 0  Train Loss: 0.1655, Validation Loss: 0.1698\n",
            "k-fold 0  Train Loss: 0.1645, Validation Loss: 0.1687\n",
            "k-fold 0  Train Loss: 0.1636, Validation Loss: 0.1678\n",
            "k-fold 0  Train Loss: 0.1629, Validation Loss: 0.1669\n",
            "k-fold 0  Train Loss: 0.1621, Validation Loss: 0.1662\n",
            "k-fold 0  Train Loss: 0.1621, Validation Loss: 0.1662\n",
            "k-fold 0  Train Loss: 0.1620, Validation Loss: 0.1661\n",
            "k-fold 0  Train Loss: 0.1621, Validation Loss: 0.1664\n",
            "k-fold 0  Train Loss: 0.1623, Validation Loss: 0.1667\n",
            "k-fold 0  Train Loss: 0.1637, Validation Loss: 0.1683\n",
            "k-fold 0  Train Loss: 0.1652, Validation Loss: 0.1700\n",
            "k-fold 0  Train Loss: 0.1667, Validation Loss: 0.1717\n",
            "k-fold 0  Train Loss: 0.1680, Validation Loss: 0.1730\n",
            "k-fold 0  Train Loss: 0.1688, Validation Loss: 0.1739\n",
            "k-fold 0  Train Loss: 0.1692, Validation Loss: 0.1744\n",
            "k-fold 0  Train Loss: 0.1700, Validation Loss: 0.1753\n",
            "k-fold 0  Train Loss: 0.1703, Validation Loss: 0.1756\n",
            "k-fold 0  Train Loss: 0.1701, Validation Loss: 0.1754\n",
            "k-fold 0  Train Loss: 0.1695, Validation Loss: 0.1748\n",
            "k-fold 0  Train Loss: 0.1686, Validation Loss: 0.1739\n",
            "k-fold 0  Train Loss: 0.1675, Validation Loss: 0.1726\n",
            "k-fold 0  Train Loss: 0.1669, Validation Loss: 0.1720\n",
            "k-fold 0  Train Loss: 0.1662, Validation Loss: 0.1712\n",
            "k-fold 0  Train Loss: 0.1650, Validation Loss: 0.1699\n",
            "k-fold 0  Train Loss: 0.1638, Validation Loss: 0.1686\n",
            "k-fold 0  Train Loss: 0.1629, Validation Loss: 0.1676\n",
            "k-fold 0  Train Loss: 0.1620, Validation Loss: 0.1666\n",
            "k-fold 0  Train Loss: 0.1613, Validation Loss: 0.1658\n",
            "k-fold 0  Train Loss: 0.1604, Validation Loss: 0.1647\n",
            "k-fold 0  Train Loss: 0.1597, Validation Loss: 0.1639\n",
            "k-fold 0  Train Loss: 0.1591, Validation Loss: 0.1633\n",
            "k-fold 0  Train Loss: 0.1587, Validation Loss: 0.1628\n",
            "k-fold 0  Train Loss: 0.1584, Validation Loss: 0.1625\n",
            "k-fold 0  Train Loss: 0.1581, Validation Loss: 0.1622\n",
            "k-fold 0  Train Loss: 0.1578, Validation Loss: 0.1618\n",
            "k-fold 0  Train Loss: 0.1573, Validation Loss: 0.1612\n",
            "k-fold 0  Train Loss: 0.1568, Validation Loss: 0.1606\n",
            "k-fold 0  Train Loss: 0.1566, Validation Loss: 0.1603\n",
            "k-fold 0  Train Loss: 0.1566, Validation Loss: 0.1602\n",
            "k-fold 0  Train Loss: 0.1568, Validation Loss: 0.1605\n",
            "k-fold 0  Train Loss: 0.1571, Validation Loss: 0.1609\n",
            "k-fold 0  Train Loss: 0.1566, Validation Loss: 0.1604\n",
            "k-fold 0  Train Loss: 0.1563, Validation Loss: 0.1600\n",
            "k-fold 0  Train Loss: 0.1560, Validation Loss: 0.1596\n",
            "k-fold 0  Train Loss: 0.1557, Validation Loss: 0.1592\n",
            "k-fold 0  Train Loss: 0.1558, Validation Loss: 0.1594\n",
            "k-fold 0  Train Loss: 0.1560, Validation Loss: 0.1597\n",
            "k-fold 0  Train Loss: 0.1561, Validation Loss: 0.1598\n",
            "k-fold 0  Train Loss: 0.1562, Validation Loss: 0.1600\n",
            "k-fold 0  Train Loss: 0.1570, Validation Loss: 0.1609\n",
            "k-fold 0  Train Loss: 0.1583, Validation Loss: 0.1624\n",
            "k-fold 0  Train Loss: 0.1597, Validation Loss: 0.1640\n",
            "k-fold 0  Train Loss: 0.1608, Validation Loss: 0.1652\n",
            "k-fold 0  Train Loss: 0.1610, Validation Loss: 0.1654\n",
            "k-fold 0  Train Loss: 0.1610, Validation Loss: 0.1653\n",
            "k-fold 0  Train Loss: 0.1605, Validation Loss: 0.1647\n",
            "k-fold 0  Train Loss: 0.1604, Validation Loss: 0.1646\n",
            "k-fold 0  Train Loss: 0.1602, Validation Loss: 0.1643\n",
            "k-fold 0  Train Loss: 0.1593, Validation Loss: 0.1631\n",
            "k-fold 0  Train Loss: 0.1584, Validation Loss: 0.1621\n",
            "k-fold 0  Train Loss: 0.1578, Validation Loss: 0.1614\n",
            "k-fold 0  Train Loss: 0.1567, Validation Loss: 0.1599\n",
            "k-fold 0  Train Loss: 0.1558, Validation Loss: 0.1588\n",
            "k-fold 0  Train Loss: 0.1551, Validation Loss: 0.1579\n",
            "k-fold 0  Train Loss: 0.1544, Validation Loss: 0.1570\n",
            "k-fold 0  Train Loss: 0.1536, Validation Loss: 0.1559\n",
            "k-fold 0  Train Loss: 0.1529, Validation Loss: 0.1550\n",
            "k-fold 0  Train Loss: 0.1525, Validation Loss: 0.1543\n",
            "k-fold 0  Train Loss: 0.1523, Validation Loss: 0.1540\n",
            "k-fold 0  Train Loss: 0.1518, Validation Loss: 0.1534\n",
            "k-fold 0  Train Loss: 0.1515, Validation Loss: 0.1529\n",
            "k-fold 0  Train Loss: 0.1512, Validation Loss: 0.1523\n",
            "k-fold 0  Train Loss: 0.1509, Validation Loss: 0.1518\n",
            "k-fold 0  Train Loss: 0.1507, Validation Loss: 0.1515\n",
            "k-fold 0  Train Loss: 0.1505, Validation Loss: 0.1513\n",
            "k-fold 0  Train Loss: 0.1504, Validation Loss: 0.1513\n",
            "k-fold 0  Train Loss: 0.1504, Validation Loss: 0.1514\n",
            "k-fold 0  Train Loss: 0.1508, Validation Loss: 0.1522\n",
            "k-fold 0  Train Loss: 0.1514, Validation Loss: 0.1531\n",
            "k-fold 0  Train Loss: 0.1522, Validation Loss: 0.1543\n",
            "k-fold 0  Train Loss: 0.1532, Validation Loss: 0.1555\n",
            "k-fold 0  Train Loss: 0.1542, Validation Loss: 0.1569\n",
            "k-fold 0  Train Loss: 0.1553, Validation Loss: 0.1581\n",
            "k-fold 0  Train Loss: 0.1565, Validation Loss: 0.1595\n",
            "k-fold 0  Train Loss: 0.1572, Validation Loss: 0.1604\n",
            "k-fold 0  Train Loss: 0.1584, Validation Loss: 0.1617\n",
            "k-fold 0  Train Loss: 0.1592, Validation Loss: 0.1626\n",
            "k-fold 0  Train Loss: 0.1596, Validation Loss: 0.1631\n",
            "k-fold 0  Train Loss: 0.1602, Validation Loss: 0.1637\n",
            "k-fold 0  Train Loss: 0.1587, Validation Loss: 0.1620\n",
            "k-fold 0  Train Loss: 0.1569, Validation Loss: 0.1600\n",
            "k-fold 0  Train Loss: 0.1554, Validation Loss: 0.1583\n",
            "k-fold 0  Train Loss: 0.1537, Validation Loss: 0.1564\n",
            "k-fold 0  Train Loss: 0.1522, Validation Loss: 0.1546\n",
            "k-fold 0  Train Loss: 0.1508, Validation Loss: 0.1529\n",
            "k-fold 0  Train Loss: 0.1497, Validation Loss: 0.1515\n",
            "k-fold 0  Train Loss: 0.1492, Validation Loss: 0.1508\n",
            "k-fold 0  Train Loss: 0.1488, Validation Loss: 0.1503\n",
            "k-fold 0  Train Loss: 0.1485, Validation Loss: 0.1499\n",
            "k-fold 0  Train Loss: 0.1482, Validation Loss: 0.1494\n",
            "k-fold 0  Train Loss: 0.1482, Validation Loss: 0.1494\n",
            "k-fold 0  Train Loss: 0.1487, Validation Loss: 0.1501\n",
            "k-fold 0  Train Loss: 0.1492, Validation Loss: 0.1508\n",
            "k-fold 0  Train Loss: 0.1501, Validation Loss: 0.1520\n",
            "k-fold 0  Train Loss: 0.1514, Validation Loss: 0.1536\n",
            "k-fold 0  Train Loss: 0.1528, Validation Loss: 0.1553\n",
            "k-fold 0  Train Loss: 0.1532, Validation Loss: 0.1558\n",
            "k-fold 0  Train Loss: 0.1537, Validation Loss: 0.1564\n",
            "k-fold 0  Train Loss: 0.1546, Validation Loss: 0.1574\n",
            "k-fold 0  Train Loss: 0.1550, Validation Loss: 0.1578\n",
            "k-fold 0  Train Loss: 0.1554, Validation Loss: 0.1584\n",
            "k-fold 0  Train Loss: 0.1558, Validation Loss: 0.1589\n",
            "k-fold 0  Train Loss: 0.1556, Validation Loss: 0.1587\n",
            "k-fold 0  Train Loss: 0.1547, Validation Loss: 0.1577\n",
            "k-fold 0  Train Loss: 0.1539, Validation Loss: 0.1568\n",
            "k-fold 0  Train Loss: 0.1529, Validation Loss: 0.1557\n",
            "k-fold 0  Train Loss: 0.1521, Validation Loss: 0.1548\n",
            "k-fold 0  Train Loss: 0.1518, Validation Loss: 0.1545\n",
            "k-fold 0  Train Loss: 0.1515, Validation Loss: 0.1543\n",
            "k-fold 0  Train Loss: 0.1511, Validation Loss: 0.1537\n",
            "k-fold 0  Train Loss: 0.1509, Validation Loss: 0.1536\n",
            "k-fold 0  Train Loss: 0.1509, Validation Loss: 0.1536\n",
            "k-fold 0  Train Loss: 0.1507, Validation Loss: 0.1534\n",
            "k-fold 0  Train Loss: 0.1508, Validation Loss: 0.1536\n",
            "k-fold 0  Train Loss: 0.1505, Validation Loss: 0.1533\n",
            "k-fold 0  Train Loss: 0.1495, Validation Loss: 0.1521\n",
            "k-fold 0  Train Loss: 0.1486, Validation Loss: 0.1512\n",
            "k-fold 0  Train Loss: 0.1478, Validation Loss: 0.1502\n",
            "k-fold 0  Train Loss: 0.1472, Validation Loss: 0.1495\n",
            "k-fold 0  Train Loss: 0.1470, Validation Loss: 0.1492\n",
            "k-fold 0  Train Loss: 0.1471, Validation Loss: 0.1494\n",
            "k-fold 0  Train Loss: 0.1471, Validation Loss: 0.1493\n",
            "k-fold 0  Train Loss: 0.1472, Validation Loss: 0.1495\n",
            "k-fold 0  Train Loss: 0.1466, Validation Loss: 0.1489\n",
            "k-fold 0  Train Loss: 0.1465, Validation Loss: 0.1488\n",
            "k-fold 0  Train Loss: 0.1464, Validation Loss: 0.1487\n",
            "k-fold 0  Train Loss: 0.1464, Validation Loss: 0.1488\n",
            "k-fold 0  Train Loss: 0.1464, Validation Loss: 0.1488\n",
            "k-fold 0  Train Loss: 0.1465, Validation Loss: 0.1490\n",
            "k-fold 0  Train Loss: 0.1465, Validation Loss: 0.1490\n",
            "k-fold 0  Train Loss: 0.1461, Validation Loss: 0.1486\n",
            "k-fold 0  Train Loss: 0.1451, Validation Loss: 0.1474\n",
            "k-fold 0  Train Loss: 0.1445, Validation Loss: 0.1465\n",
            "k-fold 0  Train Loss: 0.1439, Validation Loss: 0.1457\n",
            "k-fold 0  Train Loss: 0.1435, Validation Loss: 0.1451\n",
            "k-fold 0  Train Loss: 0.1437, Validation Loss: 0.1454\n",
            "k-fold 0  Train Loss: 0.1443, Validation Loss: 0.1461\n",
            "k-fold 0  Train Loss: 0.1452, Validation Loss: 0.1472\n",
            "k-fold 0  Train Loss: 0.1462, Validation Loss: 0.1483\n",
            "k-fold 0  Train Loss: 0.1480, Validation Loss: 0.1505\n",
            "k-fold 0  Train Loss: 0.1499, Validation Loss: 0.1527\n",
            "k-fold 0  Train Loss: 0.1517, Validation Loss: 0.1549\n",
            "k-fold 0  Train Loss: 0.1533, Validation Loss: 0.1568\n",
            "k-fold 0  Train Loss: 0.1530, Validation Loss: 0.1563\n",
            "k-fold 0  Train Loss: 0.1519, Validation Loss: 0.1551\n",
            "k-fold 0  Train Loss: 0.1507, Validation Loss: 0.1537\n",
            "k-fold 0  Train Loss: 0.1498, Validation Loss: 0.1526\n",
            "k-fold 0  Train Loss: 0.1495, Validation Loss: 0.1523\n",
            "k-fold 0  Train Loss: 0.1487, Validation Loss: 0.1514\n",
            "k-fold 0  Train Loss: 0.1478, Validation Loss: 0.1503\n",
            "k-fold 0  Train Loss: 0.1472, Validation Loss: 0.1496\n",
            "k-fold 0  Train Loss: 0.1473, Validation Loss: 0.1497\n",
            "k-fold 0  Train Loss: 0.1474, Validation Loss: 0.1498\n",
            "k-fold 0  Train Loss: 0.1475, Validation Loss: 0.1500\n",
            "k-fold 0  Train Loss: 0.1476, Validation Loss: 0.1501\n",
            "k-fold 0  Train Loss: 0.1480, Validation Loss: 0.1506\n",
            "k-fold 0  Train Loss: 0.1483, Validation Loss: 0.1509\n",
            "k-fold 0  Train Loss: 0.1486, Validation Loss: 0.1512\n",
            "k-fold 0  Train Loss: 0.1486, Validation Loss: 0.1513\n",
            "k-fold 0  Train Loss: 0.1486, Validation Loss: 0.1512\n",
            "k-fold 0  Train Loss: 0.1481, Validation Loss: 0.1507\n",
            "k-fold 0  Train Loss: 0.1477, Validation Loss: 0.1502\n",
            "k-fold 0  Train Loss: 0.1470, Validation Loss: 0.1495\n",
            "k-fold 0  Train Loss: 0.1456, Validation Loss: 0.1478\n",
            "k-fold 0  Train Loss: 0.1451, Validation Loss: 0.1472\n",
            "k-fold 0  Train Loss: 0.1443, Validation Loss: 0.1462\n",
            "k-fold 0  Train Loss: 0.1434, Validation Loss: 0.1452\n",
            "k-fold 0  Train Loss: 0.1425, Validation Loss: 0.1441\n",
            "k-fold 0  Train Loss: 0.1418, Validation Loss: 0.1433\n",
            "k-fold 0  Train Loss: 0.1413, Validation Loss: 0.1426\n",
            "k-fold 0  Train Loss: 0.1409, Validation Loss: 0.1421\n",
            "k-fold 0  Train Loss: 0.1402, Validation Loss: 0.1412\n",
            "k-fold 0  Train Loss: 0.1398, Validation Loss: 0.1406\n",
            "k-fold 0  Train Loss: 0.1395, Validation Loss: 0.1401\n",
            "k-fold 0  Train Loss: 0.1392, Validation Loss: 0.1396\n",
            "k-fold 0  Train Loss: 0.1388, Validation Loss: 0.1389\n",
            "k-fold 0  Train Loss: 0.1386, Validation Loss: 0.1384\n",
            "k-fold 0  Train Loss: 0.1384, Validation Loss: 0.1379\n",
            "k-fold 0  Train Loss: 0.1382, Validation Loss: 0.1377\n",
            "k-fold 0  Train Loss: 0.1381, Validation Loss: 0.1373\n",
            "k-fold 0  Train Loss: 0.1379, Validation Loss: 0.1371\n",
            "k-fold 0  Train Loss: 0.1378, Validation Loss: 0.1369\n",
            "k-fold 0  Train Loss: 0.1376, Validation Loss: 0.1368\n",
            "k-fold 0  Train Loss: 0.1375, Validation Loss: 0.1364\n",
            "k-fold 0  Train Loss: 0.1374, Validation Loss: 0.1361\n",
            "k-fold 0  Train Loss: 0.1372, Validation Loss: 0.1360\n",
            "k-fold 0  Train Loss: 0.1371, Validation Loss: 0.1361\n",
            "k-fold 0  Train Loss: 0.1370, Validation Loss: 0.1360\n",
            "k-fold 0  Train Loss: 0.1369, Validation Loss: 0.1359\n",
            "k-fold 0  Train Loss: 0.1370, Validation Loss: 0.1362\n",
            "k-fold 0  Train Loss: 0.1373, Validation Loss: 0.1369\n",
            "k-fold 0  Train Loss: 0.1377, Validation Loss: 0.1377\n",
            "k-fold 0  Train Loss: 0.1384, Validation Loss: 0.1387\n",
            "k-fold 0  Train Loss: 0.1391, Validation Loss: 0.1398\n",
            "k-fold 0  Train Loss: 0.1399, Validation Loss: 0.1410\n",
            "k-fold 0  Train Loss: 0.1410, Validation Loss: 0.1424\n",
            "k-fold 0  Train Loss: 0.1418, Validation Loss: 0.1435\n",
            "k-fold 0  Train Loss: 0.1425, Validation Loss: 0.1443\n",
            "k-fold 0  Train Loss: 0.1427, Validation Loss: 0.1446\n",
            "k-fold 0  Train Loss: 0.1420, Validation Loss: 0.1438\n",
            "k-fold 0  Train Loss: 0.1413, Validation Loss: 0.1428\n",
            "k-fold 0  Train Loss: 0.1404, Validation Loss: 0.1417\n",
            "k-fold 0  Train Loss: 0.1392, Validation Loss: 0.1402\n",
            "k-fold 0  Train Loss: 0.1390, Validation Loss: 0.1399\n",
            "k-fold 0  Train Loss: 0.1389, Validation Loss: 0.1398\n",
            "k-fold 0  Train Loss: 0.1384, Validation Loss: 0.1391\n",
            "k-fold 0  Train Loss: 0.1378, Validation Loss: 0.1383\n",
            "k-fold 0  Train Loss: 0.1369, Validation Loss: 0.1371\n",
            "k-fold 0  Train Loss: 0.1362, Validation Loss: 0.1362\n",
            "k-fold 0  Train Loss: 0.1357, Validation Loss: 0.1356\n",
            "k-fold 0  Train Loss: 0.1355, Validation Loss: 0.1352\n",
            "k-fold 0  Train Loss: 0.1357, Validation Loss: 0.1357\n",
            "k-fold 0  Train Loss: 0.1360, Validation Loss: 0.1363\n",
            "k-fold 0  Train Loss: 0.1363, Validation Loss: 0.1368\n",
            "k-fold 0  Train Loss: 0.1369, Validation Loss: 0.1377\n",
            "k-fold 0  Train Loss: 0.1378, Validation Loss: 0.1390\n",
            "k-fold 0  Train Loss: 0.1388, Validation Loss: 0.1403\n",
            "k-fold 0  Train Loss: 0.1399, Validation Loss: 0.1417\n",
            "k-fold 0  Train Loss: 0.1405, Validation Loss: 0.1424\n",
            "k-fold 0  Train Loss: 0.1411, Validation Loss: 0.1431\n",
            "k-fold 0  Train Loss: 0.1421, Validation Loss: 0.1443\n",
            "k-fold 0  Train Loss: 0.1431, Validation Loss: 0.1455\n",
            "k-fold 0  Train Loss: 0.1431, Validation Loss: 0.1455\n",
            "k-fold 0  Train Loss: 0.1436, Validation Loss: 0.1461\n",
            "k-fold 0  Train Loss: 0.1437, Validation Loss: 0.1461\n",
            "k-fold 0  Train Loss: 0.1437, Validation Loss: 0.1461\n",
            "k-fold 0  Train Loss: 0.1433, Validation Loss: 0.1457\n",
            "k-fold 0  Train Loss: 0.1421, Validation Loss: 0.1444\n",
            "k-fold 0  Train Loss: 0.1409, Validation Loss: 0.1430\n",
            "k-fold 0  Train Loss: 0.1395, Validation Loss: 0.1414\n",
            "k-fold 0  Train Loss: 0.1381, Validation Loss: 0.1397\n",
            "k-fold 0  Train Loss: 0.1369, Validation Loss: 0.1383\n",
            "k-fold 0  Train Loss: 0.1359, Validation Loss: 0.1370\n",
            "k-fold 0  Train Loss: 0.1351, Validation Loss: 0.1360\n",
            "k-fold 0  Train Loss: 0.1346, Validation Loss: 0.1352\n",
            "k-fold 0  Train Loss: 0.1342, Validation Loss: 0.1347\n",
            "k-fold 0  Train Loss: 0.1338, Validation Loss: 0.1341\n",
            "k-fold 0  Train Loss: 0.1338, Validation Loss: 0.1340\n",
            "k-fold 0  Train Loss: 0.1336, Validation Loss: 0.1339\n",
            "k-fold 0  Train Loss: 0.1331, Validation Loss: 0.1332\n",
            "k-fold 0  Train Loss: 0.1329, Validation Loss: 0.1330\n",
            "k-fold 0  Train Loss: 0.1328, Validation Loss: 0.1329\n",
            "k-fold 0  Train Loss: 0.1328, Validation Loss: 0.1331\n",
            "k-fold 0  Train Loss: 0.1328, Validation Loss: 0.1332\n",
            "k-fold 0  Train Loss: 0.1328, Validation Loss: 0.1334\n",
            "k-fold 0  Train Loss: 0.1330, Validation Loss: 0.1338\n",
            "k-fold 0  Train Loss: 0.1332, Validation Loss: 0.1341\n",
            "k-fold 0  Train Loss: 0.1331, Validation Loss: 0.1339\n",
            "k-fold 0  Train Loss: 0.1329, Validation Loss: 0.1337\n",
            "k-fold 0  Train Loss: 0.1327, Validation Loss: 0.1334\n",
            "k-fold 0  Train Loss: 0.1327, Validation Loss: 0.1335\n",
            "k-fold 0  Train Loss: 0.1326, Validation Loss: 0.1335\n",
            "k-fold 0  Train Loss: 0.1325, Validation Loss: 0.1332\n",
            "k-fold 0  Train Loss: 0.1325, Validation Loss: 0.1332\n",
            "k-fold 0  Train Loss: 0.1324, Validation Loss: 0.1332\n",
            "k-fold 0  Train Loss: 0.1327, Validation Loss: 0.1334\n",
            "k-fold 0  Train Loss: 0.1329, Validation Loss: 0.1337\n",
            "k-fold 0  Train Loss: 0.1335, Validation Loss: 0.1344\n",
            "k-fold 0  Train Loss: 0.1340, Validation Loss: 0.1349\n",
            "k-fold 0  Train Loss: 0.1342, Validation Loss: 0.1351\n",
            "k-fold 0  Train Loss: 0.1345, Validation Loss: 0.1355\n",
            "k-fold 0  Train Loss: 0.1345, Validation Loss: 0.1356\n",
            "k-fold 0  Train Loss: 0.1344, Validation Loss: 0.1354\n",
            "k-fold 0  Train Loss: 0.1342, Validation Loss: 0.1352\n",
            "k-fold 0  Train Loss: 0.1341, Validation Loss: 0.1350\n",
            "k-fold 0  Train Loss: 0.1337, Validation Loss: 0.1344\n",
            "k-fold 0  Train Loss: 0.1333, Validation Loss: 0.1340\n",
            "k-fold 0  Train Loss: 0.1337, Validation Loss: 0.1345\n",
            "k-fold 0  Train Loss: 0.1343, Validation Loss: 0.1351\n",
            "k-fold 0  Train Loss: 0.1340, Validation Loss: 0.1348\n",
            "k-fold 0  Train Loss: 0.1338, Validation Loss: 0.1344\n",
            "k-fold 0  Train Loss: 0.1333, Validation Loss: 0.1336\n",
            "k-fold 0  Train Loss: 0.1322, Validation Loss: 0.1321\n",
            "k-fold 0  Train Loss: 0.1313, Validation Loss: 0.1308\n",
            "k-fold 0  Train Loss: 0.1308, Validation Loss: 0.1300\n",
            "k-fold 0  Train Loss: 0.1304, Validation Loss: 0.1293\n",
            "k-fold 0  Train Loss: 0.1300, Validation Loss: 0.1286\n",
            "k-fold 0  Train Loss: 0.1299, Validation Loss: 0.1285\n",
            "k-fold 0  Train Loss: 0.1301, Validation Loss: 0.1288\n",
            "k-fold 0  Train Loss: 0.1301, Validation Loss: 0.1288\n",
            "k-fold 0  Train Loss: 0.1301, Validation Loss: 0.1288\n",
            "k-fold 0  Train Loss: 0.1301, Validation Loss: 0.1288\n",
            "k-fold 0  Train Loss: 0.1307, Validation Loss: 0.1297\n",
            "k-fold 0  Train Loss: 0.1305, Validation Loss: 0.1294\n",
            "k-fold 0  Train Loss: 0.1302, Validation Loss: 0.1288\n",
            "k-fold 0  Train Loss: 0.1301, Validation Loss: 0.1286\n",
            "k-fold 0  Train Loss: 0.1300, Validation Loss: 0.1285\n",
            "k-fold 0  Train Loss: 0.1298, Validation Loss: 0.1283\n",
            "k-fold 0  Train Loss: 0.1296, Validation Loss: 0.1279\n",
            "k-fold 0  Train Loss: 0.1294, Validation Loss: 0.1275\n",
            "k-fold 0  Train Loss: 0.1292, Validation Loss: 0.1272\n",
            "k-fold 0  Train Loss: 0.1291, Validation Loss: 0.1271\n",
            "k-fold 0  Train Loss: 0.1289, Validation Loss: 0.1269\n",
            "k-fold 0  Train Loss: 0.1287, Validation Loss: 0.1266\n",
            "k-fold 0  Train Loss: 0.1284, Validation Loss: 0.1261\n",
            "k-fold 0  Train Loss: 0.1282, Validation Loss: 0.1256\n",
            "k-fold 0  Train Loss: 0.1279, Validation Loss: 0.1251\n",
            "k-fold 0  Train Loss: 0.1278, Validation Loss: 0.1250\n",
            "k-fold 0  Train Loss: 0.1276, Validation Loss: 0.1248\n",
            "k-fold 0  Train Loss: 0.1277, Validation Loss: 0.1251\n",
            "k-fold 0  Train Loss: 0.1279, Validation Loss: 0.1257\n",
            "k-fold 0  Train Loss: 0.1279, Validation Loss: 0.1260\n",
            "k-fold 0  Train Loss: 0.1283, Validation Loss: 0.1268\n",
            "k-fold 0  Train Loss: 0.1289, Validation Loss: 0.1277\n",
            "k-fold 0  Train Loss: 0.1296, Validation Loss: 0.1287\n",
            "k-fold 0  Train Loss: 0.1302, Validation Loss: 0.1296\n",
            "k-fold 0  Train Loss: 0.1304, Validation Loss: 0.1299\n",
            "k-fold 0  Train Loss: 0.1314, Validation Loss: 0.1311\n",
            "k-fold 0  Train Loss: 0.1319, Validation Loss: 0.1318\n",
            "k-fold 0  Train Loss: 0.1328, Validation Loss: 0.1328\n",
            "k-fold 0  Train Loss: 0.1337, Validation Loss: 0.1339\n",
            "k-fold 0  Train Loss: 0.1345, Validation Loss: 0.1347\n",
            "k-fold 0  Train Loss: 0.1347, Validation Loss: 0.1349\n",
            "k-fold 0  Train Loss: 0.1349, Validation Loss: 0.1350\n",
            "k-fold 0  Train Loss: 0.1348, Validation Loss: 0.1348\n",
            "k-fold 0  Train Loss: 0.1351, Validation Loss: 0.1350\n",
            "k-fold 0  Train Loss: 0.1349, Validation Loss: 0.1346\n",
            "k-fold 0  Train Loss: 0.1347, Validation Loss: 0.1344\n",
            "k-fold 0  Train Loss: 0.1343, Validation Loss: 0.1339\n",
            "k-fold 0  Train Loss: 0.1336, Validation Loss: 0.1330\n",
            "k-fold 0  Train Loss: 0.1328, Validation Loss: 0.1322\n",
            "k-fold 0  Train Loss: 0.1322, Validation Loss: 0.1315\n",
            "k-fold 0  Train Loss: 0.1316, Validation Loss: 0.1309\n",
            "k-fold 0  Train Loss: 0.1297, Validation Loss: 0.1286\n",
            "k-fold 0  Train Loss: 0.1282, Validation Loss: 0.1268\n",
            "k-fold 0  Train Loss: 0.1276, Validation Loss: 0.1260\n",
            "k-fold 0  Train Loss: 0.1273, Validation Loss: 0.1255\n",
            "k-fold 0  Train Loss: 0.1270, Validation Loss: 0.1248\n",
            "k-fold 0  Train Loss: 0.1267, Validation Loss: 0.1242\n",
            "k-fold 0  Train Loss: 0.1263, Validation Loss: 0.1236\n",
            "k-fold 0  Train Loss: 0.1260, Validation Loss: 0.1231\n",
            "k-fold 0  Train Loss: 0.1258, Validation Loss: 0.1224\n",
            "k-fold 0  Train Loss: 0.1255, Validation Loss: 0.1218\n",
            "k-fold 0  Train Loss: 0.1252, Validation Loss: 0.1212\n",
            "k-fold 0  Train Loss: 0.1250, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1249, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.1247, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1246, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.1245, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.1245, Validation Loss: 0.1197\n",
            "k-fold 0  Train Loss: 0.1247, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.1250, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1255, Validation Loss: 0.1216\n",
            "k-fold 0  Train Loss: 0.1249, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.1247, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.1247, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1246, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1244, Validation Loss: 0.1195\n",
            "k-fold 0  Train Loss: 0.1242, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.1241, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.1241, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.1244, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1248, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1252, Validation Loss: 0.1213\n",
            "k-fold 0  Train Loss: 0.1255, Validation Loss: 0.1218\n",
            "k-fold 0  Train Loss: 0.1253, Validation Loss: 0.1216\n",
            "k-fold 0  Train Loss: 0.1253, Validation Loss: 0.1216\n",
            "k-fold 0  Train Loss: 0.1249, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.1249, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.1240, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.1233, Validation Loss: 0.1192\n",
            "k-fold 0  Train Loss: 0.1227, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.1224, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.1223, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.1225, Validation Loss: 0.1191\n",
            "k-fold 0  Train Loss: 0.1228, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1231, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1239, Validation Loss: 0.1221\n",
            "k-fold 0  Train Loss: 0.1248, Validation Loss: 0.1235\n",
            "k-fold 0  Train Loss: 0.1256, Validation Loss: 0.1248\n",
            "k-fold 0  Train Loss: 0.1265, Validation Loss: 0.1260\n",
            "k-fold 0  Train Loss: 0.1274, Validation Loss: 0.1272\n",
            "k-fold 0  Train Loss: 0.1283, Validation Loss: 0.1284\n",
            "k-fold 0  Train Loss: 0.1288, Validation Loss: 0.1291\n",
            "k-fold 0  Train Loss: 0.1289, Validation Loss: 0.1293\n",
            "k-fold 0  Train Loss: 0.1294, Validation Loss: 0.1299\n",
            "k-fold 0  Train Loss: 0.1290, Validation Loss: 0.1296\n",
            "k-fold 0  Train Loss: 0.1284, Validation Loss: 0.1290\n",
            "k-fold 0  Train Loss: 0.1276, Validation Loss: 0.1282\n",
            "k-fold 0  Train Loss: 0.1274, Validation Loss: 0.1280\n",
            "k-fold 0  Train Loss: 0.1269, Validation Loss: 0.1276\n",
            "k-fold 0  Train Loss: 0.1264, Validation Loss: 0.1271\n",
            "k-fold 0  Train Loss: 0.1257, Validation Loss: 0.1263\n",
            "k-fold 0  Train Loss: 0.1248, Validation Loss: 0.1252\n",
            "k-fold 0  Train Loss: 0.1243, Validation Loss: 0.1247\n",
            "k-fold 0  Train Loss: 0.1241, Validation Loss: 0.1244\n",
            "k-fold 0  Train Loss: 0.1236, Validation Loss: 0.1238\n",
            "k-fold 0  Train Loss: 0.1240, Validation Loss: 0.1243\n",
            "k-fold 0  Train Loss: 0.1242, Validation Loss: 0.1246\n",
            "k-fold 0  Train Loss: 0.1242, Validation Loss: 0.1247\n",
            "k-fold 0  Train Loss: 0.1243, Validation Loss: 0.1248\n",
            "k-fold 0  Train Loss: 0.1235, Validation Loss: 0.1239\n",
            "k-fold 0  Train Loss: 0.1229, Validation Loss: 0.1231\n",
            "k-fold 0  Train Loss: 0.1224, Validation Loss: 0.1223\n",
            "k-fold 0  Train Loss: 0.1220, Validation Loss: 0.1218\n",
            "k-fold 0  Train Loss: 0.1213, Validation Loss: 0.1206\n",
            "k-fold 0  Train Loss: 0.1208, Validation Loss: 0.1196\n",
            "k-fold 0  Train Loss: 0.1204, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.1203, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1204, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1204, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.1206, Validation Loss: 0.1189\n",
            "k-fold 0  Train Loss: 0.1207, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.1217, Validation Loss: 0.1212\n",
            "k-fold 0  Train Loss: 0.1232, Validation Loss: 0.1235\n",
            "k-fold 0  Train Loss: 0.1250, Validation Loss: 0.1262\n",
            "k-fold 0  Train Loss: 0.1273, Validation Loss: 0.1292\n",
            "k-fold 0  Train Loss: 0.1294, Validation Loss: 0.1318\n",
            "k-fold 0  Train Loss: 0.1310, Validation Loss: 0.1338\n",
            "k-fold 0  Train Loss: 0.1318, Validation Loss: 0.1348\n",
            "k-fold 0  Train Loss: 0.1320, Validation Loss: 0.1351\n",
            "k-fold 0  Train Loss: 0.1320, Validation Loss: 0.1351\n",
            "k-fold 0  Train Loss: 0.1307, Validation Loss: 0.1336\n",
            "k-fold 0  Train Loss: 0.1294, Validation Loss: 0.1321\n",
            "k-fold 0  Train Loss: 0.1276, Validation Loss: 0.1299\n",
            "k-fold 0  Train Loss: 0.1271, Validation Loss: 0.1292\n",
            "k-fold 0  Train Loss: 0.1268, Validation Loss: 0.1289\n",
            "k-fold 0  Train Loss: 0.1261, Validation Loss: 0.1280\n",
            "k-fold 0  Train Loss: 0.1254, Validation Loss: 0.1271\n",
            "k-fold 0  Train Loss: 0.1244, Validation Loss: 0.1260\n",
            "k-fold 0  Train Loss: 0.1239, Validation Loss: 0.1253\n",
            "k-fold 0  Train Loss: 0.1232, Validation Loss: 0.1245\n",
            "k-fold 0  Train Loss: 0.1226, Validation Loss: 0.1236\n",
            "k-fold 0  Train Loss: 0.1223, Validation Loss: 0.1232\n",
            "k-fold 0  Train Loss: 0.1220, Validation Loss: 0.1230\n",
            "k-fold 0  Train Loss: 0.1220, Validation Loss: 0.1230\n",
            "k-fold 0  Train Loss: 0.1218, Validation Loss: 0.1227\n",
            "k-fold 0  Train Loss: 0.1219, Validation Loss: 0.1229\n",
            "k-fold 0  Train Loss: 0.1221, Validation Loss: 0.1231\n",
            "k-fold 0  Train Loss: 0.1221, Validation Loss: 0.1232\n",
            "k-fold 0  Train Loss: 0.1225, Validation Loss: 0.1236\n",
            "k-fold 0  Train Loss: 0.1230, Validation Loss: 0.1242\n",
            "k-fold 0  Train Loss: 0.1236, Validation Loss: 0.1249\n",
            "k-fold 0  Train Loss: 0.1237, Validation Loss: 0.1250\n",
            "k-fold 0  Train Loss: 0.1235, Validation Loss: 0.1247\n",
            "k-fold 0  Train Loss: 0.1238, Validation Loss: 0.1250\n",
            "k-fold 0  Train Loss: 0.1239, Validation Loss: 0.1251\n",
            "k-fold 0  Train Loss: 0.1247, Validation Loss: 0.1259\n",
            "k-fold 0  Train Loss: 0.1250, Validation Loss: 0.1263\n",
            "k-fold 0  Train Loss: 0.1246, Validation Loss: 0.1258\n",
            "k-fold 0  Train Loss: 0.1243, Validation Loss: 0.1254\n",
            "k-fold 0  Train Loss: 0.1239, Validation Loss: 0.1249\n",
            "k-fold 0  Train Loss: 0.1231, Validation Loss: 0.1239\n",
            "k-fold 0  Train Loss: 0.1223, Validation Loss: 0.1230\n",
            "k-fold 0  Train Loss: 0.1218, Validation Loss: 0.1224\n",
            "k-fold 0  Train Loss: 0.1214, Validation Loss: 0.1217\n",
            "k-fold 0  Train Loss: 0.1209, Validation Loss: 0.1211\n",
            "k-fold 0  Train Loss: 0.1205, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1202, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.1201, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.1202, Validation Loss: 0.1206\n",
            "k-fold 0  Train Loss: 0.1196, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.1191, Validation Loss: 0.1192\n",
            "k-fold 0  Train Loss: 0.1187, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.1186, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1186, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1185, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1184, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1184, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1183, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1183, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1183, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1183, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.1189, Validation Loss: 0.1195\n",
            "k-fold 0  Train Loss: 0.1197, Validation Loss: 0.1206\n",
            "k-fold 0  Train Loss: 0.1206, Validation Loss: 0.1217\n",
            "k-fold 0  Train Loss: 0.1214, Validation Loss: 0.1226\n",
            "k-fold 0  Train Loss: 0.1225, Validation Loss: 0.1238\n",
            "k-fold 0  Train Loss: 0.1237, Validation Loss: 0.1251\n",
            "k-fold 0  Train Loss: 0.1246, Validation Loss: 0.1260\n",
            "k-fold 0  Train Loss: 0.1254, Validation Loss: 0.1268\n",
            "k-fold 0  Train Loss: 0.1272, Validation Loss: 0.1286\n",
            "k-fold 0  Train Loss: 0.1280, Validation Loss: 0.1293\n",
            "k-fold 0  Train Loss: 0.1291, Validation Loss: 0.1304\n",
            "k-fold 0  Train Loss: 0.1295, Validation Loss: 0.1305\n",
            "k-fold 0  Train Loss: 0.1297, Validation Loss: 0.1308\n",
            "k-fold 0  Train Loss: 0.1294, Validation Loss: 0.1306\n",
            "k-fold 0  Train Loss: 0.1285, Validation Loss: 0.1296\n",
            "k-fold 0  Train Loss: 0.1273, Validation Loss: 0.1284\n",
            "k-fold 0  Train Loss: 0.1255, Validation Loss: 0.1266\n",
            "k-fold 0  Train Loss: 0.1235, Validation Loss: 0.1244\n",
            "k-fold 0  Train Loss: 0.1221, Validation Loss: 0.1228\n",
            "k-fold 0  Train Loss: 0.1207, Validation Loss: 0.1211\n",
            "k-fold 0  Train Loss: 0.1198, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.1190, Validation Loss: 0.1190\n",
            "k-fold 0  Train Loss: 0.1181, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1173, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.1169, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1166, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1165, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.1167, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.1156, Validation Loss: 0.1130\n",
            "k-fold 0  Train Loss: 0.1151, Validation Loss: 0.1120\n",
            "k-fold 0  Train Loss: 0.1150, Validation Loss: 0.1116\n",
            "k-fold 0  Train Loss: 0.1151, Validation Loss: 0.1115\n",
            "k-fold 0  Train Loss: 0.1156, Validation Loss: 0.1113\n",
            "k-fold 0  Train Loss: 0.1163, Validation Loss: 0.1115\n",
            "k-fold 0  Train Loss: 0.1166, Validation Loss: 0.1118\n",
            "k-fold 0  Train Loss: 0.1162, Validation Loss: 0.1119\n",
            "k-fold 0  Train Loss: 0.1162, Validation Loss: 0.1121\n",
            "k-fold 0  Train Loss: 0.1155, Validation Loss: 0.1122\n",
            "k-fold 0  Train Loss: 0.1148, Validation Loss: 0.1123\n",
            "k-fold 0  Train Loss: 0.1144, Validation Loss: 0.1126\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1128\n",
            "k-fold 0  Train Loss: 0.1142, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1142, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1144, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.1147, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.1152, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.1158, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1165, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1162, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1163, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1164, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.1167, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.1168, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.1167, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.1167, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.1164, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1162, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1164, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.1163, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1160, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.1150, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1144, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.1142, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.1145, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.1146, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.1150, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.1156, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.1166, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1175, Validation Loss: 0.1196\n",
            "k-fold 0  Train Loss: 0.1179, Validation Loss: 0.1202\n",
            "k-fold 0  Train Loss: 0.1181, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1181, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1183, Validation Loss: 0.1211\n",
            "k-fold 0  Train Loss: 0.1190, Validation Loss: 0.1220\n",
            "k-fold 0  Train Loss: 0.1195, Validation Loss: 0.1226\n",
            "k-fold 0  Train Loss: 0.1196, Validation Loss: 0.1227\n",
            "k-fold 0  Train Loss: 0.1198, Validation Loss: 0.1231\n",
            "k-fold 0  Train Loss: 0.1202, Validation Loss: 0.1236\n",
            "k-fold 0  Train Loss: 0.1203, Validation Loss: 0.1238\n",
            "k-fold 0  Train Loss: 0.1208, Validation Loss: 0.1244\n",
            "k-fold 0  Train Loss: 0.1210, Validation Loss: 0.1246\n",
            "k-fold 0  Train Loss: 0.1204, Validation Loss: 0.1241\n",
            "k-fold 0  Train Loss: 0.1195, Validation Loss: 0.1231\n",
            "k-fold 0  Train Loss: 0.1189, Validation Loss: 0.1224\n",
            "k-fold 0  Train Loss: 0.1180, Validation Loss: 0.1215\n",
            "k-fold 0  Train Loss: 0.1169, Validation Loss: 0.1202\n",
            "k-fold 0  Train Loss: 0.1163, Validation Loss: 0.1195\n",
            "k-fold 0  Train Loss: 0.1157, Validation Loss: 0.1187\n",
            "k-fold 0  Train Loss: 0.1149, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1146, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.1142, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.1140, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.1138, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.1142, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1145, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1148, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1152, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1155, Validation Loss: 0.1187\n",
            "k-fold 0  Train Loss: 0.1163, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.1170, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.1176, Validation Loss: 0.1215\n",
            "k-fold 0  Train Loss: 0.1169, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.1164, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.1156, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.1147, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1141, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.1134, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.1128, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1125, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.1122, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.1121, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.1118, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.1118, Validation Loss: 0.1138\n",
            "k-fold 0  Train Loss: 0.1116, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1115, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1114, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1114, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1114, Validation Loss: 0.1125\n",
            "k-fold 0  Train Loss: 0.1113, Validation Loss: 0.1125\n",
            "k-fold 0  Train Loss: 0.1112, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1111, Validation Loss: 0.1128\n",
            "k-fold 0  Train Loss: 0.1110, Validation Loss: 0.1128\n",
            "k-fold 0  Train Loss: 0.1110, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1110, Validation Loss: 0.1133\n",
            "k-fold 0  Train Loss: 0.1110, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1109, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1108, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1107, Validation Loss: 0.1130\n",
            "k-fold 0  Train Loss: 0.1107, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1108, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1111, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.1115, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.1119, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.1134, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.1154, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.1165, Validation Loss: 0.1222\n",
            "k-fold 0  Train Loss: 0.1175, Validation Loss: 0.1235\n",
            "k-fold 0  Train Loss: 0.1181, Validation Loss: 0.1242\n",
            "k-fold 0  Train Loss: 0.1181, Validation Loss: 0.1242\n",
            "k-fold 0  Train Loss: 0.1175, Validation Loss: 0.1235\n",
            "k-fold 0  Train Loss: 0.1169, Validation Loss: 0.1229\n",
            "k-fold 0  Train Loss: 0.1156, Validation Loss: 0.1214\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.1132, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.1122, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.1112, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1104, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.1102, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.1101, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.1099, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1098, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1096, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1096, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1096, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1095, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.1094, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1093, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1091, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1093, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1096, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.1100, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1104, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1108, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.1115, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1124, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.1134, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1209\n",
            "k-fold 0  Train Loss: 0.1153, Validation Loss: 0.1220\n",
            "k-fold 0  Train Loss: 0.1161, Validation Loss: 0.1230\n",
            "k-fold 0  Train Loss: 0.1166, Validation Loss: 0.1235\n",
            "k-fold 0  Train Loss: 0.1165, Validation Loss: 0.1235\n",
            "k-fold 0  Train Loss: 0.1159, Validation Loss: 0.1227\n",
            "k-fold 0  Train Loss: 0.1154, Validation Loss: 0.1222\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1209\n",
            "k-fold 0  Train Loss: 0.1132, Validation Loss: 0.1195\n",
            "k-fold 0  Train Loss: 0.1115, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.1101, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.1094, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.1090, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1126\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1122\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1119\n",
            "k-fold 0  Train Loss: 0.1087, Validation Loss: 0.1119\n",
            "k-fold 0  Train Loss: 0.1086, Validation Loss: 0.1121\n",
            "k-fold 0  Train Loss: 0.1087, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1089, Validation Loss: 0.1133\n",
            "k-fold 0  Train Loss: 0.1094, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.1095, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.1100, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.1106, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1107, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.1108, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.1105, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.1102, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1099, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.1098, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.1095, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1090, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1133\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.1103, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.1108, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.1119, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1136, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.1147, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1154, Validation Loss: 0.1214\n",
            "k-fold 0  Train Loss: 0.1155, Validation Loss: 0.1216\n",
            "k-fold 0  Train Loss: 0.1155, Validation Loss: 0.1215\n",
            "k-fold 0  Train Loss: 0.1146, Validation Loss: 0.1205\n",
            "k-fold 0  Train Loss: 0.1132, Validation Loss: 0.1190\n",
            "k-fold 0  Train Loss: 0.1121, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1115, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.1112, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1106, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.1079, Validation Loss: 0.1123\n",
            "k-fold 0  Train Loss: 0.1077, Validation Loss: 0.1118\n",
            "k-fold 0  Train Loss: 0.1079, Validation Loss: 0.1118\n",
            "k-fold 0  Train Loss: 0.1081, Validation Loss: 0.1119\n",
            "k-fold 0  Train Loss: 0.1083, Validation Loss: 0.1121\n",
            "k-fold 0  Train Loss: 0.1084, Validation Loss: 0.1123\n",
            "k-fold 0  Train Loss: 0.1082, Validation Loss: 0.1124\n",
            "k-fold 0  Train Loss: 0.1081, Validation Loss: 0.1126\n",
            "k-fold 0  Train Loss: 0.1080, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1078, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1077, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.1078, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1080, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.1085, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.1091, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.1094, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1099, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.1096, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1095, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1091, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.1085, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.1082, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.1079, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1076, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.1071, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1130\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1127\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1125\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1122\n",
            "k-fold 0  Train Loss: 0.1068, Validation Loss: 0.1121\n",
            "k-fold 0  Train Loss: 0.1067, Validation Loss: 0.1120\n",
            "k-fold 0  Train Loss: 0.1065, Validation Loss: 0.1119\n",
            "k-fold 0  Train Loss: 0.1065, Validation Loss: 0.1124\n",
            "k-fold 0  Train Loss: 0.1067, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1071, Validation Loss: 0.1133\n",
            "k-fold 0  Train Loss: 0.1071, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1073, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1078, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1090, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.1101, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.1113, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.1125, Validation Loss: 0.1190\n",
            "k-fold 0  Train Loss: 0.1135, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.1141, Validation Loss: 0.1206\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1206\n",
            "k-fold 0  Train Loss: 0.1107, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.1084, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.1063, Validation Loss: 0.1124\n",
            "k-fold 0  Train Loss: 0.1062, Validation Loss: 0.1116\n",
            "k-fold 0  Train Loss: 0.1068, Validation Loss: 0.1117\n",
            "k-fold 0  Train Loss: 0.1077, Validation Loss: 0.1123\n",
            "k-fold 0  Train Loss: 0.1087, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1093, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1133\n",
            "k-fold 0  Train Loss: 0.1082, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.1074, Validation Loss: 0.1123\n",
            "k-fold 0  Train Loss: 0.1064, Validation Loss: 0.1117\n",
            "k-fold 0  Train Loss: 0.1054, Validation Loss: 0.1111\n",
            "k-fold 0  Train Loss: 0.1049, Validation Loss: 0.1111\n",
            "k-fold 0  Train Loss: 0.1063, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1096, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.1138, Validation Loss: 0.1233\n",
            "k-fold 0  Train Loss: 0.1176, Validation Loss: 0.1273\n",
            "k-fold 0  Train Loss: 0.1215, Validation Loss: 0.1313\n",
            "k-fold 0  Train Loss: 0.1244, Validation Loss: 0.1341\n",
            "k-fold 0  Train Loss: 0.1261, Validation Loss: 0.1359\n",
            "k-fold 0  Train Loss: 0.1266, Validation Loss: 0.1366\n",
            "k-fold 0  Train Loss: 0.1261, Validation Loss: 0.1364\n",
            "k-fold 0  Train Loss: 0.1247, Validation Loss: 0.1354\n",
            "k-fold 0  Train Loss: 0.1233, Validation Loss: 0.1343\n",
            "k-fold 0  Train Loss: 0.1214, Validation Loss: 0.1328\n",
            "k-fold 0  Train Loss: 0.1198, Validation Loss: 0.1315\n",
            "k-fold 0  Train Loss: 0.1175, Validation Loss: 0.1296\n",
            "k-fold 0  Train Loss: 0.1154, Validation Loss: 0.1278\n",
            "k-fold 0  Train Loss: 0.1135, Validation Loss: 0.1262\n",
            "k-fold 0  Train Loss: 0.1119, Validation Loss: 0.1247\n",
            "k-fold 0  Train Loss: 0.1110, Validation Loss: 0.1238\n",
            "k-fold 0  Train Loss: 0.1102, Validation Loss: 0.1232\n",
            "k-fold 0  Train Loss: 0.1094, Validation Loss: 0.1224\n",
            "k-fold 0  Train Loss: 0.1092, Validation Loss: 0.1222\n",
            "k-fold 0  Train Loss: 0.1088, Validation Loss: 0.1218\n",
            "k-fold 0  Train Loss: 0.1085, Validation Loss: 0.1214\n",
            "k-fold 0  Train Loss: 0.1083, Validation Loss: 0.1213\n",
            "k-fold 0  Train Loss: 0.1081, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.1079, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1076, Validation Loss: 0.1205\n",
            "k-fold 0  Train Loss: 0.1074, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.1069, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.1065, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.1061, Validation Loss: 0.1190\n",
            "k-fold 0  Train Loss: 0.1058, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.1054, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.1051, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1049, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.1047, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1045, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1044, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.1044, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1044, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1042, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1042, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1041, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.1041, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1040, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1038, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.1038, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.1037, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.1042, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.1047, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1050, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.1053, Validation Loss: 0.1191\n",
            "k-fold 0  Train Loss: 0.1062, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.1067, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.1074, Validation Loss: 0.1209\n",
            "k-fold 0  Train Loss: 0.1073, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1076, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.1080, Validation Loss: 0.1214\n",
            "k-fold 0  Train Loss: 0.1074, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1067, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.1060, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.1057, Validation Loss: 0.1189\n",
            "k-fold 0  Train Loss: 0.1050, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.1044, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1045, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.1044, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1043, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1041, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.1047, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.1053, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.1055, Validation Loss: 0.1187\n",
            "k-fold 0  Train Loss: 0.1057, Validation Loss: 0.1189\n",
            "k-fold 0  Train Loss: 0.1050, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.1038, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.1031, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1028, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.1041, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1057, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.1070, Validation Loss: 0.1197\n",
            "k-fold 0  Train Loss: 0.1083, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.1095, Validation Loss: 0.1220\n",
            "k-fold 0  Train Loss: 0.1098, Validation Loss: 0.1222\n",
            "k-fold 0  Train Loss: 0.1097, Validation Loss: 0.1220\n",
            "k-fold 0  Train Loss: 0.1094, Validation Loss: 0.1217\n",
            "k-fold 0  Train Loss: 0.1091, Validation Loss: 0.1213\n",
            "k-fold 0  Train Loss: 0.1087, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.1079, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.1069, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.1047, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.1032, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.1024, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.1019, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.1021, Validation Loss: 0.1138\n",
            "k-fold 0  Train Loss: 0.1027, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.1032, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.1037, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1035, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.1027, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.1019, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.1010, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.1005, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.1004, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.1011, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.1024, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1036, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.1047, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.1055, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.1059, Validation Loss: 0.1212\n",
            "k-fold 0  Train Loss: 0.1036, Validation Loss: 0.1187\n",
            "k-fold 0  Train Loss: 0.1019, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.1006, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1002, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1002, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1005, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1009, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1011, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1010, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1008, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1006, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1005, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.1008, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.1011, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.1014, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.1018, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.1023, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.1029, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.1036, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.1040, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.1045, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1048, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1049, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1049, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.1047, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.1046, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1039, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.1033, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.1032, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.1030, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.1026, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.1023, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.1015, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.1008, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.1004, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.1002, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1000, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.1000, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.0999, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.0998, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.0996, Validation Loss: 0.1133\n",
            "k-fold 0  Train Loss: 0.0994, Validation Loss: 0.1130\n",
            "k-fold 0  Train Loss: 0.0992, Validation Loss: 0.1128\n",
            "k-fold 0  Train Loss: 0.0991, Validation Loss: 0.1129\n",
            "k-fold 0  Train Loss: 0.0994, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.1001, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.1009, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.1018, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.1046, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.1076, Validation Loss: 0.1225\n",
            "k-fold 0  Train Loss: 0.1103, Validation Loss: 0.1251\n",
            "k-fold 0  Train Loss: 0.1125, Validation Loss: 0.1272\n",
            "k-fold 0  Train Loss: 0.1146, Validation Loss: 0.1292\n",
            "k-fold 0  Train Loss: 0.1156, Validation Loss: 0.1303\n",
            "k-fold 0  Train Loss: 0.1154, Validation Loss: 0.1302\n",
            "k-fold 0  Train Loss: 0.1143, Validation Loss: 0.1294\n",
            "k-fold 0  Train Loss: 0.1115, Validation Loss: 0.1272\n",
            "k-fold 0  Train Loss: 0.1086, Validation Loss: 0.1247\n",
            "k-fold 0  Train Loss: 0.1058, Validation Loss: 0.1223\n",
            "k-fold 0  Train Loss: 0.1034, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.1022, Validation Loss: 0.1189\n",
            "k-fold 0  Train Loss: 0.1016, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.1006, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.1001, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.0998, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0996, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0993, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0989, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.0987, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0982, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0982, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0981, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.0980, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.0978, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1130\n",
            "k-fold 0  Train Loss: 0.0989, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.0992, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.0991, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.0990, Validation Loss: 0.1138\n",
            "k-fold 0  Train Loss: 0.0990, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.0987, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.0989, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0986, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0982, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0982, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0988, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0990, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0990, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0988, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0981, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.0978, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0977, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0975, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.0974, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0971, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0970, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0968, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0967, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.0966, Validation Loss: 0.1139\n",
            "k-fold 0  Train Loss: 0.0964, Validation Loss: 0.1138\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1138\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.0962, Validation Loss: 0.1140\n",
            "k-fold 0  Train Loss: 0.0964, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0967, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0971, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0975, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0979, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0983, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0987, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0988, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0987, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0986, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0984, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0982, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.0976, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0972, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0968, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.0962, Validation Loss: 0.1134\n",
            "k-fold 0  Train Loss: 0.0962, Validation Loss: 0.1135\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1138\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0966, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0974, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0985, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0981, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0976, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0976, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.0976, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0969, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0967, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0970, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0969, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0966, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0962, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0964, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0966, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0962, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0959, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0958, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0955, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0955, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0959, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0961, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0970, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0971, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0970, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.0944, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.0937, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0938, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0938, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0936, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0935, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0934, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0934, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0938, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0942, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0947, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0948, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0956, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0954, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0954, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0952, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.0952, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0950, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0947, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0945, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0942, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0937, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0936, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0935, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0937, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0940, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0941, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0935, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0930, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0927, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0928, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0928, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0929, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0929, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0932, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0937, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0944, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0953, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0955, Validation Loss: 0.1187\n",
            "k-fold 0  Train Loss: 0.0954, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0952, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.0951, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0950, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0948, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.0947, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0945, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0945, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0933, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0929, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0934, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0941, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0952, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0959, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0957, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.0949, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0944, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0941, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0934, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0929, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0932, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0943, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0958, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.0965, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.0967, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.0968, Validation Loss: 0.1212\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1205\n",
            "k-fold 0  Train Loss: 0.0951, Validation Loss: 0.1196\n",
            "k-fold 0  Train Loss: 0.0947, Validation Loss: 0.1190\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0929, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0927, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0922, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0920, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0923, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0927, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0928, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0935, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0941, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0944, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0943, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.0942, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0934, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0929, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0927, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0923, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0919, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0915, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0914, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.0915, Validation Loss: 0.1149\n",
            "k-fold 0  Train Loss: 0.0917, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0919, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0922, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0927, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0932, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.0936, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0941, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0943, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.0952, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.0958, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.0960, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.0963, Validation Loss: 0.1202\n",
            "k-fold 0  Train Loss: 0.0952, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.0940, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0931, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0919, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0914, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0910, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0907, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0905, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0905, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0906, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0910, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0904, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0902, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0899, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0897, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0893, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0893, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0893, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0895, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0893, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0892, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0890, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0890, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0892, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.0897, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0903, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0907, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0916, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.0928, Validation Loss: 0.1190\n",
            "k-fold 0  Train Loss: 0.0932, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.0936, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.0937, Validation Loss: 0.1201\n",
            "k-fold 0  Train Loss: 0.0935, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.0931, Validation Loss: 0.1197\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.0917, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0901, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0897, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0895, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0895, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0895, Validation Loss: 0.1162\n",
            "k-fold 0  Train Loss: 0.0894, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0894, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0897, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0902, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0916, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0924, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0930, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0934, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0936, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0938, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0939, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0936, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.0932, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0931, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0931, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0928, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0923, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.0918, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0914, Validation Loss: 0.1141\n",
            "k-fold 0  Train Loss: 0.0911, Validation Loss: 0.1137\n",
            "k-fold 0  Train Loss: 0.0905, Validation Loss: 0.1132\n",
            "k-fold 0  Train Loss: 0.0900, Validation Loss: 0.1125\n",
            "k-fold 0  Train Loss: 0.0898, Validation Loss: 0.1121\n",
            "k-fold 0  Train Loss: 0.0896, Validation Loss: 0.1120\n",
            "k-fold 0  Train Loss: 0.0896, Validation Loss: 0.1120\n",
            "k-fold 0  Train Loss: 0.0895, Validation Loss: 0.1121\n",
            "k-fold 0  Train Loss: 0.0890, Validation Loss: 0.1124\n",
            "k-fold 0  Train Loss: 0.0885, Validation Loss: 0.1128\n",
            "k-fold 0  Train Loss: 0.0881, Validation Loss: 0.1131\n",
            "k-fold 0  Train Loss: 0.0879, Validation Loss: 0.1136\n",
            "k-fold 0  Train Loss: 0.0879, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0881, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.0881, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.0889, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0900, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.0906, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0910, Validation Loss: 0.1191\n",
            "k-fold 0  Train Loss: 0.0918, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.0927, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.0926, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.0919, Validation Loss: 0.1197\n",
            "k-fold 0  Train Loss: 0.0912, Validation Loss: 0.1191\n",
            "k-fold 0  Train Loss: 0.0907, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0900, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0898, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0895, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0894, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0889, Validation Loss: 0.1168\n",
            "k-fold 0  Train Loss: 0.0884, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0875, Validation Loss: 0.1152\n",
            "k-fold 0  Train Loss: 0.0874, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0873, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0873, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0875, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0879, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0883, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0885, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0883, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0881, Validation Loss: 0.1142\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1143\n",
            "k-fold 0  Train Loss: 0.0878, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0875, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0874, Validation Loss: 0.1148\n",
            "k-fold 0  Train Loss: 0.0874, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0873, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0872, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0872, Validation Loss: 0.1160\n",
            "k-fold 0  Train Loss: 0.0872, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0874, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0877, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0872, Validation Loss: 0.1165\n",
            "k-fold 0  Train Loss: 0.0868, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0867, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0866, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1157\n",
            "k-fold 0  Train Loss: 0.0864, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0869, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0877, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0879, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0879, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.0878, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0871, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0866, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1153\n",
            "k-fold 0  Train Loss: 0.0864, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0870, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1154\n",
            "k-fold 0  Train Loss: 0.0886, Validation Loss: 0.1156\n",
            "k-fold 0  Train Loss: 0.0896, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0900, Validation Loss: 0.1158\n",
            "k-fold 0  Train Loss: 0.0891, Validation Loss: 0.1151\n",
            "k-fold 0  Train Loss: 0.0878, Validation Loss: 0.1144\n",
            "k-fold 0  Train Loss: 0.0877, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0869, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0859, Validation Loss: 0.1146\n",
            "k-fold 0  Train Loss: 0.0856, Validation Loss: 0.1145\n",
            "k-fold 0  Train Loss: 0.0857, Validation Loss: 0.1147\n",
            "k-fold 0  Train Loss: 0.0858, Validation Loss: 0.1150\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1155\n",
            "k-fold 0  Train Loss: 0.0865, Validation Loss: 0.1159\n",
            "k-fold 0  Train Loss: 0.0870, Validation Loss: 0.1164\n",
            "k-fold 0  Train Loss: 0.0876, Validation Loss: 0.1169\n",
            "k-fold 0  Train Loss: 0.0878, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0881, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0881, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0879, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0873, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.0868, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0866, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0865, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0865, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0867, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0870, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0873, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.0875, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0877, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0877, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.0878, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0884, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0890, Validation Loss: 0.1191\n",
            "k-fold 0  Train Loss: 0.0896, Validation Loss: 0.1194\n",
            "k-fold 0  Train Loss: 0.0902, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.0909, Validation Loss: 0.1202\n",
            "k-fold 0  Train Loss: 0.0915, Validation Loss: 0.1206\n",
            "k-fold 0  Train Loss: 0.0921, Validation Loss: 0.1209\n",
            "k-fold 0  Train Loss: 0.0922, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.0917, Validation Loss: 0.1202\n",
            "k-fold 0  Train Loss: 0.0914, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.0907, Validation Loss: 0.1191\n",
            "k-fold 0  Train Loss: 0.0902, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0900, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0899, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.0891, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0890, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0886, Validation Loss: 0.1171\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1166\n",
            "k-fold 0  Train Loss: 0.0873, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0878, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0884, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0886, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0891, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0892, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0892, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0887, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0885, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0870, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0860, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0856, Validation Loss: 0.1161\n",
            "k-fold 0  Train Loss: 0.0856, Validation Loss: 0.1163\n",
            "k-fold 0  Train Loss: 0.0860, Validation Loss: 0.1167\n",
            "k-fold 0  Train Loss: 0.0866, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0872, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0875, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0856, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0847, Validation Loss: 0.1172\n",
            "k-fold 0  Train Loss: 0.0846, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0851, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0855, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0859, Validation Loss: 0.1186\n",
            "k-fold 0  Train Loss: 0.0862, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.0864, Validation Loss: 0.1189\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1189\n",
            "k-fold 0  Train Loss: 0.0860, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.0855, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0853, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0851, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0849, Validation Loss: 0.1173\n",
            "k-fold 0  Train Loss: 0.0846, Validation Loss: 0.1170\n",
            "k-fold 0  Train Loss: 0.0851, Validation Loss: 0.1174\n",
            "k-fold 0  Train Loss: 0.0855, Validation Loss: 0.1178\n",
            "k-fold 0  Train Loss: 0.0858, Validation Loss: 0.1180\n",
            "k-fold 0  Train Loss: 0.0860, Validation Loss: 0.1181\n",
            "k-fold 0  Train Loss: 0.0861, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0865, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0862, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0865, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0868, Validation Loss: 0.1182\n",
            "k-fold 0  Train Loss: 0.0872, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0874, Validation Loss: 0.1183\n",
            "k-fold 0  Train Loss: 0.0880, Validation Loss: 0.1188\n",
            "k-fold 0  Train Loss: 0.0888, Validation Loss: 0.1193\n",
            "k-fold 0  Train Loss: 0.0894, Validation Loss: 0.1197\n",
            "k-fold 0  Train Loss: 0.0896, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.0900, Validation Loss: 0.1202\n",
            "k-fold 0  Train Loss: 0.0903, Validation Loss: 0.1203\n",
            "k-fold 0  Train Loss: 0.0907, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.0907, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.0908, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.0909, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.0913, Validation Loss: 0.1207\n",
            "k-fold 0  Train Loss: 0.0909, Validation Loss: 0.1205\n",
            "k-fold 0  Train Loss: 0.0910, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.0909, Validation Loss: 0.1210\n",
            "k-fold 0  Train Loss: 0.0904, Validation Loss: 0.1208\n",
            "k-fold 0  Train Loss: 0.0898, Validation Loss: 0.1205\n",
            "k-fold 0  Train Loss: 0.0890, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.0877, Validation Loss: 0.1192\n",
            "k-fold 0  Train Loss: 0.0869, Validation Loss: 0.1187\n",
            "k-fold 0  Train Loss: 0.0863, Validation Loss: 0.1184\n",
            "k-fold 0  Train Loss: 0.0845, Validation Loss: 0.1177\n",
            "k-fold 0  Train Loss: 0.0838, Validation Loss: 0.1175\n",
            "k-fold 0  Train Loss: 0.0838, Validation Loss: 0.1176\n",
            "k-fold 0  Train Loss: 0.0844, Validation Loss: 0.1179\n",
            "k-fold 0  Train Loss: 0.0853, Validation Loss: 0.1185\n",
            "k-fold 0  Train Loss: 0.0862, Validation Loss: 0.1192\n",
            "k-fold 0  Train Loss: 0.0867, Validation Loss: 0.1197\n",
            "k-fold 0  Train Loss: 0.0866, Validation Loss: 0.1199\n",
            "k-fold 0  Train Loss: 0.0858, Validation Loss: 0.1198\n",
            "k-fold 0  Train Loss: 0.0854, Validation Loss: 0.1200\n",
            "k-fold 0  Train Loss: 0.0854, Validation Loss: 0.1204\n",
            "k-fold 0  Train Loss: 0.0861, Validation Loss: 0.1211\n",
            "k-fold 0  Train Loss: 0.0868, Validation Loss: 0.1216\n",
            "k-fold 1  Train Loss: 0.3835, Validation Loss: 0.3814\n",
            "k-fold 1  Train Loss: 0.3683, Validation Loss: 0.3660\n",
            "k-fold 1  Train Loss: 0.3533, Validation Loss: 0.3506\n",
            "k-fold 1  Train Loss: 0.3399, Validation Loss: 0.3368\n",
            "k-fold 1  Train Loss: 0.3265, Validation Loss: 0.3229\n",
            "k-fold 1  Train Loss: 0.3133, Validation Loss: 0.3093\n",
            "k-fold 1  Train Loss: 0.3003, Validation Loss: 0.2957\n",
            "k-fold 1  Train Loss: 0.2879, Validation Loss: 0.2826\n",
            "k-fold 1  Train Loss: 0.2761, Validation Loss: 0.2699\n",
            "k-fold 1  Train Loss: 0.2650, Validation Loss: 0.2579\n",
            "k-fold 1  Train Loss: 0.2553, Validation Loss: 0.2474\n",
            "k-fold 1  Train Loss: 0.2465, Validation Loss: 0.2374\n",
            "k-fold 1  Train Loss: 0.2388, Validation Loss: 0.2283\n",
            "k-fold 1  Train Loss: 0.2321, Validation Loss: 0.2199\n",
            "k-fold 1  Train Loss: 0.2267, Validation Loss: 0.2126\n",
            "k-fold 1  Train Loss: 0.2220, Validation Loss: 0.2062\n",
            "k-fold 1  Train Loss: 0.2179, Validation Loss: 0.2000\n",
            "k-fold 1  Train Loss: 0.2147, Validation Loss: 0.1950\n",
            "k-fold 1  Train Loss: 0.2122, Validation Loss: 0.1911\n",
            "k-fold 1  Train Loss: 0.2101, Validation Loss: 0.1873\n",
            "k-fold 1  Train Loss: 0.2085, Validation Loss: 0.1842\n",
            "k-fold 1  Train Loss: 0.2073, Validation Loss: 0.1817\n",
            "k-fold 1  Train Loss: 0.2064, Validation Loss: 0.1797\n",
            "k-fold 1  Train Loss: 0.2054, Validation Loss: 0.1783\n",
            "k-fold 1  Train Loss: 0.2045, Validation Loss: 0.1771\n",
            "k-fold 1  Train Loss: 0.2036, Validation Loss: 0.1761\n",
            "k-fold 1  Train Loss: 0.2026, Validation Loss: 0.1750\n",
            "k-fold 1  Train Loss: 0.2017, Validation Loss: 0.1742\n",
            "k-fold 1  Train Loss: 0.2008, Validation Loss: 0.1733\n",
            "k-fold 1  Train Loss: 0.1999, Validation Loss: 0.1726\n",
            "k-fold 1  Train Loss: 0.1991, Validation Loss: 0.1720\n",
            "k-fold 1  Train Loss: 0.1983, Validation Loss: 0.1714\n",
            "k-fold 1  Train Loss: 0.1976, Validation Loss: 0.1708\n",
            "k-fold 1  Train Loss: 0.1969, Validation Loss: 0.1703\n",
            "k-fold 1  Train Loss: 0.1963, Validation Loss: 0.1698\n",
            "k-fold 1  Train Loss: 0.1956, Validation Loss: 0.1691\n",
            "k-fold 1  Train Loss: 0.1950, Validation Loss: 0.1686\n",
            "k-fold 1  Train Loss: 0.1944, Validation Loss: 0.1681\n",
            "k-fold 1  Train Loss: 0.1938, Validation Loss: 0.1675\n",
            "k-fold 1  Train Loss: 0.1932, Validation Loss: 0.1669\n",
            "k-fold 1  Train Loss: 0.1925, Validation Loss: 0.1664\n",
            "k-fold 1  Train Loss: 0.1918, Validation Loss: 0.1657\n",
            "k-fold 1  Train Loss: 0.1912, Validation Loss: 0.1651\n",
            "k-fold 1  Train Loss: 0.1905, Validation Loss: 0.1643\n",
            "k-fold 1  Train Loss: 0.1899, Validation Loss: 0.1637\n",
            "k-fold 1  Train Loss: 0.1893, Validation Loss: 0.1632\n",
            "k-fold 1  Train Loss: 0.1888, Validation Loss: 0.1626\n",
            "k-fold 1  Train Loss: 0.1881, Validation Loss: 0.1618\n",
            "k-fold 1  Train Loss: 0.1874, Validation Loss: 0.1610\n",
            "k-fold 1  Train Loss: 0.1868, Validation Loss: 0.1603\n",
            "k-fold 1  Train Loss: 0.1861, Validation Loss: 0.1594\n",
            "k-fold 1  Train Loss: 0.1856, Validation Loss: 0.1589\n",
            "k-fold 1  Train Loss: 0.1852, Validation Loss: 0.1584\n",
            "k-fold 1  Train Loss: 0.1848, Validation Loss: 0.1580\n",
            "k-fold 1  Train Loss: 0.1842, Validation Loss: 0.1574\n",
            "k-fold 1  Train Loss: 0.1837, Validation Loss: 0.1566\n",
            "k-fold 1  Train Loss: 0.1830, Validation Loss: 0.1558\n",
            "k-fold 1  Train Loss: 0.1826, Validation Loss: 0.1553\n",
            "k-fold 1  Train Loss: 0.1821, Validation Loss: 0.1547\n",
            "k-fold 1  Train Loss: 0.1814, Validation Loss: 0.1540\n",
            "k-fold 1  Train Loss: 0.1807, Validation Loss: 0.1531\n",
            "k-fold 1  Train Loss: 0.1800, Validation Loss: 0.1523\n",
            "k-fold 1  Train Loss: 0.1795, Validation Loss: 0.1517\n",
            "k-fold 1  Train Loss: 0.1791, Validation Loss: 0.1512\n",
            "k-fold 1  Train Loss: 0.1787, Validation Loss: 0.1508\n",
            "k-fold 1  Train Loss: 0.1784, Validation Loss: 0.1505\n",
            "k-fold 1  Train Loss: 0.1779, Validation Loss: 0.1500\n",
            "k-fold 1  Train Loss: 0.1776, Validation Loss: 0.1497\n",
            "k-fold 1  Train Loss: 0.1775, Validation Loss: 0.1495\n",
            "k-fold 1  Train Loss: 0.1773, Validation Loss: 0.1493\n",
            "k-fold 1  Train Loss: 0.1769, Validation Loss: 0.1488\n",
            "k-fold 1  Train Loss: 0.1765, Validation Loss: 0.1484\n",
            "k-fold 1  Train Loss: 0.1760, Validation Loss: 0.1478\n",
            "k-fold 1  Train Loss: 0.1756, Validation Loss: 0.1473\n",
            "k-fold 1  Train Loss: 0.1753, Validation Loss: 0.1470\n",
            "k-fold 1  Train Loss: 0.1751, Validation Loss: 0.1467\n",
            "k-fold 1  Train Loss: 0.1748, Validation Loss: 0.1465\n",
            "k-fold 1  Train Loss: 0.1746, Validation Loss: 0.1462\n",
            "k-fold 1  Train Loss: 0.1741, Validation Loss: 0.1457\n",
            "k-fold 1  Train Loss: 0.1737, Validation Loss: 0.1452\n",
            "k-fold 1  Train Loss: 0.1732, Validation Loss: 0.1447\n",
            "k-fold 1  Train Loss: 0.1726, Validation Loss: 0.1440\n",
            "k-fold 1  Train Loss: 0.1721, Validation Loss: 0.1434\n",
            "k-fold 1  Train Loss: 0.1715, Validation Loss: 0.1428\n",
            "k-fold 1  Train Loss: 0.1708, Validation Loss: 0.1421\n",
            "k-fold 1  Train Loss: 0.1702, Validation Loss: 0.1415\n",
            "k-fold 1  Train Loss: 0.1698, Validation Loss: 0.1411\n",
            "k-fold 1  Train Loss: 0.1696, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.1695, Validation Loss: 0.1407\n",
            "k-fold 1  Train Loss: 0.1692, Validation Loss: 0.1404\n",
            "k-fold 1  Train Loss: 0.1690, Validation Loss: 0.1400\n",
            "k-fold 1  Train Loss: 0.1690, Validation Loss: 0.1401\n",
            "k-fold 1  Train Loss: 0.1691, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.1691, Validation Loss: 0.1403\n",
            "k-fold 1  Train Loss: 0.1691, Validation Loss: 0.1404\n",
            "k-fold 1  Train Loss: 0.1691, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.1689, Validation Loss: 0.1405\n",
            "k-fold 1  Train Loss: 0.1685, Validation Loss: 0.1401\n",
            "k-fold 1  Train Loss: 0.1682, Validation Loss: 0.1399\n",
            "k-fold 1  Train Loss: 0.1684, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.1683, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.1684, Validation Loss: 0.1404\n",
            "k-fold 1  Train Loss: 0.1683, Validation Loss: 0.1405\n",
            "k-fold 1  Train Loss: 0.1683, Validation Loss: 0.1407\n",
            "k-fold 1  Train Loss: 0.1683, Validation Loss: 0.1408\n",
            "k-fold 1  Train Loss: 0.1683, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.1681, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.1684, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.1686, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.1687, Validation Loss: 0.1416\n",
            "k-fold 1  Train Loss: 0.1684, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.1681, Validation Loss: 0.1411\n",
            "k-fold 1  Train Loss: 0.1679, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.1674, Validation Loss: 0.1404\n",
            "k-fold 1  Train Loss: 0.1669, Validation Loss: 0.1399\n",
            "k-fold 1  Train Loss: 0.1674, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.1679, Validation Loss: 0.1411\n",
            "k-fold 1  Train Loss: 0.1682, Validation Loss: 0.1416\n",
            "k-fold 1  Train Loss: 0.1680, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.1664, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.1653, Validation Loss: 0.1394\n",
            "k-fold 1  Train Loss: 0.1642, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.1632, Validation Loss: 0.1378\n",
            "k-fold 1  Train Loss: 0.1625, Validation Loss: 0.1374\n",
            "k-fold 1  Train Loss: 0.1620, Validation Loss: 0.1371\n",
            "k-fold 1  Train Loss: 0.1617, Validation Loss: 0.1370\n",
            "k-fold 1  Train Loss: 0.1610, Validation Loss: 0.1364\n",
            "k-fold 1  Train Loss: 0.1608, Validation Loss: 0.1364\n",
            "k-fold 1  Train Loss: 0.1609, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.1609, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.1606, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.1604, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.1603, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.1600, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.1597, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.1601, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.1604, Validation Loss: 0.1370\n",
            "k-fold 1  Train Loss: 0.1606, Validation Loss: 0.1373\n",
            "k-fold 1  Train Loss: 0.1610, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.1623, Validation Loss: 0.1390\n",
            "k-fold 1  Train Loss: 0.1635, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.1646, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.1650, Validation Loss: 0.1420\n",
            "k-fold 1  Train Loss: 0.1655, Validation Loss: 0.1425\n",
            "k-fold 1  Train Loss: 0.1657, Validation Loss: 0.1428\n",
            "k-fold 1  Train Loss: 0.1651, Validation Loss: 0.1423\n",
            "k-fold 1  Train Loss: 0.1645, Validation Loss: 0.1419\n",
            "k-fold 1  Train Loss: 0.1641, Validation Loss: 0.1416\n",
            "k-fold 1  Train Loss: 0.1638, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.1634, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.1628, Validation Loss: 0.1407\n",
            "k-fold 1  Train Loss: 0.1626, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.1622, Validation Loss: 0.1403\n",
            "k-fold 1  Train Loss: 0.1620, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.1617, Validation Loss: 0.1400\n",
            "k-fold 1  Train Loss: 0.1627, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.1639, Validation Loss: 0.1426\n",
            "k-fold 1  Train Loss: 0.1649, Validation Loss: 0.1439\n",
            "k-fold 1  Train Loss: 0.1651, Validation Loss: 0.1442\n",
            "k-fold 1  Train Loss: 0.1639, Validation Loss: 0.1430\n",
            "k-fold 1  Train Loss: 0.1622, Validation Loss: 0.1411\n",
            "k-fold 1  Train Loss: 0.1606, Validation Loss: 0.1395\n",
            "k-fold 1  Train Loss: 0.1597, Validation Loss: 0.1385\n",
            "k-fold 1  Train Loss: 0.1579, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.1564, Validation Loss: 0.1350\n",
            "k-fold 1  Train Loss: 0.1553, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.1546, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.1542, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.1538, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.1534, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.1530, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.1529, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.1529, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.1529, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.1530, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.1528, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.1527, Validation Loss: 0.1315\n",
            "k-fold 1  Train Loss: 0.1526, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.1526, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.1519, Validation Loss: 0.1306\n",
            "k-fold 1  Train Loss: 0.1514, Validation Loss: 0.1301\n",
            "k-fold 1  Train Loss: 0.1509, Validation Loss: 0.1296\n",
            "k-fold 1  Train Loss: 0.1504, Validation Loss: 0.1292\n",
            "k-fold 1  Train Loss: 0.1499, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.1495, Validation Loss: 0.1285\n",
            "k-fold 1  Train Loss: 0.1491, Validation Loss: 0.1282\n",
            "k-fold 1  Train Loss: 0.1488, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.1485, Validation Loss: 0.1278\n",
            "k-fold 1  Train Loss: 0.1483, Validation Loss: 0.1277\n",
            "k-fold 1  Train Loss: 0.1482, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1482, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1478, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.1475, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.1472, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1471, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1469, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1468, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1467, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1465, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1463, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1462, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1462, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1462, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1460, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1456, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1454, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1451, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1448, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1445, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1442, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1440, Validation Loss: 0.1253\n",
            "k-fold 1  Train Loss: 0.1440, Validation Loss: 0.1251\n",
            "k-fold 1  Train Loss: 0.1442, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1444, Validation Loss: 0.1249\n",
            "k-fold 1  Train Loss: 0.1443, Validation Loss: 0.1249\n",
            "k-fold 1  Train Loss: 0.1446, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1449, Validation Loss: 0.1251\n",
            "k-fold 1  Train Loss: 0.1454, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1456, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1456, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1456, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1454, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1454, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1453, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1452, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1450, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1447, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1447, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1443, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1439, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1436, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1429, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1423, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1419, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1415, Validation Loss: 0.1251\n",
            "k-fold 1  Train Loss: 0.1414, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1413, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1412, Validation Loss: 0.1253\n",
            "k-fold 1  Train Loss: 0.1412, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1415, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1418, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1421, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1425, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1435, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.1442, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1445, Validation Loss: 0.1292\n",
            "k-fold 1  Train Loss: 0.1445, Validation Loss: 0.1294\n",
            "k-fold 1  Train Loss: 0.1443, Validation Loss: 0.1294\n",
            "k-fold 1  Train Loss: 0.1439, Validation Loss: 0.1291\n",
            "k-fold 1  Train Loss: 0.1429, Validation Loss: 0.1283\n",
            "k-fold 1  Train Loss: 0.1421, Validation Loss: 0.1277\n",
            "k-fold 1  Train Loss: 0.1419, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1417, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1415, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1412, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1406, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1401, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1398, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1391, Validation Loss: 0.1253\n",
            "k-fold 1  Train Loss: 0.1382, Validation Loss: 0.1247\n",
            "k-fold 1  Train Loss: 0.1375, Validation Loss: 0.1245\n",
            "k-fold 1  Train Loss: 0.1369, Validation Loss: 0.1242\n",
            "k-fold 1  Train Loss: 0.1364, Validation Loss: 0.1241\n",
            "k-fold 1  Train Loss: 0.1366, Validation Loss: 0.1242\n",
            "k-fold 1  Train Loss: 0.1370, Validation Loss: 0.1246\n",
            "k-fold 1  Train Loss: 0.1375, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1385, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1388, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1389, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1391, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1394, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1400, Validation Loss: 0.1284\n",
            "k-fold 1  Train Loss: 0.1407, Validation Loss: 0.1291\n",
            "k-fold 1  Train Loss: 0.1411, Validation Loss: 0.1297\n",
            "k-fold 1  Train Loss: 0.1414, Validation Loss: 0.1301\n",
            "k-fold 1  Train Loss: 0.1423, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.1427, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.1428, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.1428, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.1420, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.1410, Validation Loss: 0.1302\n",
            "k-fold 1  Train Loss: 0.1400, Validation Loss: 0.1293\n",
            "k-fold 1  Train Loss: 0.1393, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.1378, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1370, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1367, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1366, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1369, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1372, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1373, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1371, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1366, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.1360, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1355, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1353, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1357, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1366, Validation Loss: 0.1278\n",
            "k-fold 1  Train Loss: 0.1376, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.1388, Validation Loss: 0.1298\n",
            "k-fold 1  Train Loss: 0.1395, Validation Loss: 0.1304\n",
            "k-fold 1  Train Loss: 0.1404, Validation Loss: 0.1312\n",
            "k-fold 1  Train Loss: 0.1406, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.1408, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.1398, Validation Loss: 0.1308\n",
            "k-fold 1  Train Loss: 0.1385, Validation Loss: 0.1297\n",
            "k-fold 1  Train Loss: 0.1371, Validation Loss: 0.1286\n",
            "k-fold 1  Train Loss: 0.1352, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1334, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1316, Validation Loss: 0.1241\n",
            "k-fold 1  Train Loss: 0.1304, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1297, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1294, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1292, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1289, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1287, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1285, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1285, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1288, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1290, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1300, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1314, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1333, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1353, Validation Loss: 0.1286\n",
            "k-fold 1  Train Loss: 0.1349, Validation Loss: 0.1283\n",
            "k-fold 1  Train Loss: 0.1344, Validation Loss: 0.1279\n",
            "k-fold 1  Train Loss: 0.1335, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1323, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1318, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1315, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1308, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1298, Validation Loss: 0.1244\n",
            "k-fold 1  Train Loss: 0.1286, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1278, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1269, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1261, Validation Loss: 0.1220\n",
            "k-fold 1  Train Loss: 0.1251, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1247, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1246, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1246, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1248, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1249, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1247, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1244, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1241, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1239, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1236, Validation Loss: 0.1218\n",
            "k-fold 1  Train Loss: 0.1236, Validation Loss: 0.1214\n",
            "k-fold 1  Train Loss: 0.1241, Validation Loss: 0.1213\n",
            "k-fold 1  Train Loss: 0.1249, Validation Loss: 0.1215\n",
            "k-fold 1  Train Loss: 0.1257, Validation Loss: 0.1217\n",
            "k-fold 1  Train Loss: 0.1265, Validation Loss: 0.1220\n",
            "k-fold 1  Train Loss: 0.1292, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1320, Validation Loss: 0.1253\n",
            "k-fold 1  Train Loss: 0.1341, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1362, Validation Loss: 0.1285\n",
            "k-fold 1  Train Loss: 0.1395, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.1424, Validation Loss: 0.1339\n",
            "k-fold 1  Train Loss: 0.1440, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.1435, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.1422, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.1393, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.1362, Validation Loss: 0.1304\n",
            "k-fold 1  Train Loss: 0.1331, Validation Loss: 0.1283\n",
            "k-fold 1  Train Loss: 0.1305, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1279, Validation Loss: 0.1249\n",
            "k-fold 1  Train Loss: 0.1256, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1236, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1222, Validation Loss: 0.1218\n",
            "k-fold 1  Train Loss: 0.1215, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1214, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1215, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1216, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1218, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1215, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1210, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1206, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1205, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1209, Validation Loss: 0.1220\n",
            "k-fold 1  Train Loss: 0.1216, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1227, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1237, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1246, Validation Loss: 0.1246\n",
            "k-fold 1  Train Loss: 0.1252, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1252, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1247, Validation Loss: 0.1246\n",
            "k-fold 1  Train Loss: 0.1242, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1233, Validation Loss: 0.1237\n",
            "k-fold 1  Train Loss: 0.1232, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1232, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1230, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1225, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1214, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1202, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1193, Validation Loss: 0.1215\n",
            "k-fold 1  Train Loss: 0.1189, Validation Loss: 0.1214\n",
            "k-fold 1  Train Loss: 0.1182, Validation Loss: 0.1213\n",
            "k-fold 1  Train Loss: 0.1178, Validation Loss: 0.1214\n",
            "k-fold 1  Train Loss: 0.1174, Validation Loss: 0.1217\n",
            "k-fold 1  Train Loss: 0.1173, Validation Loss: 0.1218\n",
            "k-fold 1  Train Loss: 0.1175, Validation Loss: 0.1216\n",
            "k-fold 1  Train Loss: 0.1182, Validation Loss: 0.1216\n",
            "k-fold 1  Train Loss: 0.1192, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1201, Validation Loss: 0.1223\n",
            "k-fold 1  Train Loss: 0.1194, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1186, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1179, Validation Loss: 0.1220\n",
            "k-fold 1  Train Loss: 0.1175, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1173, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1170, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1168, Validation Loss: 0.1223\n",
            "k-fold 1  Train Loss: 0.1168, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1166, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1165, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1164, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1164, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1164, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1163, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1163, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1162, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1156, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1154, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1152, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1150, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1148, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1148, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1149, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1151, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1152, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1153, Validation Loss: 0.1223\n",
            "k-fold 1  Train Loss: 0.1153, Validation Loss: 0.1223\n",
            "k-fold 1  Train Loss: 0.1151, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1150, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1148, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1145, Validation Loss: 0.1222\n",
            "k-fold 1  Train Loss: 0.1146, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1149, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1149, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1149, Validation Loss: 0.1218\n",
            "k-fold 1  Train Loss: 0.1150, Validation Loss: 0.1217\n",
            "k-fold 1  Train Loss: 0.1156, Validation Loss: 0.1218\n",
            "k-fold 1  Train Loss: 0.1159, Validation Loss: 0.1219\n",
            "k-fold 1  Train Loss: 0.1162, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1161, Validation Loss: 0.1221\n",
            "k-fold 1  Train Loss: 0.1162, Validation Loss: 0.1223\n",
            "k-fold 1  Train Loss: 0.1160, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1157, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1153, Validation Loss: 0.1224\n",
            "k-fold 1  Train Loss: 0.1152, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1152, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1150, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1152, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1148, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1142, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1137, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1132, Validation Loss: 0.1225\n",
            "k-fold 1  Train Loss: 0.1130, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1129, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1127, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1125, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1122, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1120, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1119, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1120, Validation Loss: 0.1244\n",
            "k-fold 1  Train Loss: 0.1118, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1118, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1123, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1126, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1130, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1129, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1131, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1138, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1157, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1176, Validation Loss: 0.1249\n",
            "k-fold 1  Train Loss: 0.1186, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1194, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1186, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1180, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1172, Validation Loss: 0.1248\n",
            "k-fold 1  Train Loss: 0.1154, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1151, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1145, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1141, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1137, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1131, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1126, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1123, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1122, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1119, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1113, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1109, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1105, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1103, Validation Loss: 0.1241\n",
            "k-fold 1  Train Loss: 0.1103, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1102, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1102, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1104, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1107, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1109, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1108, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1114, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1119, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1120, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1122, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1116, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1114, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1110, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1103, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1104, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1104, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1105, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1102, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1104, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1108, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1111, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1115, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1122, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1124, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1127, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1131, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1110, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1097, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1086, Validation Loss: 0.1251\n",
            "k-fold 1  Train Loss: 0.1082, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1082, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1084, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1086, Validation Loss: 0.1278\n",
            "k-fold 1  Train Loss: 0.1087, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.1087, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1089, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1094, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1102, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1114, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1123, Validation Loss: 0.1263\n",
            "k-fold 1  Train Loss: 0.1139, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1149, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.1159, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1159, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.1160, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1159, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1149, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1142, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1130, Validation Loss: 0.1245\n",
            "k-fold 1  Train Loss: 0.1120, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1109, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1100, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1091, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1085, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1081, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1078, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1076, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1075, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1073, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1073, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1072, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1071, Validation Loss: 0.1241\n",
            "k-fold 1  Train Loss: 0.1073, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1075, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1078, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1080, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1076, Validation Loss: 0.1233\n",
            "k-fold 1  Train Loss: 0.1071, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1069, Validation Loss: 0.1242\n",
            "k-fold 1  Train Loss: 0.1069, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1076, Validation Loss: 0.1237\n",
            "k-fold 1  Train Loss: 0.1087, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1098, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1108, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1136, Validation Loss: 0.1246\n",
            "k-fold 1  Train Loss: 0.1165, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1186, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1199, Validation Loss: 0.1278\n",
            "k-fold 1  Train Loss: 0.1211, Validation Loss: 0.1287\n",
            "k-fold 1  Train Loss: 0.1212, Validation Loss: 0.1290\n",
            "k-fold 1  Train Loss: 0.1206, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1187, Validation Loss: 0.1281\n",
            "k-fold 1  Train Loss: 0.1173, Validation Loss: 0.1277\n",
            "k-fold 1  Train Loss: 0.1153, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1141, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1122, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1108, Validation Loss: 0.1257\n",
            "k-fold 1  Train Loss: 0.1099, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1085, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1077, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1073, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1070, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1069, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1067, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1067, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1066, Validation Loss: 0.1277\n",
            "k-fold 1  Train Loss: 0.1063, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1060, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1056, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1056, Validation Loss: 0.1257\n",
            "k-fold 1  Train Loss: 0.1059, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1064, Validation Loss: 0.1245\n",
            "k-fold 1  Train Loss: 0.1074, Validation Loss: 0.1241\n",
            "k-fold 1  Train Loss: 0.1083, Validation Loss: 0.1239\n",
            "k-fold 1  Train Loss: 0.1092, Validation Loss: 0.1237\n",
            "k-fold 1  Train Loss: 0.1101, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1098, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1095, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1093, Validation Loss: 0.1238\n",
            "k-fold 1  Train Loss: 0.1084, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1078, Validation Loss: 0.1242\n",
            "k-fold 1  Train Loss: 0.1073, Validation Loss: 0.1245\n",
            "k-fold 1  Train Loss: 0.1064, Validation Loss: 0.1250\n",
            "k-fold 1  Train Loss: 0.1058, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1057, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1056, Validation Loss: 0.1255\n",
            "k-fold 1  Train Loss: 0.1052, Validation Loss: 0.1257\n",
            "k-fold 1  Train Loss: 0.1048, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1048, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1049, Validation Loss: 0.1259\n",
            "k-fold 1  Train Loss: 0.1051, Validation Loss: 0.1257\n",
            "k-fold 1  Train Loss: 0.1054, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1051, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1050, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1053, Validation Loss: 0.1263\n",
            "k-fold 1  Train Loss: 0.1051, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1057, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1064, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1064, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1069, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1084, Validation Loss: 0.1277\n",
            "k-fold 1  Train Loss: 0.1096, Validation Loss: 0.1283\n",
            "k-fold 1  Train Loss: 0.1104, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.1107, Validation Loss: 0.1292\n",
            "k-fold 1  Train Loss: 0.1118, Validation Loss: 0.1298\n",
            "k-fold 1  Train Loss: 0.1121, Validation Loss: 0.1301\n",
            "k-fold 1  Train Loss: 0.1120, Validation Loss: 0.1303\n",
            "k-fold 1  Train Loss: 0.1114, Validation Loss: 0.1301\n",
            "k-fold 1  Train Loss: 0.1111, Validation Loss: 0.1301\n",
            "k-fold 1  Train Loss: 0.1103, Validation Loss: 0.1298\n",
            "k-fold 1  Train Loss: 0.1093, Validation Loss: 0.1295\n",
            "k-fold 1  Train Loss: 0.1078, Validation Loss: 0.1292\n",
            "k-fold 1  Train Loss: 0.1057, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1040, Validation Loss: 0.1293\n",
            "k-fold 1  Train Loss: 0.1033, Validation Loss: 0.1300\n",
            "k-fold 1  Train Loss: 0.1031, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.1035, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.1041, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.1048, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.1054, Validation Loss: 0.1357\n",
            "k-fold 1  Train Loss: 0.1065, Validation Loss: 0.1375\n",
            "k-fold 1  Train Loss: 0.1077, Validation Loss: 0.1392\n",
            "k-fold 1  Train Loss: 0.1075, Validation Loss: 0.1391\n",
            "k-fold 1  Train Loss: 0.1062, Validation Loss: 0.1376\n",
            "k-fold 1  Train Loss: 0.1047, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.1031, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1304\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1290\n",
            "k-fold 1  Train Loss: 0.1023, Validation Loss: 0.1284\n",
            "k-fold 1  Train Loss: 0.1024, Validation Loss: 0.1279\n",
            "k-fold 1  Train Loss: 0.1026, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1031, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1032, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1032, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1035, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1036, Validation Loss: 0.1263\n",
            "k-fold 1  Train Loss: 0.1035, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1034, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1032, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1029, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1024, Validation Loss: 0.1263\n",
            "k-fold 1  Train Loss: 0.1021, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1019, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1017, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1018, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1019, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1020, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1036, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1048, Validation Loss: 0.1247\n",
            "k-fold 1  Train Loss: 0.1064, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1082, Validation Loss: 0.1242\n",
            "k-fold 1  Train Loss: 0.1096, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1108, Validation Loss: 0.1246\n",
            "k-fold 1  Train Loss: 0.1110, Validation Loss: 0.1244\n",
            "k-fold 1  Train Loss: 0.1104, Validation Loss: 0.1242\n",
            "k-fold 1  Train Loss: 0.1094, Validation Loss: 0.1237\n",
            "k-fold 1  Train Loss: 0.1086, Validation Loss: 0.1234\n",
            "k-fold 1  Train Loss: 0.1075, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1062, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1045, Validation Loss: 0.1230\n",
            "k-fold 1  Train Loss: 0.1033, Validation Loss: 0.1236\n",
            "k-fold 1  Train Loss: 0.1027, Validation Loss: 0.1249\n",
            "k-fold 1  Train Loss: 0.1026, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1025, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1024, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1023, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1254\n",
            "k-fold 1  Train Loss: 0.1023, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1027, Validation Loss: 0.1237\n",
            "k-fold 1  Train Loss: 0.1034, Validation Loss: 0.1232\n",
            "k-fold 1  Train Loss: 0.1041, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1052, Validation Loss: 0.1226\n",
            "k-fold 1  Train Loss: 0.1056, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1062, Validation Loss: 0.1227\n",
            "k-fold 1  Train Loss: 0.1067, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1068, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1070, Validation Loss: 0.1228\n",
            "k-fold 1  Train Loss: 0.1068, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1063, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1054, Validation Loss: 0.1229\n",
            "k-fold 1  Train Loss: 0.1044, Validation Loss: 0.1231\n",
            "k-fold 1  Train Loss: 0.1034, Validation Loss: 0.1235\n",
            "k-fold 1  Train Loss: 0.1025, Validation Loss: 0.1240\n",
            "k-fold 1  Train Loss: 0.1023, Validation Loss: 0.1243\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1244\n",
            "k-fold 1  Train Loss: 0.1020, Validation Loss: 0.1248\n",
            "k-fold 1  Train Loss: 0.1018, Validation Loss: 0.1252\n",
            "k-fold 1  Train Loss: 0.1014, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1011, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1011, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1011, Validation Loss: 0.1261\n",
            "k-fold 1  Train Loss: 0.1012, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1016, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1023, Validation Loss: 0.1256\n",
            "k-fold 1  Train Loss: 0.1025, Validation Loss: 0.1258\n",
            "k-fold 1  Train Loss: 0.1029, Validation Loss: 0.1260\n",
            "k-fold 1  Train Loss: 0.1028, Validation Loss: 0.1264\n",
            "k-fold 1  Train Loss: 0.1027, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1028, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1030, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1031, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1028, Validation Loss: 0.1279\n",
            "k-fold 1  Train Loss: 0.1027, Validation Loss: 0.1281\n",
            "k-fold 1  Train Loss: 0.1024, Validation Loss: 0.1281\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.1017, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.1014, Validation Loss: 0.1281\n",
            "k-fold 1  Train Loss: 0.1012, Validation Loss: 0.1278\n",
            "k-fold 1  Train Loss: 0.1012, Validation Loss: 0.1275\n",
            "k-fold 1  Train Loss: 0.1013, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.1010, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1010, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1011, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1014, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1012, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1011, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1009, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1008, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1005, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1007, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1008, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1009, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1007, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1015, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1028, Validation Loss: 0.1263\n",
            "k-fold 1  Train Loss: 0.1037, Validation Loss: 0.1262\n",
            "k-fold 1  Train Loss: 0.1042, Validation Loss: 0.1263\n",
            "k-fold 1  Train Loss: 0.1047, Validation Loss: 0.1265\n",
            "k-fold 1  Train Loss: 0.1049, Validation Loss: 0.1266\n",
            "k-fold 1  Train Loss: 0.1049, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1046, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1043, Validation Loss: 0.1267\n",
            "k-fold 1  Train Loss: 0.1035, Validation Loss: 0.1268\n",
            "k-fold 1  Train Loss: 0.1029, Validation Loss: 0.1269\n",
            "k-fold 1  Train Loss: 0.1026, Validation Loss: 0.1270\n",
            "k-fold 1  Train Loss: 0.1024, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1021, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.1019, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1014, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1010, Validation Loss: 0.1278\n",
            "k-fold 1  Train Loss: 0.1005, Validation Loss: 0.1282\n",
            "k-fold 1  Train Loss: 0.1001, Validation Loss: 0.1287\n",
            "k-fold 1  Train Loss: 0.1000, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.0998, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.0997, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.0998, Validation Loss: 0.1286\n",
            "k-fold 1  Train Loss: 0.0996, Validation Loss: 0.1287\n",
            "k-fold 1  Train Loss: 0.0992, Validation Loss: 0.1291\n",
            "k-fold 1  Train Loss: 0.0989, Validation Loss: 0.1294\n",
            "k-fold 1  Train Loss: 0.0986, Validation Loss: 0.1299\n",
            "k-fold 1  Train Loss: 0.0983, Validation Loss: 0.1304\n",
            "k-fold 1  Train Loss: 0.0984, Validation Loss: 0.1300\n",
            "k-fold 1  Train Loss: 0.0983, Validation Loss: 0.1302\n",
            "k-fold 1  Train Loss: 0.0985, Validation Loss: 0.1300\n",
            "k-fold 1  Train Loss: 0.0987, Validation Loss: 0.1300\n",
            "k-fold 1  Train Loss: 0.0990, Validation Loss: 0.1297\n",
            "k-fold 1  Train Loss: 0.0992, Validation Loss: 0.1295\n",
            "k-fold 1  Train Loss: 0.0995, Validation Loss: 0.1294\n",
            "k-fold 1  Train Loss: 0.0998, Validation Loss: 0.1292\n",
            "k-fold 1  Train Loss: 0.1002, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1005, Validation Loss: 0.1288\n",
            "k-fold 1  Train Loss: 0.1007, Validation Loss: 0.1286\n",
            "k-fold 1  Train Loss: 0.1007, Validation Loss: 0.1284\n",
            "k-fold 1  Train Loss: 0.1006, Validation Loss: 0.1282\n",
            "k-fold 1  Train Loss: 0.1007, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.1008, Validation Loss: 0.1279\n",
            "k-fold 1  Train Loss: 0.1009, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.1008, Validation Loss: 0.1274\n",
            "k-fold 1  Train Loss: 0.1005, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.1001, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.0996, Validation Loss: 0.1271\n",
            "k-fold 1  Train Loss: 0.0993, Validation Loss: 0.1272\n",
            "k-fold 1  Train Loss: 0.0989, Validation Loss: 0.1273\n",
            "k-fold 1  Train Loss: 0.0984, Validation Loss: 0.1276\n",
            "k-fold 1  Train Loss: 0.0979, Validation Loss: 0.1280\n",
            "k-fold 1  Train Loss: 0.0975, Validation Loss: 0.1291\n",
            "k-fold 1  Train Loss: 0.0973, Validation Loss: 0.1299\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1308\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0969, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0969, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1312\n",
            "k-fold 1  Train Loss: 0.0973, Validation Loss: 0.1308\n",
            "k-fold 1  Train Loss: 0.0973, Validation Loss: 0.1307\n",
            "k-fold 1  Train Loss: 0.0976, Validation Loss: 0.1304\n",
            "k-fold 1  Train Loss: 0.0978, Validation Loss: 0.1302\n",
            "k-fold 1  Train Loss: 0.0976, Validation Loss: 0.1303\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1305\n",
            "k-fold 1  Train Loss: 0.0970, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0969, Validation Loss: 0.1310\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1312\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1303\n",
            "k-fold 1  Train Loss: 0.0970, Validation Loss: 0.1302\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1300\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1298\n",
            "k-fold 1  Train Loss: 0.0977, Validation Loss: 0.1294\n",
            "k-fold 1  Train Loss: 0.0982, Validation Loss: 0.1292\n",
            "k-fold 1  Train Loss: 0.0988, Validation Loss: 0.1290\n",
            "k-fold 1  Train Loss: 0.0995, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1001, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1003, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.1000, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.0999, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.0983, Validation Loss: 0.1293\n",
            "k-fold 1  Train Loss: 0.0973, Validation Loss: 0.1298\n",
            "k-fold 1  Train Loss: 0.0967, Validation Loss: 0.1303\n",
            "k-fold 1  Train Loss: 0.0963, Validation Loss: 0.1308\n",
            "k-fold 1  Train Loss: 0.0958, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.0957, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0956, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0956, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0955, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0954, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0953, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0952, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0952, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0954, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0958, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0962, Validation Loss: 0.1310\n",
            "k-fold 1  Train Loss: 0.0966, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0969, Validation Loss: 0.1308\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1310\n",
            "k-fold 1  Train Loss: 0.0969, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.0965, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0965, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0961, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0962, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0964, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0966, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0968, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0959, Validation Loss: 0.1340\n",
            "k-fold 1  Train Loss: 0.0953, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0949, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0945, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0944, Validation Loss: 0.1376\n",
            "k-fold 1  Train Loss: 0.0943, Validation Loss: 0.1381\n",
            "k-fold 1  Train Loss: 0.0943, Validation Loss: 0.1384\n",
            "k-fold 1  Train Loss: 0.0942, Validation Loss: 0.1382\n",
            "k-fold 1  Train Loss: 0.0942, Validation Loss: 0.1373\n",
            "k-fold 1  Train Loss: 0.0943, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.0945, Validation Loss: 0.1364\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0951, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0959, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0970, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0983, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0983, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0978, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0970, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.0965, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0954, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0950, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0952, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0956, Validation Loss: 0.1364\n",
            "k-fold 1  Train Loss: 0.0966, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1385\n",
            "k-fold 1  Train Loss: 0.0974, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0972, Validation Loss: 0.1382\n",
            "k-fold 1  Train Loss: 0.0963, Validation Loss: 0.1364\n",
            "k-fold 1  Train Loss: 0.0957, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0958, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0963, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0975, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0993, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.1009, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.1022, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.1034, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.1034, Validation Loss: 0.1335\n",
            "k-fold 1  Train Loss: 0.1032, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.1027, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.1016, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0996, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0982, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.0969, Validation Loss: 0.1315\n",
            "k-fold 1  Train Loss: 0.0955, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.0945, Validation Loss: 0.1315\n",
            "k-fold 1  Train Loss: 0.0938, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0933, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0934, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0938, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0939, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0932, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.0931, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0934, Validation Loss: 0.1305\n",
            "k-fold 1  Train Loss: 0.0938, Validation Loss: 0.1297\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1289\n",
            "k-fold 1  Train Loss: 0.0956, Validation Loss: 0.1285\n",
            "k-fold 1  Train Loss: 0.0960, Validation Loss: 0.1284\n",
            "k-fold 1  Train Loss: 0.0962, Validation Loss: 0.1284\n",
            "k-fold 1  Train Loss: 0.0961, Validation Loss: 0.1286\n",
            "k-fold 1  Train Loss: 0.0958, Validation Loss: 0.1287\n",
            "k-fold 1  Train Loss: 0.0949, Validation Loss: 0.1290\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1293\n",
            "k-fold 1  Train Loss: 0.0942, Validation Loss: 0.1296\n",
            "k-fold 1  Train Loss: 0.0938, Validation Loss: 0.1302\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1306\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1310\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0934, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0933, Validation Loss: 0.1339\n",
            "k-fold 1  Train Loss: 0.0932, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0931, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0931, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0930, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0927, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0926, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0925, Validation Loss: 0.1341\n",
            "k-fold 1  Train Loss: 0.0925, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0928, Validation Loss: 0.1332\n",
            "k-fold 1  Train Loss: 0.0932, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0937, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0941, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0952, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0955, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0956, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.0960, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0965, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0971, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0976, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0973, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0988, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.1000, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.1007, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.1010, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0997, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0978, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0960, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0950, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0944, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0932, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0931, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0932, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0937, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.0933, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0934, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0927, Validation Loss: 0.1319\n",
            "k-fold 1  Train Loss: 0.0931, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0932, Validation Loss: 0.1319\n",
            "k-fold 1  Train Loss: 0.0930, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0929, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0938, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.0941, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.0941, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0945, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0950, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0947, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0945, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0943, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0938, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0934, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0928, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0923, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0917, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1332\n",
            "k-fold 1  Train Loss: 0.0906, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0901, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0899, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0899, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0898, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0893, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0893, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0896, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0898, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0901, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0909, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0912, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0913, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0912, Validation Loss: 0.1350\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0908, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0908, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0905, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1382\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1385\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1371\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1360\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0913, Validation Loss: 0.1341\n",
            "k-fold 1  Train Loss: 0.0916, Validation Loss: 0.1339\n",
            "k-fold 1  Train Loss: 0.0917, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0917, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0917, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0920, Validation Loss: 0.1335\n",
            "k-fold 1  Train Loss: 0.0924, Validation Loss: 0.1332\n",
            "k-fold 1  Train Loss: 0.0925, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0922, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0923, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0920, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0918, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0902, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0901, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0900, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0890, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0888, Validation Loss: 0.1340\n",
            "k-fold 1  Train Loss: 0.0889, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0899, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0901, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0900, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0921, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0926, Validation Loss: 0.1340\n",
            "k-fold 1  Train Loss: 0.0926, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0926, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0921, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0902, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0891, Validation Loss: 0.1370\n",
            "k-fold 1  Train Loss: 0.0890, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1378\n",
            "k-fold 1  Train Loss: 0.0893, Validation Loss: 0.1378\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0899, Validation Loss: 0.1373\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1365\n",
            "k-fold 1  Train Loss: 0.0924, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0936, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0947, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0954, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0962, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0963, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0940, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0921, Validation Loss: 0.1370\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1380\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1389\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1399\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1410\n",
            "k-fold 1  Train Loss: 0.0905, Validation Loss: 0.1417\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1420\n",
            "k-fold 1  Train Loss: 0.0902, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0902, Validation Loss: 0.1393\n",
            "k-fold 1  Train Loss: 0.0906, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.0913, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0915, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0912, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0908, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0905, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0902, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0898, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0929, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0963, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0965, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0961, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.0953, Validation Loss: 0.1312\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0935, Validation Loss: 0.1306\n",
            "k-fold 1  Train Loss: 0.0921, Validation Loss: 0.1305\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1305\n",
            "k-fold 1  Train Loss: 0.0900, Validation Loss: 0.1308\n",
            "k-fold 1  Train Loss: 0.0891, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.0887, Validation Loss: 0.1314\n",
            "k-fold 1  Train Loss: 0.0884, Validation Loss: 0.1315\n",
            "k-fold 1  Train Loss: 0.0882, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0881, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.0879, Validation Loss: 0.1319\n",
            "k-fold 1  Train Loss: 0.0879, Validation Loss: 0.1319\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1319\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1332\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0875, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1318\n",
            "k-fold 1  Train Loss: 0.0880, Validation Loss: 0.1315\n",
            "k-fold 1  Train Loss: 0.0882, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0900, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0909, Validation Loss: 0.1309\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1310\n",
            "k-fold 1  Train Loss: 0.0915, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.0917, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0905, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0891, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0885, Validation Loss: 0.1340\n",
            "k-fold 1  Train Loss: 0.0885, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0884, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0881, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0879, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0879, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1360\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1360\n",
            "k-fold 1  Train Loss: 0.0881, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0884, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0887, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0891, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0892, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0893, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0895, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0888, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0883, Validation Loss: 0.1365\n",
            "k-fold 1  Train Loss: 0.0884, Validation Loss: 0.1375\n",
            "k-fold 1  Train Loss: 0.0887, Validation Loss: 0.1381\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1396\n",
            "k-fold 1  Train Loss: 0.0899, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0900, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0906, Validation Loss: 0.1423\n",
            "k-fold 1  Train Loss: 0.0910, Validation Loss: 0.1429\n",
            "k-fold 1  Train Loss: 0.0912, Validation Loss: 0.1433\n",
            "k-fold 1  Train Loss: 0.0906, Validation Loss: 0.1429\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1422\n",
            "k-fold 1  Train Loss: 0.0888, Validation Loss: 0.1415\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1401\n",
            "k-fold 1  Train Loss: 0.0867, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.0867, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0868, Validation Loss: 0.1350\n",
            "k-fold 1  Train Loss: 0.0871, Validation Loss: 0.1340\n",
            "k-fold 1  Train Loss: 0.0875, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0875, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0874, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1321\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1319\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0880, Validation Loss: 0.1317\n",
            "k-fold 1  Train Loss: 0.0887, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.0896, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.0898, Validation Loss: 0.1311\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1313\n",
            "k-fold 1  Train Loss: 0.0888, Validation Loss: 0.1316\n",
            "k-fold 1  Train Loss: 0.0881, Validation Loss: 0.1320\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0877, Validation Loss: 0.1324\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0875, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0870, Validation Loss: 0.1335\n",
            "k-fold 1  Train Loss: 0.0866, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1383\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1390\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1389\n",
            "k-fold 1  Train Loss: 0.0866, Validation Loss: 0.1403\n",
            "k-fold 1  Train Loss: 0.0867, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.0865, Validation Loss: 0.1410\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1408\n",
            "k-fold 1  Train Loss: 0.0864, Validation Loss: 0.1421\n",
            "k-fold 1  Train Loss: 0.0865, Validation Loss: 0.1428\n",
            "k-fold 1  Train Loss: 0.0865, Validation Loss: 0.1431\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1428\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1426\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1422\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1419\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1400\n",
            "k-fold 1  Train Loss: 0.0860, Validation Loss: 0.1395\n",
            "k-fold 1  Train Loss: 0.0860, Validation Loss: 0.1392\n",
            "k-fold 1  Train Loss: 0.0870, Validation Loss: 0.1380\n",
            "k-fold 1  Train Loss: 0.0881, Validation Loss: 0.1374\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0907, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.0925, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0941, Validation Loss: 0.1360\n",
            "k-fold 1  Train Loss: 0.0954, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0958, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0960, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0963, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0964, Validation Loss: 0.1350\n",
            "k-fold 1  Train Loss: 0.0958, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0946, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0929, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1346\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1346\n",
            "k-fold 1  Train Loss: 0.0888, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0874, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0865, Validation Loss: 0.1385\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0853, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.0848, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0843, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1416\n",
            "k-fold 1  Train Loss: 0.0838, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0836, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.0836, Validation Loss: 0.1399\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1391\n",
            "k-fold 1  Train Loss: 0.0839, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1383\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0852, Validation Loss: 0.1373\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0868, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0872, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0874, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0868, Validation Loss: 0.1372\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1375\n",
            "k-fold 1  Train Loss: 0.0855, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1390\n",
            "k-fold 1  Train Loss: 0.0833, Validation Loss: 0.1401\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1429\n",
            "k-fold 1  Train Loss: 0.0834, Validation Loss: 0.1450\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1472\n",
            "k-fold 1  Train Loss: 0.0856, Validation Loss: 0.1490\n",
            "k-fold 1  Train Loss: 0.0868, Validation Loss: 0.1504\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1500\n",
            "k-fold 1  Train Loss: 0.0854, Validation Loss: 0.1490\n",
            "k-fold 1  Train Loss: 0.0842, Validation Loss: 0.1468\n",
            "k-fold 1  Train Loss: 0.0835, Validation Loss: 0.1447\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1424\n",
            "k-fold 1  Train Loss: 0.0839, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0854, Validation Loss: 0.1394\n",
            "k-fold 1  Train Loss: 0.0870, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0868, Validation Loss: 0.1388\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1390\n",
            "k-fold 1  Train Loss: 0.0855, Validation Loss: 0.1393\n",
            "k-fold 1  Train Loss: 0.0849, Validation Loss: 0.1397\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1407\n",
            "k-fold 1  Train Loss: 0.0843, Validation Loss: 0.1420\n",
            "k-fold 1  Train Loss: 0.0844, Validation Loss: 0.1431\n",
            "k-fold 1  Train Loss: 0.0846, Validation Loss: 0.1439\n",
            "k-fold 1  Train Loss: 0.0852, Validation Loss: 0.1450\n",
            "k-fold 1  Train Loss: 0.0856, Validation Loss: 0.1454\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1452\n",
            "k-fold 1  Train Loss: 0.0859, Validation Loss: 0.1446\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1441\n",
            "k-fold 1  Train Loss: 0.0857, Validation Loss: 0.1435\n",
            "k-fold 1  Train Loss: 0.0854, Validation Loss: 0.1424\n",
            "k-fold 1  Train Loss: 0.0852, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.0852, Validation Loss: 0.1394\n",
            "k-fold 1  Train Loss: 0.0859, Validation Loss: 0.1378\n",
            "k-fold 1  Train Loss: 0.0867, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1360\n",
            "k-fold 1  Train Loss: 0.0887, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0894, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0900, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0898, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0898, Validation Loss: 0.1340\n",
            "k-fold 1  Train Loss: 0.0890, Validation Loss: 0.1341\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0868, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0850, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0847, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0842, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0826, Validation Loss: 0.1357\n",
            "k-fold 1  Train Loss: 0.0824, Validation Loss: 0.1359\n",
            "k-fold 1  Train Loss: 0.0824, Validation Loss: 0.1359\n",
            "k-fold 1  Train Loss: 0.0826, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0828, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0828, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0833, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1372\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1375\n",
            "k-fold 1  Train Loss: 0.0826, Validation Loss: 0.1383\n",
            "k-fold 1  Train Loss: 0.0824, Validation Loss: 0.1391\n",
            "k-fold 1  Train Loss: 0.0823, Validation Loss: 0.1392\n",
            "k-fold 1  Train Loss: 0.0824, Validation Loss: 0.1389\n",
            "k-fold 1  Train Loss: 0.0826, Validation Loss: 0.1385\n",
            "k-fold 1  Train Loss: 0.0830, Validation Loss: 0.1382\n",
            "k-fold 1  Train Loss: 0.0834, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0838, Validation Loss: 0.1377\n",
            "k-fold 1  Train Loss: 0.0848, Validation Loss: 0.1374\n",
            "k-fold 1  Train Loss: 0.0859, Validation Loss: 0.1370\n",
            "k-fold 1  Train Loss: 0.0866, Validation Loss: 0.1369\n",
            "k-fold 1  Train Loss: 0.0873, Validation Loss: 0.1367\n",
            "k-fold 1  Train Loss: 0.0857, Validation Loss: 0.1371\n",
            "k-fold 1  Train Loss: 0.0843, Validation Loss: 0.1376\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0824, Validation Loss: 0.1395\n",
            "k-fold 1  Train Loss: 0.0823, Validation Loss: 0.1404\n",
            "k-fold 1  Train Loss: 0.0822, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.0821, Validation Loss: 0.1418\n",
            "k-fold 1  Train Loss: 0.0821, Validation Loss: 0.1420\n",
            "k-fold 1  Train Loss: 0.0825, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0835, Validation Loss: 0.1397\n",
            "k-fold 1  Train Loss: 0.0843, Validation Loss: 0.1391\n",
            "k-fold 1  Train Loss: 0.0851, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0862, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0865, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0886, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1376\n",
            "k-fold 1  Train Loss: 0.0916, Validation Loss: 0.1375\n",
            "k-fold 1  Train Loss: 0.0924, Validation Loss: 0.1374\n",
            "k-fold 1  Train Loss: 0.0918, Validation Loss: 0.1379\n",
            "k-fold 1  Train Loss: 0.0913, Validation Loss: 0.1385\n",
            "k-fold 1  Train Loss: 0.0913, Validation Loss: 0.1389\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1393\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.0917, Validation Loss: 0.1408\n",
            "k-fold 1  Train Loss: 0.0920, Validation Loss: 0.1418\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1423\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1430\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1437\n",
            "k-fold 1  Train Loss: 0.0919, Validation Loss: 0.1446\n",
            "k-fold 1  Train Loss: 0.0914, Validation Loss: 0.1452\n",
            "k-fold 1  Train Loss: 0.0911, Validation Loss: 0.1458\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1453\n",
            "k-fold 1  Train Loss: 0.0879, Validation Loss: 0.1446\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1437\n",
            "k-fold 1  Train Loss: 0.0849, Validation Loss: 0.1434\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1426\n",
            "k-fold 1  Train Loss: 0.0825, Validation Loss: 0.1415\n",
            "k-fold 1  Train Loss: 0.0817, Validation Loss: 0.1403\n",
            "k-fold 1  Train Loss: 0.0812, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0810, Validation Loss: 0.1370\n",
            "k-fold 1  Train Loss: 0.0815, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0822, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0840, Validation Loss: 0.1327\n",
            "k-fold 1  Train Loss: 0.0843, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0851, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1322\n",
            "k-fold 1  Train Loss: 0.0865, Validation Loss: 0.1323\n",
            "k-fold 1  Train Loss: 0.0871, Validation Loss: 0.1325\n",
            "k-fold 1  Train Loss: 0.0879, Validation Loss: 0.1326\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1328\n",
            "k-fold 1  Train Loss: 0.0878, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0876, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0866, Validation Loss: 0.1335\n",
            "k-fold 1  Train Loss: 0.0854, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0846, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0840, Validation Loss: 0.1353\n",
            "k-fold 1  Train Loss: 0.0842, Validation Loss: 0.1352\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0840, Validation Loss: 0.1357\n",
            "k-fold 1  Train Loss: 0.0839, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0838, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0835, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0833, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1361\n",
            "k-fold 1  Train Loss: 0.0828, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0827, Validation Loss: 0.1350\n",
            "k-fold 1  Train Loss: 0.0825, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0825, Validation Loss: 0.1341\n",
            "k-fold 1  Train Loss: 0.0824, Validation Loss: 0.1339\n",
            "k-fold 1  Train Loss: 0.0826, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0827, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0827, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.0828, Validation Loss: 0.1332\n",
            "k-fold 1  Train Loss: 0.0830, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1336\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1339\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1347\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1338\n",
            "k-fold 1  Train Loss: 0.0842, Validation Loss: 0.1333\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1330\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0842, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1329\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1331\n",
            "k-fold 1  Train Loss: 0.0834, Validation Loss: 0.1334\n",
            "k-fold 1  Train Loss: 0.0834, Validation Loss: 0.1337\n",
            "k-fold 1  Train Loss: 0.0834, Validation Loss: 0.1341\n",
            "k-fold 1  Train Loss: 0.0830, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0832, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0833, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0838, Validation Loss: 0.1345\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0844, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0845, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0846, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0847, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0851, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0858, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0861, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0863, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0864, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0854, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0848, Validation Loss: 0.1351\n",
            "k-fold 1  Train Loss: 0.0837, Validation Loss: 0.1357\n",
            "k-fold 1  Train Loss: 0.0831, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0825, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0819, Validation Loss: 0.1373\n",
            "k-fold 1  Train Loss: 0.0814, Validation Loss: 0.1378\n",
            "k-fold 1  Train Loss: 0.0807, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0807, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0806, Validation Loss: 0.1388\n",
            "k-fold 1  Train Loss: 0.0805, Validation Loss: 0.1388\n",
            "k-fold 1  Train Loss: 0.0805, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0804, Validation Loss: 0.1386\n",
            "k-fold 1  Train Loss: 0.0807, Validation Loss: 0.1382\n",
            "k-fold 1  Train Loss: 0.0809, Validation Loss: 0.1381\n",
            "k-fold 1  Train Loss: 0.0809, Validation Loss: 0.1383\n",
            "k-fold 1  Train Loss: 0.0811, Validation Loss: 0.1381\n",
            "k-fold 1  Train Loss: 0.0813, Validation Loss: 0.1380\n",
            "k-fold 1  Train Loss: 0.0809, Validation Loss: 0.1384\n",
            "k-fold 1  Train Loss: 0.0807, Validation Loss: 0.1387\n",
            "k-fold 1  Train Loss: 0.0806, Validation Loss: 0.1389\n",
            "k-fold 1  Train Loss: 0.0805, Validation Loss: 0.1390\n",
            "k-fold 1  Train Loss: 0.0802, Validation Loss: 0.1395\n",
            "k-fold 1  Train Loss: 0.0806, Validation Loss: 0.1395\n",
            "k-fold 1  Train Loss: 0.0811, Validation Loss: 0.1395\n",
            "k-fold 1  Train Loss: 0.0811, Validation Loss: 0.1400\n",
            "k-fold 1  Train Loss: 0.0812, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.0817, Validation Loss: 0.1401\n",
            "k-fold 1  Train Loss: 0.0819, Validation Loss: 0.1400\n",
            "k-fold 1  Train Loss: 0.0821, Validation Loss: 0.1400\n",
            "k-fold 1  Train Loss: 0.0819, Validation Loss: 0.1401\n",
            "k-fold 1  Train Loss: 0.0815, Validation Loss: 0.1403\n",
            "k-fold 1  Train Loss: 0.0811, Validation Loss: 0.1404\n",
            "k-fold 1  Train Loss: 0.0808, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.0805, Validation Loss: 0.1402\n",
            "k-fold 1  Train Loss: 0.0802, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0800, Validation Loss: 0.1408\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1413\n",
            "k-fold 1  Train Loss: 0.0793, Validation Loss: 0.1421\n",
            "k-fold 1  Train Loss: 0.0794, Validation Loss: 0.1422\n",
            "k-fold 1  Train Loss: 0.0794, Validation Loss: 0.1423\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1423\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1424\n",
            "k-fold 1  Train Loss: 0.0797, Validation Loss: 0.1422\n",
            "k-fold 1  Train Loss: 0.0798, Validation Loss: 0.1419\n",
            "k-fold 1  Train Loss: 0.0798, Validation Loss: 0.1418\n",
            "k-fold 1  Train Loss: 0.0799, Validation Loss: 0.1418\n",
            "k-fold 1  Train Loss: 0.0803, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0808, Validation Loss: 0.1410\n",
            "k-fold 1  Train Loss: 0.0811, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.0815, Validation Loss: 0.1406\n",
            "k-fold 1  Train Loss: 0.0810, Validation Loss: 0.1414\n",
            "k-fold 1  Train Loss: 0.0805, Validation Loss: 0.1427\n",
            "k-fold 1  Train Loss: 0.0803, Validation Loss: 0.1434\n",
            "k-fold 1  Train Loss: 0.0801, Validation Loss: 0.1447\n",
            "k-fold 1  Train Loss: 0.0798, Validation Loss: 0.1457\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1466\n",
            "k-fold 1  Train Loss: 0.0795, Validation Loss: 0.1472\n",
            "k-fold 1  Train Loss: 0.0795, Validation Loss: 0.1478\n",
            "k-fold 1  Train Loss: 0.0793, Validation Loss: 0.1477\n",
            "k-fold 1  Train Loss: 0.0791, Validation Loss: 0.1462\n",
            "k-fold 1  Train Loss: 0.0795, Validation Loss: 0.1440\n",
            "k-fold 1  Train Loss: 0.0801, Validation Loss: 0.1432\n",
            "k-fold 1  Train Loss: 0.0813, Validation Loss: 0.1421\n",
            "k-fold 1  Train Loss: 0.0829, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.0846, Validation Loss: 0.1410\n",
            "k-fold 1  Train Loss: 0.0872, Validation Loss: 0.1409\n",
            "k-fold 1  Train Loss: 0.0893, Validation Loss: 0.1412\n",
            "k-fold 1  Train Loss: 0.0906, Validation Loss: 0.1416\n",
            "k-fold 1  Train Loss: 0.0905, Validation Loss: 0.1419\n",
            "k-fold 1  Train Loss: 0.0904, Validation Loss: 0.1421\n",
            "k-fold 1  Train Loss: 0.0903, Validation Loss: 0.1422\n",
            "k-fold 1  Train Loss: 0.0897, Validation Loss: 0.1423\n",
            "k-fold 1  Train Loss: 0.0881, Validation Loss: 0.1425\n",
            "k-fold 1  Train Loss: 0.0869, Validation Loss: 0.1426\n",
            "k-fold 1  Train Loss: 0.0854, Validation Loss: 0.1427\n",
            "k-fold 1  Train Loss: 0.0841, Validation Loss: 0.1428\n",
            "k-fold 1  Train Loss: 0.0833, Validation Loss: 0.1428\n",
            "k-fold 1  Train Loss: 0.0819, Validation Loss: 0.1435\n",
            "k-fold 1  Train Loss: 0.0810, Validation Loss: 0.1445\n",
            "k-fold 1  Train Loss: 0.0804, Validation Loss: 0.1455\n",
            "k-fold 1  Train Loss: 0.0799, Validation Loss: 0.1460\n",
            "k-fold 1  Train Loss: 0.0792, Validation Loss: 0.1455\n",
            "k-fold 1  Train Loss: 0.0786, Validation Loss: 0.1448\n",
            "k-fold 1  Train Loss: 0.0782, Validation Loss: 0.1447\n",
            "k-fold 1  Train Loss: 0.0781, Validation Loss: 0.1445\n",
            "k-fold 1  Train Loss: 0.0781, Validation Loss: 0.1446\n",
            "k-fold 1  Train Loss: 0.0780, Validation Loss: 0.1439\n",
            "k-fold 1  Train Loss: 0.0780, Validation Loss: 0.1432\n",
            "k-fold 1  Train Loss: 0.0781, Validation Loss: 0.1422\n",
            "k-fold 1  Train Loss: 0.0783, Validation Loss: 0.1405\n",
            "k-fold 1  Train Loss: 0.0785, Validation Loss: 0.1390\n",
            "k-fold 1  Train Loss: 0.0789, Validation Loss: 0.1378\n",
            "k-fold 1  Train Loss: 0.0798, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0797, Validation Loss: 0.1359\n",
            "k-fold 1  Train Loss: 0.0799, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0800, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0802, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0800, Validation Loss: 0.1356\n",
            "k-fold 1  Train Loss: 0.0798, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0797, Validation Loss: 0.1365\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1368\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0796, Validation Loss: 0.1359\n",
            "k-fold 1  Train Loss: 0.0795, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0799, Validation Loss: 0.1348\n",
            "k-fold 1  Train Loss: 0.0803, Validation Loss: 0.1344\n",
            "k-fold 1  Train Loss: 0.0806, Validation Loss: 0.1342\n",
            "k-fold 1  Train Loss: 0.0807, Validation Loss: 0.1343\n",
            "k-fold 1  Train Loss: 0.0801, Validation Loss: 0.1349\n",
            "k-fold 1  Train Loss: 0.0797, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0795, Validation Loss: 0.1359\n",
            "k-fold 1  Train Loss: 0.0793, Validation Loss: 0.1360\n",
            "k-fold 1  Train Loss: 0.0786, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0782, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0781, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0780, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0779, Validation Loss: 0.1366\n",
            "k-fold 1  Train Loss: 0.0781, Validation Loss: 0.1362\n",
            "k-fold 1  Train Loss: 0.0784, Validation Loss: 0.1355\n",
            "k-fold 1  Train Loss: 0.0787, Validation Loss: 0.1350\n",
            "k-fold 1  Train Loss: 0.0787, Validation Loss: 0.1354\n",
            "k-fold 1  Train Loss: 0.0789, Validation Loss: 0.1358\n",
            "k-fold 1  Train Loss: 0.0790, Validation Loss: 0.1359\n",
            "k-fold 1  Train Loss: 0.0789, Validation Loss: 0.1363\n",
            "k-fold 1  Train Loss: 0.0788, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.3991, Validation Loss: 0.4079\n",
            "k-fold 2  Train Loss: 0.3871, Validation Loss: 0.3958\n",
            "k-fold 2  Train Loss: 0.3754, Validation Loss: 0.3839\n",
            "k-fold 2  Train Loss: 0.3641, Validation Loss: 0.3726\n",
            "k-fold 2  Train Loss: 0.3531, Validation Loss: 0.3617\n",
            "k-fold 2  Train Loss: 0.3424, Validation Loss: 0.3511\n",
            "k-fold 2  Train Loss: 0.3319, Validation Loss: 0.3407\n",
            "k-fold 2  Train Loss: 0.3216, Validation Loss: 0.3306\n",
            "k-fold 2  Train Loss: 0.3117, Validation Loss: 0.3208\n",
            "k-fold 2  Train Loss: 0.3019, Validation Loss: 0.3113\n",
            "k-fold 2  Train Loss: 0.2924, Validation Loss: 0.3021\n",
            "k-fold 2  Train Loss: 0.2832, Validation Loss: 0.2933\n",
            "k-fold 2  Train Loss: 0.2743, Validation Loss: 0.2848\n",
            "k-fold 2  Train Loss: 0.2660, Validation Loss: 0.2768\n",
            "k-fold 2  Train Loss: 0.2582, Validation Loss: 0.2696\n",
            "k-fold 2  Train Loss: 0.2507, Validation Loss: 0.2627\n",
            "k-fold 2  Train Loss: 0.2437, Validation Loss: 0.2562\n",
            "k-fold 2  Train Loss: 0.2373, Validation Loss: 0.2501\n",
            "k-fold 2  Train Loss: 0.2316, Validation Loss: 0.2446\n",
            "k-fold 2  Train Loss: 0.2265, Validation Loss: 0.2398\n",
            "k-fold 2  Train Loss: 0.2218, Validation Loss: 0.2353\n",
            "k-fold 2  Train Loss: 0.2177, Validation Loss: 0.2314\n",
            "k-fold 2  Train Loss: 0.2138, Validation Loss: 0.2278\n",
            "k-fold 2  Train Loss: 0.2107, Validation Loss: 0.2249\n",
            "k-fold 2  Train Loss: 0.2080, Validation Loss: 0.2224\n",
            "k-fold 2  Train Loss: 0.2056, Validation Loss: 0.2203\n",
            "k-fold 2  Train Loss: 0.2035, Validation Loss: 0.2185\n",
            "k-fold 2  Train Loss: 0.2020, Validation Loss: 0.2172\n",
            "k-fold 2  Train Loss: 0.2007, Validation Loss: 0.2159\n",
            "k-fold 2  Train Loss: 0.1994, Validation Loss: 0.2148\n",
            "k-fold 2  Train Loss: 0.1982, Validation Loss: 0.2137\n",
            "k-fold 2  Train Loss: 0.1972, Validation Loss: 0.2127\n",
            "k-fold 2  Train Loss: 0.1962, Validation Loss: 0.2117\n",
            "k-fold 2  Train Loss: 0.1951, Validation Loss: 0.2108\n",
            "k-fold 2  Train Loss: 0.1941, Validation Loss: 0.2099\n",
            "k-fold 2  Train Loss: 0.1933, Validation Loss: 0.2091\n",
            "k-fold 2  Train Loss: 0.1924, Validation Loss: 0.2083\n",
            "k-fold 2  Train Loss: 0.1916, Validation Loss: 0.2076\n",
            "k-fold 2  Train Loss: 0.1908, Validation Loss: 0.2069\n",
            "k-fold 2  Train Loss: 0.1900, Validation Loss: 0.2062\n",
            "k-fold 2  Train Loss: 0.1891, Validation Loss: 0.2054\n",
            "k-fold 2  Train Loss: 0.1882, Validation Loss: 0.2046\n",
            "k-fold 2  Train Loss: 0.1873, Validation Loss: 0.2038\n",
            "k-fold 2  Train Loss: 0.1866, Validation Loss: 0.2033\n",
            "k-fold 2  Train Loss: 0.1862, Validation Loss: 0.2030\n",
            "k-fold 2  Train Loss: 0.1857, Validation Loss: 0.2026\n",
            "k-fold 2  Train Loss: 0.1851, Validation Loss: 0.2021\n",
            "k-fold 2  Train Loss: 0.1846, Validation Loss: 0.2017\n",
            "k-fold 2  Train Loss: 0.1840, Validation Loss: 0.2012\n",
            "k-fold 2  Train Loss: 0.1835, Validation Loss: 0.2009\n",
            "k-fold 2  Train Loss: 0.1830, Validation Loss: 0.2005\n",
            "k-fold 2  Train Loss: 0.1825, Validation Loss: 0.2002\n",
            "k-fold 2  Train Loss: 0.1820, Validation Loss: 0.1999\n",
            "k-fold 2  Train Loss: 0.1813, Validation Loss: 0.1994\n",
            "k-fold 2  Train Loss: 0.1805, Validation Loss: 0.1988\n",
            "k-fold 2  Train Loss: 0.1800, Validation Loss: 0.1986\n",
            "k-fold 2  Train Loss: 0.1796, Validation Loss: 0.1984\n",
            "k-fold 2  Train Loss: 0.1790, Validation Loss: 0.1981\n",
            "k-fold 2  Train Loss: 0.1784, Validation Loss: 0.1978\n",
            "k-fold 2  Train Loss: 0.1782, Validation Loss: 0.1979\n",
            "k-fold 2  Train Loss: 0.1782, Validation Loss: 0.1982\n",
            "k-fold 2  Train Loss: 0.1779, Validation Loss: 0.1982\n",
            "k-fold 2  Train Loss: 0.1773, Validation Loss: 0.1979\n",
            "k-fold 2  Train Loss: 0.1771, Validation Loss: 0.1981\n",
            "k-fold 2  Train Loss: 0.1767, Validation Loss: 0.1981\n",
            "k-fold 2  Train Loss: 0.1762, Validation Loss: 0.1979\n",
            "k-fold 2  Train Loss: 0.1753, Validation Loss: 0.1973\n",
            "k-fold 2  Train Loss: 0.1739, Validation Loss: 0.1961\n",
            "k-fold 2  Train Loss: 0.1723, Validation Loss: 0.1948\n",
            "k-fold 2  Train Loss: 0.1709, Validation Loss: 0.1936\n",
            "k-fold 2  Train Loss: 0.1690, Validation Loss: 0.1919\n",
            "k-fold 2  Train Loss: 0.1680, Validation Loss: 0.1911\n",
            "k-fold 2  Train Loss: 0.1671, Validation Loss: 0.1904\n",
            "k-fold 2  Train Loss: 0.1657, Validation Loss: 0.1893\n",
            "k-fold 2  Train Loss: 0.1642, Validation Loss: 0.1881\n",
            "k-fold 2  Train Loss: 0.1631, Validation Loss: 0.1873\n",
            "k-fold 2  Train Loss: 0.1620, Validation Loss: 0.1865\n",
            "k-fold 2  Train Loss: 0.1608, Validation Loss: 0.1856\n",
            "k-fold 2  Train Loss: 0.1595, Validation Loss: 0.1846\n",
            "k-fold 2  Train Loss: 0.1580, Validation Loss: 0.1835\n",
            "k-fold 2  Train Loss: 0.1566, Validation Loss: 0.1824\n",
            "k-fold 2  Train Loss: 0.1552, Validation Loss: 0.1813\n",
            "k-fold 2  Train Loss: 0.1540, Validation Loss: 0.1805\n",
            "k-fold 2  Train Loss: 0.1528, Validation Loss: 0.1796\n",
            "k-fold 2  Train Loss: 0.1519, Validation Loss: 0.1791\n",
            "k-fold 2  Train Loss: 0.1510, Validation Loss: 0.1787\n",
            "k-fold 2  Train Loss: 0.1502, Validation Loss: 0.1783\n",
            "k-fold 2  Train Loss: 0.1494, Validation Loss: 0.1780\n",
            "k-fold 2  Train Loss: 0.1486, Validation Loss: 0.1777\n",
            "k-fold 2  Train Loss: 0.1478, Validation Loss: 0.1776\n",
            "k-fold 2  Train Loss: 0.1471, Validation Loss: 0.1777\n",
            "k-fold 2  Train Loss: 0.1468, Validation Loss: 0.1783\n",
            "k-fold 2  Train Loss: 0.1469, Validation Loss: 0.1794\n",
            "k-fold 2  Train Loss: 0.1474, Validation Loss: 0.1807\n",
            "k-fold 2  Train Loss: 0.1482, Validation Loss: 0.1821\n",
            "k-fold 2  Train Loss: 0.1484, Validation Loss: 0.1827\n",
            "k-fold 2  Train Loss: 0.1483, Validation Loss: 0.1832\n",
            "k-fold 2  Train Loss: 0.1483, Validation Loss: 0.1837\n",
            "k-fold 2  Train Loss: 0.1485, Validation Loss: 0.1843\n",
            "k-fold 2  Train Loss: 0.1477, Validation Loss: 0.1841\n",
            "k-fold 2  Train Loss: 0.1468, Validation Loss: 0.1836\n",
            "k-fold 2  Train Loss: 0.1455, Validation Loss: 0.1829\n",
            "k-fold 2  Train Loss: 0.1443, Validation Loss: 0.1822\n",
            "k-fold 2  Train Loss: 0.1437, Validation Loss: 0.1820\n",
            "k-fold 2  Train Loss: 0.1433, Validation Loss: 0.1821\n",
            "k-fold 2  Train Loss: 0.1427, Validation Loss: 0.1819\n",
            "k-fold 2  Train Loss: 0.1419, Validation Loss: 0.1814\n",
            "k-fold 2  Train Loss: 0.1411, Validation Loss: 0.1807\n",
            "k-fold 2  Train Loss: 0.1404, Validation Loss: 0.1801\n",
            "k-fold 2  Train Loss: 0.1399, Validation Loss: 0.1799\n",
            "k-fold 2  Train Loss: 0.1390, Validation Loss: 0.1793\n",
            "k-fold 2  Train Loss: 0.1384, Validation Loss: 0.1790\n",
            "k-fold 2  Train Loss: 0.1381, Validation Loss: 0.1790\n",
            "k-fold 2  Train Loss: 0.1377, Validation Loss: 0.1789\n",
            "k-fold 2  Train Loss: 0.1374, Validation Loss: 0.1787\n",
            "k-fold 2  Train Loss: 0.1369, Validation Loss: 0.1783\n",
            "k-fold 2  Train Loss: 0.1366, Validation Loss: 0.1780\n",
            "k-fold 2  Train Loss: 0.1366, Validation Loss: 0.1779\n",
            "k-fold 2  Train Loss: 0.1363, Validation Loss: 0.1777\n",
            "k-fold 2  Train Loss: 0.1362, Validation Loss: 0.1776\n",
            "k-fold 2  Train Loss: 0.1358, Validation Loss: 0.1772\n",
            "k-fold 2  Train Loss: 0.1356, Validation Loss: 0.1771\n",
            "k-fold 2  Train Loss: 0.1352, Validation Loss: 0.1767\n",
            "k-fold 2  Train Loss: 0.1347, Validation Loss: 0.1762\n",
            "k-fold 2  Train Loss: 0.1338, Validation Loss: 0.1754\n",
            "k-fold 2  Train Loss: 0.1330, Validation Loss: 0.1748\n",
            "k-fold 2  Train Loss: 0.1324, Validation Loss: 0.1742\n",
            "k-fold 2  Train Loss: 0.1317, Validation Loss: 0.1735\n",
            "k-fold 2  Train Loss: 0.1311, Validation Loss: 0.1730\n",
            "k-fold 2  Train Loss: 0.1306, Validation Loss: 0.1727\n",
            "k-fold 2  Train Loss: 0.1303, Validation Loss: 0.1726\n",
            "k-fold 2  Train Loss: 0.1303, Validation Loss: 0.1727\n",
            "k-fold 2  Train Loss: 0.1303, Validation Loss: 0.1729\n",
            "k-fold 2  Train Loss: 0.1303, Validation Loss: 0.1731\n",
            "k-fold 2  Train Loss: 0.1303, Validation Loss: 0.1732\n",
            "k-fold 2  Train Loss: 0.1296, Validation Loss: 0.1728\n",
            "k-fold 2  Train Loss: 0.1292, Validation Loss: 0.1728\n",
            "k-fold 2  Train Loss: 0.1287, Validation Loss: 0.1725\n",
            "k-fold 2  Train Loss: 0.1284, Validation Loss: 0.1725\n",
            "k-fold 2  Train Loss: 0.1279, Validation Loss: 0.1723\n",
            "k-fold 2  Train Loss: 0.1276, Validation Loss: 0.1724\n",
            "k-fold 2  Train Loss: 0.1273, Validation Loss: 0.1725\n",
            "k-fold 2  Train Loss: 0.1271, Validation Loss: 0.1725\n",
            "k-fold 2  Train Loss: 0.1266, Validation Loss: 0.1726\n",
            "k-fold 2  Train Loss: 0.1263, Validation Loss: 0.1726\n",
            "k-fold 2  Train Loss: 0.1260, Validation Loss: 0.1728\n",
            "k-fold 2  Train Loss: 0.1258, Validation Loss: 0.1729\n",
            "k-fold 2  Train Loss: 0.1256, Validation Loss: 0.1730\n",
            "k-fold 2  Train Loss: 0.1254, Validation Loss: 0.1731\n",
            "k-fold 2  Train Loss: 0.1252, Validation Loss: 0.1733\n",
            "k-fold 2  Train Loss: 0.1251, Validation Loss: 0.1734\n",
            "k-fold 2  Train Loss: 0.1249, Validation Loss: 0.1734\n",
            "k-fold 2  Train Loss: 0.1247, Validation Loss: 0.1732\n",
            "k-fold 2  Train Loss: 0.1245, Validation Loss: 0.1731\n",
            "k-fold 2  Train Loss: 0.1243, Validation Loss: 0.1731\n",
            "k-fold 2  Train Loss: 0.1254, Validation Loss: 0.1746\n",
            "k-fold 2  Train Loss: 0.1273, Validation Loss: 0.1765\n",
            "k-fold 2  Train Loss: 0.1297, Validation Loss: 0.1786\n",
            "k-fold 2  Train Loss: 0.1323, Validation Loss: 0.1807\n",
            "k-fold 2  Train Loss: 0.1332, Validation Loss: 0.1812\n",
            "k-fold 2  Train Loss: 0.1338, Validation Loss: 0.1814\n",
            "k-fold 2  Train Loss: 0.1336, Validation Loss: 0.1810\n",
            "k-fold 2  Train Loss: 0.1324, Validation Loss: 0.1796\n",
            "k-fold 2  Train Loss: 0.1308, Validation Loss: 0.1779\n",
            "k-fold 2  Train Loss: 0.1288, Validation Loss: 0.1757\n",
            "k-fold 2  Train Loss: 0.1272, Validation Loss: 0.1738\n",
            "k-fold 2  Train Loss: 0.1259, Validation Loss: 0.1722\n",
            "k-fold 2  Train Loss: 0.1249, Validation Loss: 0.1708\n",
            "k-fold 2  Train Loss: 0.1240, Validation Loss: 0.1695\n",
            "k-fold 2  Train Loss: 0.1233, Validation Loss: 0.1684\n",
            "k-fold 2  Train Loss: 0.1230, Validation Loss: 0.1678\n",
            "k-fold 2  Train Loss: 0.1231, Validation Loss: 0.1679\n",
            "k-fold 2  Train Loss: 0.1235, Validation Loss: 0.1683\n",
            "k-fold 2  Train Loss: 0.1236, Validation Loss: 0.1683\n",
            "k-fold 2  Train Loss: 0.1241, Validation Loss: 0.1687\n",
            "k-fold 2  Train Loss: 0.1247, Validation Loss: 0.1693\n",
            "k-fold 2  Train Loss: 0.1258, Validation Loss: 0.1703\n",
            "k-fold 2  Train Loss: 0.1265, Validation Loss: 0.1708\n",
            "k-fold 2  Train Loss: 0.1268, Validation Loss: 0.1709\n",
            "k-fold 2  Train Loss: 0.1279, Validation Loss: 0.1718\n",
            "k-fold 2  Train Loss: 0.1285, Validation Loss: 0.1721\n",
            "k-fold 2  Train Loss: 0.1285, Validation Loss: 0.1719\n",
            "k-fold 2  Train Loss: 0.1282, Validation Loss: 0.1714\n",
            "k-fold 2  Train Loss: 0.1262, Validation Loss: 0.1696\n",
            "k-fold 2  Train Loss: 0.1245, Validation Loss: 0.1680\n",
            "k-fold 2  Train Loss: 0.1233, Validation Loss: 0.1667\n",
            "k-fold 2  Train Loss: 0.1223, Validation Loss: 0.1655\n",
            "k-fold 2  Train Loss: 0.1220, Validation Loss: 0.1650\n",
            "k-fold 2  Train Loss: 0.1217, Validation Loss: 0.1646\n",
            "k-fold 2  Train Loss: 0.1215, Validation Loss: 0.1642\n",
            "k-fold 2  Train Loss: 0.1215, Validation Loss: 0.1641\n",
            "k-fold 2  Train Loss: 0.1222, Validation Loss: 0.1650\n",
            "k-fold 2  Train Loss: 0.1231, Validation Loss: 0.1659\n",
            "k-fold 2  Train Loss: 0.1240, Validation Loss: 0.1667\n",
            "k-fold 2  Train Loss: 0.1246, Validation Loss: 0.1673\n",
            "k-fold 2  Train Loss: 0.1262, Validation Loss: 0.1687\n",
            "k-fold 2  Train Loss: 0.1278, Validation Loss: 0.1701\n",
            "k-fold 2  Train Loss: 0.1293, Validation Loss: 0.1714\n",
            "k-fold 2  Train Loss: 0.1302, Validation Loss: 0.1722\n",
            "k-fold 2  Train Loss: 0.1307, Validation Loss: 0.1727\n",
            "k-fold 2  Train Loss: 0.1301, Validation Loss: 0.1722\n",
            "k-fold 2  Train Loss: 0.1297, Validation Loss: 0.1718\n",
            "k-fold 2  Train Loss: 0.1285, Validation Loss: 0.1707\n",
            "k-fold 2  Train Loss: 0.1269, Validation Loss: 0.1692\n",
            "k-fold 2  Train Loss: 0.1256, Validation Loss: 0.1680\n",
            "k-fold 2  Train Loss: 0.1242, Validation Loss: 0.1666\n",
            "k-fold 2  Train Loss: 0.1223, Validation Loss: 0.1647\n",
            "k-fold 2  Train Loss: 0.1216, Validation Loss: 0.1641\n",
            "k-fold 2  Train Loss: 0.1206, Validation Loss: 0.1633\n",
            "k-fold 2  Train Loss: 0.1201, Validation Loss: 0.1630\n",
            "k-fold 2  Train Loss: 0.1196, Validation Loss: 0.1626\n",
            "k-fold 2  Train Loss: 0.1194, Validation Loss: 0.1625\n",
            "k-fold 2  Train Loss: 0.1192, Validation Loss: 0.1625\n",
            "k-fold 2  Train Loss: 0.1189, Validation Loss: 0.1622\n",
            "k-fold 2  Train Loss: 0.1190, Validation Loss: 0.1626\n",
            "k-fold 2  Train Loss: 0.1199, Validation Loss: 0.1638\n",
            "k-fold 2  Train Loss: 0.1206, Validation Loss: 0.1646\n",
            "k-fold 2  Train Loss: 0.1216, Validation Loss: 0.1658\n",
            "k-fold 2  Train Loss: 0.1222, Validation Loss: 0.1665\n",
            "k-fold 2  Train Loss: 0.1226, Validation Loss: 0.1668\n",
            "k-fold 2  Train Loss: 0.1220, Validation Loss: 0.1661\n",
            "k-fold 2  Train Loss: 0.1212, Validation Loss: 0.1652\n",
            "k-fold 2  Train Loss: 0.1206, Validation Loss: 0.1645\n",
            "k-fold 2  Train Loss: 0.1208, Validation Loss: 0.1646\n",
            "k-fold 2  Train Loss: 0.1209, Validation Loss: 0.1648\n",
            "k-fold 2  Train Loss: 0.1207, Validation Loss: 0.1647\n",
            "k-fold 2  Train Loss: 0.1200, Validation Loss: 0.1640\n",
            "k-fold 2  Train Loss: 0.1191, Validation Loss: 0.1630\n",
            "k-fold 2  Train Loss: 0.1182, Validation Loss: 0.1619\n",
            "k-fold 2  Train Loss: 0.1174, Validation Loss: 0.1608\n",
            "k-fold 2  Train Loss: 0.1167, Validation Loss: 0.1597\n",
            "k-fold 2  Train Loss: 0.1159, Validation Loss: 0.1582\n",
            "k-fold 2  Train Loss: 0.1155, Validation Loss: 0.1573\n",
            "k-fold 2  Train Loss: 0.1153, Validation Loss: 0.1566\n",
            "k-fold 2  Train Loss: 0.1151, Validation Loss: 0.1562\n",
            "k-fold 2  Train Loss: 0.1151, Validation Loss: 0.1564\n",
            "k-fold 2  Train Loss: 0.1153, Validation Loss: 0.1571\n",
            "k-fold 2  Train Loss: 0.1155, Validation Loss: 0.1577\n",
            "k-fold 2  Train Loss: 0.1160, Validation Loss: 0.1585\n",
            "k-fold 2  Train Loss: 0.1163, Validation Loss: 0.1592\n",
            "k-fold 2  Train Loss: 0.1168, Validation Loss: 0.1599\n",
            "k-fold 2  Train Loss: 0.1173, Validation Loss: 0.1608\n",
            "k-fold 2  Train Loss: 0.1177, Validation Loss: 0.1615\n",
            "k-fold 2  Train Loss: 0.1178, Validation Loss: 0.1617\n",
            "k-fold 2  Train Loss: 0.1176, Validation Loss: 0.1618\n",
            "k-fold 2  Train Loss: 0.1179, Validation Loss: 0.1623\n",
            "k-fold 2  Train Loss: 0.1176, Validation Loss: 0.1621\n",
            "k-fold 2  Train Loss: 0.1175, Validation Loss: 0.1623\n",
            "k-fold 2  Train Loss: 0.1177, Validation Loss: 0.1627\n",
            "k-fold 2  Train Loss: 0.1173, Validation Loss: 0.1624\n",
            "k-fold 2  Train Loss: 0.1174, Validation Loss: 0.1628\n",
            "k-fold 2  Train Loss: 0.1177, Validation Loss: 0.1633\n",
            "k-fold 2  Train Loss: 0.1178, Validation Loss: 0.1635\n",
            "k-fold 2  Train Loss: 0.1176, Validation Loss: 0.1634\n",
            "k-fold 2  Train Loss: 0.1170, Validation Loss: 0.1628\n",
            "k-fold 2  Train Loss: 0.1161, Validation Loss: 0.1618\n",
            "k-fold 2  Train Loss: 0.1159, Validation Loss: 0.1614\n",
            "k-fold 2  Train Loss: 0.1154, Validation Loss: 0.1607\n",
            "k-fold 2  Train Loss: 0.1150, Validation Loss: 0.1601\n",
            "k-fold 2  Train Loss: 0.1147, Validation Loss: 0.1596\n",
            "k-fold 2  Train Loss: 0.1144, Validation Loss: 0.1592\n",
            "k-fold 2  Train Loss: 0.1140, Validation Loss: 0.1587\n",
            "k-fold 2  Train Loss: 0.1141, Validation Loss: 0.1587\n",
            "k-fold 2  Train Loss: 0.1141, Validation Loss: 0.1586\n",
            "k-fold 2  Train Loss: 0.1141, Validation Loss: 0.1585\n",
            "k-fold 2  Train Loss: 0.1140, Validation Loss: 0.1584\n",
            "k-fold 2  Train Loss: 0.1143, Validation Loss: 0.1588\n",
            "k-fold 2  Train Loss: 0.1149, Validation Loss: 0.1596\n",
            "k-fold 2  Train Loss: 0.1156, Validation Loss: 0.1604\n",
            "k-fold 2  Train Loss: 0.1157, Validation Loss: 0.1606\n",
            "k-fold 2  Train Loss: 0.1157, Validation Loss: 0.1606\n",
            "k-fold 2  Train Loss: 0.1145, Validation Loss: 0.1591\n",
            "k-fold 2  Train Loss: 0.1135, Validation Loss: 0.1577\n",
            "k-fold 2  Train Loss: 0.1129, Validation Loss: 0.1566\n",
            "k-fold 2  Train Loss: 0.1123, Validation Loss: 0.1554\n",
            "k-fold 2  Train Loss: 0.1121, Validation Loss: 0.1546\n",
            "k-fold 2  Train Loss: 0.1120, Validation Loss: 0.1542\n",
            "k-fold 2  Train Loss: 0.1120, Validation Loss: 0.1540\n",
            "k-fold 2  Train Loss: 0.1119, Validation Loss: 0.1537\n",
            "k-fold 2  Train Loss: 0.1120, Validation Loss: 0.1533\n",
            "k-fold 2  Train Loss: 0.1119, Validation Loss: 0.1532\n",
            "k-fold 2  Train Loss: 0.1117, Validation Loss: 0.1532\n",
            "k-fold 2  Train Loss: 0.1115, Validation Loss: 0.1535\n",
            "k-fold 2  Train Loss: 0.1114, Validation Loss: 0.1538\n",
            "k-fold 2  Train Loss: 0.1114, Validation Loss: 0.1543\n",
            "k-fold 2  Train Loss: 0.1116, Validation Loss: 0.1552\n",
            "k-fold 2  Train Loss: 0.1122, Validation Loss: 0.1565\n",
            "k-fold 2  Train Loss: 0.1127, Validation Loss: 0.1573\n",
            "k-fold 2  Train Loss: 0.1132, Validation Loss: 0.1580\n",
            "k-fold 2  Train Loss: 0.1139, Validation Loss: 0.1592\n",
            "k-fold 2  Train Loss: 0.1144, Validation Loss: 0.1598\n",
            "k-fold 2  Train Loss: 0.1154, Validation Loss: 0.1612\n",
            "k-fold 2  Train Loss: 0.1161, Validation Loss: 0.1621\n",
            "k-fold 2  Train Loss: 0.1168, Validation Loss: 0.1630\n",
            "k-fold 2  Train Loss: 0.1170, Validation Loss: 0.1633\n",
            "k-fold 2  Train Loss: 0.1176, Validation Loss: 0.1641\n",
            "k-fold 2  Train Loss: 0.1182, Validation Loss: 0.1648\n",
            "k-fold 2  Train Loss: 0.1180, Validation Loss: 0.1646\n",
            "k-fold 2  Train Loss: 0.1174, Validation Loss: 0.1639\n",
            "k-fold 2  Train Loss: 0.1170, Validation Loss: 0.1634\n",
            "k-fold 2  Train Loss: 0.1158, Validation Loss: 0.1621\n",
            "k-fold 2  Train Loss: 0.1145, Validation Loss: 0.1605\n",
            "k-fold 2  Train Loss: 0.1135, Validation Loss: 0.1591\n",
            "k-fold 2  Train Loss: 0.1125, Validation Loss: 0.1577\n",
            "k-fold 2  Train Loss: 0.1115, Validation Loss: 0.1561\n",
            "k-fold 2  Train Loss: 0.1110, Validation Loss: 0.1551\n",
            "k-fold 2  Train Loss: 0.1104, Validation Loss: 0.1539\n",
            "k-fold 2  Train Loss: 0.1104, Validation Loss: 0.1540\n",
            "k-fold 2  Train Loss: 0.1104, Validation Loss: 0.1541\n",
            "k-fold 2  Train Loss: 0.1103, Validation Loss: 0.1538\n",
            "k-fold 2  Train Loss: 0.1104, Validation Loss: 0.1541\n",
            "k-fold 2  Train Loss: 0.1104, Validation Loss: 0.1544\n",
            "k-fold 2  Train Loss: 0.1108, Validation Loss: 0.1555\n",
            "k-fold 2  Train Loss: 0.1113, Validation Loss: 0.1564\n",
            "k-fold 2  Train Loss: 0.1117, Validation Loss: 0.1572\n",
            "k-fold 2  Train Loss: 0.1132, Validation Loss: 0.1593\n",
            "k-fold 2  Train Loss: 0.1150, Validation Loss: 0.1617\n",
            "k-fold 2  Train Loss: 0.1162, Validation Loss: 0.1633\n",
            "k-fold 2  Train Loss: 0.1177, Validation Loss: 0.1650\n",
            "k-fold 2  Train Loss: 0.1174, Validation Loss: 0.1649\n",
            "k-fold 2  Train Loss: 0.1169, Validation Loss: 0.1642\n",
            "k-fold 2  Train Loss: 0.1159, Validation Loss: 0.1631\n",
            "k-fold 2  Train Loss: 0.1151, Validation Loss: 0.1622\n",
            "k-fold 2  Train Loss: 0.1152, Validation Loss: 0.1622\n",
            "k-fold 2  Train Loss: 0.1145, Validation Loss: 0.1613\n",
            "k-fold 2  Train Loss: 0.1135, Validation Loss: 0.1602\n",
            "k-fold 2  Train Loss: 0.1124, Validation Loss: 0.1587\n",
            "k-fold 2  Train Loss: 0.1114, Validation Loss: 0.1574\n",
            "k-fold 2  Train Loss: 0.1109, Validation Loss: 0.1568\n",
            "k-fold 2  Train Loss: 0.1108, Validation Loss: 0.1566\n",
            "k-fold 2  Train Loss: 0.1107, Validation Loss: 0.1563\n",
            "k-fold 2  Train Loss: 0.1102, Validation Loss: 0.1554\n",
            "k-fold 2  Train Loss: 0.1098, Validation Loss: 0.1546\n",
            "k-fold 2  Train Loss: 0.1099, Validation Loss: 0.1545\n",
            "k-fold 2  Train Loss: 0.1095, Validation Loss: 0.1539\n",
            "k-fold 2  Train Loss: 0.1089, Validation Loss: 0.1530\n",
            "k-fold 2  Train Loss: 0.1083, Validation Loss: 0.1519\n",
            "k-fold 2  Train Loss: 0.1079, Validation Loss: 0.1510\n",
            "k-fold 2  Train Loss: 0.1077, Validation Loss: 0.1504\n",
            "k-fold 2  Train Loss: 0.1078, Validation Loss: 0.1505\n",
            "k-fold 2  Train Loss: 0.1078, Validation Loss: 0.1509\n",
            "k-fold 2  Train Loss: 0.1078, Validation Loss: 0.1512\n",
            "k-fold 2  Train Loss: 0.1079, Validation Loss: 0.1517\n",
            "k-fold 2  Train Loss: 0.1075, Validation Loss: 0.1511\n",
            "k-fold 2  Train Loss: 0.1072, Validation Loss: 0.1507\n",
            "k-fold 2  Train Loss: 0.1072, Validation Loss: 0.1510\n",
            "k-fold 2  Train Loss: 0.1071, Validation Loss: 0.1513\n",
            "k-fold 2  Train Loss: 0.1072, Validation Loss: 0.1518\n",
            "k-fold 2  Train Loss: 0.1074, Validation Loss: 0.1526\n",
            "k-fold 2  Train Loss: 0.1076, Validation Loss: 0.1533\n",
            "k-fold 2  Train Loss: 0.1081, Validation Loss: 0.1544\n",
            "k-fold 2  Train Loss: 0.1077, Validation Loss: 0.1539\n",
            "k-fold 2  Train Loss: 0.1074, Validation Loss: 0.1536\n",
            "k-fold 2  Train Loss: 0.1068, Validation Loss: 0.1526\n",
            "k-fold 2  Train Loss: 0.1066, Validation Loss: 0.1524\n",
            "k-fold 2  Train Loss: 0.1066, Validation Loss: 0.1525\n",
            "k-fold 2  Train Loss: 0.1066, Validation Loss: 0.1526\n",
            "k-fold 2  Train Loss: 0.1066, Validation Loss: 0.1527\n",
            "k-fold 2  Train Loss: 0.1066, Validation Loss: 0.1530\n",
            "k-fold 2  Train Loss: 0.1068, Validation Loss: 0.1532\n",
            "k-fold 2  Train Loss: 0.1070, Validation Loss: 0.1535\n",
            "k-fold 2  Train Loss: 0.1073, Validation Loss: 0.1539\n",
            "k-fold 2  Train Loss: 0.1076, Validation Loss: 0.1541\n",
            "k-fold 2  Train Loss: 0.1079, Validation Loss: 0.1543\n",
            "k-fold 2  Train Loss: 0.1079, Validation Loss: 0.1541\n",
            "k-fold 2  Train Loss: 0.1074, Validation Loss: 0.1532\n",
            "k-fold 2  Train Loss: 0.1071, Validation Loss: 0.1526\n",
            "k-fold 2  Train Loss: 0.1067, Validation Loss: 0.1516\n",
            "k-fold 2  Train Loss: 0.1063, Validation Loss: 0.1506\n",
            "k-fold 2  Train Loss: 0.1061, Validation Loss: 0.1498\n",
            "k-fold 2  Train Loss: 0.1059, Validation Loss: 0.1490\n",
            "k-fold 2  Train Loss: 0.1059, Validation Loss: 0.1488\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1490\n",
            "k-fold 2  Train Loss: 0.1061, Validation Loss: 0.1493\n",
            "k-fold 2  Train Loss: 0.1063, Validation Loss: 0.1496\n",
            "k-fold 2  Train Loss: 0.1062, Validation Loss: 0.1493\n",
            "k-fold 2  Train Loss: 0.1061, Validation Loss: 0.1492\n",
            "k-fold 2  Train Loss: 0.1061, Validation Loss: 0.1491\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1489\n",
            "k-fold 2  Train Loss: 0.1057, Validation Loss: 0.1486\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1483\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1486\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1489\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1489\n",
            "k-fold 2  Train Loss: 0.1057, Validation Loss: 0.1493\n",
            "k-fold 2  Train Loss: 0.1058, Validation Loss: 0.1496\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1492\n",
            "k-fold 2  Train Loss: 0.1070, Validation Loss: 0.1508\n",
            "k-fold 2  Train Loss: 0.1088, Validation Loss: 0.1527\n",
            "k-fold 2  Train Loss: 0.1103, Validation Loss: 0.1541\n",
            "k-fold 2  Train Loss: 0.1108, Validation Loss: 0.1543\n",
            "k-fold 2  Train Loss: 0.1098, Validation Loss: 0.1524\n",
            "k-fold 2  Train Loss: 0.1090, Validation Loss: 0.1510\n",
            "k-fold 2  Train Loss: 0.1085, Validation Loss: 0.1498\n",
            "k-fold 2  Train Loss: 0.1076, Validation Loss: 0.1482\n",
            "k-fold 2  Train Loss: 0.1065, Validation Loss: 0.1461\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1445\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1436\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1432\n",
            "k-fold 2  Train Loss: 0.1061, Validation Loss: 0.1429\n",
            "k-fold 2  Train Loss: 0.1061, Validation Loss: 0.1427\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.1058, Validation Loss: 0.1425\n",
            "k-fold 2  Train Loss: 0.1054, Validation Loss: 0.1428\n",
            "k-fold 2  Train Loss: 0.1050, Validation Loss: 0.1432\n",
            "k-fold 2  Train Loss: 0.1048, Validation Loss: 0.1439\n",
            "k-fold 2  Train Loss: 0.1047, Validation Loss: 0.1443\n",
            "k-fold 2  Train Loss: 0.1048, Validation Loss: 0.1452\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1470\n",
            "k-fold 2  Train Loss: 0.1062, Validation Loss: 0.1485\n",
            "k-fold 2  Train Loss: 0.1071, Validation Loss: 0.1500\n",
            "k-fold 2  Train Loss: 0.1089, Validation Loss: 0.1525\n",
            "k-fold 2  Train Loss: 0.1103, Validation Loss: 0.1546\n",
            "k-fold 2  Train Loss: 0.1114, Validation Loss: 0.1560\n",
            "k-fold 2  Train Loss: 0.1114, Validation Loss: 0.1562\n",
            "k-fold 2  Train Loss: 0.1118, Validation Loss: 0.1566\n",
            "k-fold 2  Train Loss: 0.1116, Validation Loss: 0.1565\n",
            "k-fold 2  Train Loss: 0.1116, Validation Loss: 0.1564\n",
            "k-fold 2  Train Loss: 0.1112, Validation Loss: 0.1559\n",
            "k-fold 2  Train Loss: 0.1092, Validation Loss: 0.1536\n",
            "k-fold 2  Train Loss: 0.1071, Validation Loss: 0.1510\n",
            "k-fold 2  Train Loss: 0.1057, Validation Loss: 0.1490\n",
            "k-fold 2  Train Loss: 0.1049, Validation Loss: 0.1476\n",
            "k-fold 2  Train Loss: 0.1043, Validation Loss: 0.1466\n",
            "k-fold 2  Train Loss: 0.1040, Validation Loss: 0.1459\n",
            "k-fold 2  Train Loss: 0.1039, Validation Loss: 0.1458\n",
            "k-fold 2  Train Loss: 0.1039, Validation Loss: 0.1460\n",
            "k-fold 2  Train Loss: 0.1035, Validation Loss: 0.1451\n",
            "k-fold 2  Train Loss: 0.1033, Validation Loss: 0.1443\n",
            "k-fold 2  Train Loss: 0.1033, Validation Loss: 0.1435\n",
            "k-fold 2  Train Loss: 0.1034, Validation Loss: 0.1432\n",
            "k-fold 2  Train Loss: 0.1032, Validation Loss: 0.1434\n",
            "k-fold 2  Train Loss: 0.1031, Validation Loss: 0.1442\n",
            "k-fold 2  Train Loss: 0.1033, Validation Loss: 0.1449\n",
            "k-fold 2  Train Loss: 0.1039, Validation Loss: 0.1462\n",
            "k-fold 2  Train Loss: 0.1045, Validation Loss: 0.1473\n",
            "k-fold 2  Train Loss: 0.1050, Validation Loss: 0.1480\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1489\n",
            "k-fold 2  Train Loss: 0.1062, Validation Loss: 0.1499\n",
            "k-fold 2  Train Loss: 0.1066, Validation Loss: 0.1503\n",
            "k-fold 2  Train Loss: 0.1069, Validation Loss: 0.1509\n",
            "k-fold 2  Train Loss: 0.1069, Validation Loss: 0.1508\n",
            "k-fold 2  Train Loss: 0.1065, Validation Loss: 0.1504\n",
            "k-fold 2  Train Loss: 0.1059, Validation Loss: 0.1496\n",
            "k-fold 2  Train Loss: 0.1056, Validation Loss: 0.1492\n",
            "k-fold 2  Train Loss: 0.1052, Validation Loss: 0.1488\n",
            "k-fold 2  Train Loss: 0.1047, Validation Loss: 0.1481\n",
            "k-fold 2  Train Loss: 0.1041, Validation Loss: 0.1474\n",
            "k-fold 2  Train Loss: 0.1036, Validation Loss: 0.1466\n",
            "k-fold 2  Train Loss: 0.1035, Validation Loss: 0.1465\n",
            "k-fold 2  Train Loss: 0.1037, Validation Loss: 0.1470\n",
            "k-fold 2  Train Loss: 0.1042, Validation Loss: 0.1478\n",
            "k-fold 2  Train Loss: 0.1052, Validation Loss: 0.1493\n",
            "k-fold 2  Train Loss: 0.1059, Validation Loss: 0.1503\n",
            "k-fold 2  Train Loss: 0.1068, Validation Loss: 0.1514\n",
            "k-fold 2  Train Loss: 0.1074, Validation Loss: 0.1522\n",
            "k-fold 2  Train Loss: 0.1073, Validation Loss: 0.1520\n",
            "k-fold 2  Train Loss: 0.1070, Validation Loss: 0.1517\n",
            "k-fold 2  Train Loss: 0.1069, Validation Loss: 0.1516\n",
            "k-fold 2  Train Loss: 0.1064, Validation Loss: 0.1511\n",
            "k-fold 2  Train Loss: 0.1057, Validation Loss: 0.1503\n",
            "k-fold 2  Train Loss: 0.1046, Validation Loss: 0.1490\n",
            "k-fold 2  Train Loss: 0.1039, Validation Loss: 0.1480\n",
            "k-fold 2  Train Loss: 0.1029, Validation Loss: 0.1465\n",
            "k-fold 2  Train Loss: 0.1022, Validation Loss: 0.1454\n",
            "k-fold 2  Train Loss: 0.1019, Validation Loss: 0.1448\n",
            "k-fold 2  Train Loss: 0.1017, Validation Loss: 0.1443\n",
            "k-fold 2  Train Loss: 0.1016, Validation Loss: 0.1440\n",
            "k-fold 2  Train Loss: 0.1014, Validation Loss: 0.1436\n",
            "k-fold 2  Train Loss: 0.1014, Validation Loss: 0.1434\n",
            "k-fold 2  Train Loss: 0.1013, Validation Loss: 0.1437\n",
            "k-fold 2  Train Loss: 0.1013, Validation Loss: 0.1437\n",
            "k-fold 2  Train Loss: 0.1013, Validation Loss: 0.1444\n",
            "k-fold 2  Train Loss: 0.1014, Validation Loss: 0.1453\n",
            "k-fold 2  Train Loss: 0.1020, Validation Loss: 0.1467\n",
            "k-fold 2  Train Loss: 0.1029, Validation Loss: 0.1485\n",
            "k-fold 2  Train Loss: 0.1047, Validation Loss: 0.1511\n",
            "k-fold 2  Train Loss: 0.1059, Validation Loss: 0.1528\n",
            "k-fold 2  Train Loss: 0.1068, Validation Loss: 0.1540\n",
            "k-fold 2  Train Loss: 0.1074, Validation Loss: 0.1546\n",
            "k-fold 2  Train Loss: 0.1072, Validation Loss: 0.1543\n",
            "k-fold 2  Train Loss: 0.1065, Validation Loss: 0.1534\n",
            "k-fold 2  Train Loss: 0.1060, Validation Loss: 0.1526\n",
            "k-fold 2  Train Loss: 0.1052, Validation Loss: 0.1515\n",
            "k-fold 2  Train Loss: 0.1047, Validation Loss: 0.1508\n",
            "k-fold 2  Train Loss: 0.1041, Validation Loss: 0.1499\n",
            "k-fold 2  Train Loss: 0.1039, Validation Loss: 0.1497\n",
            "k-fold 2  Train Loss: 0.1043, Validation Loss: 0.1501\n",
            "k-fold 2  Train Loss: 0.1044, Validation Loss: 0.1501\n",
            "k-fold 2  Train Loss: 0.1047, Validation Loss: 0.1505\n",
            "k-fold 2  Train Loss: 0.1047, Validation Loss: 0.1504\n",
            "k-fold 2  Train Loss: 0.1051, Validation Loss: 0.1509\n",
            "k-fold 2  Train Loss: 0.1057, Validation Loss: 0.1515\n",
            "k-fold 2  Train Loss: 0.1056, Validation Loss: 0.1513\n",
            "k-fold 2  Train Loss: 0.1055, Validation Loss: 0.1512\n",
            "k-fold 2  Train Loss: 0.1040, Validation Loss: 0.1492\n",
            "k-fold 2  Train Loss: 0.1030, Validation Loss: 0.1480\n",
            "k-fold 2  Train Loss: 0.1023, Validation Loss: 0.1470\n",
            "k-fold 2  Train Loss: 0.1017, Validation Loss: 0.1460\n",
            "k-fold 2  Train Loss: 0.1010, Validation Loss: 0.1448\n",
            "k-fold 2  Train Loss: 0.1005, Validation Loss: 0.1437\n",
            "k-fold 2  Train Loss: 0.1002, Validation Loss: 0.1427\n",
            "k-fold 2  Train Loss: 0.1000, Validation Loss: 0.1423\n",
            "k-fold 2  Train Loss: 0.1001, Validation Loss: 0.1429\n",
            "k-fold 2  Train Loss: 0.1005, Validation Loss: 0.1437\n",
            "k-fold 2  Train Loss: 0.1013, Validation Loss: 0.1450\n",
            "k-fold 2  Train Loss: 0.1020, Validation Loss: 0.1459\n",
            "k-fold 2  Train Loss: 0.1028, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.1032, Validation Loss: 0.1472\n",
            "k-fold 2  Train Loss: 0.1034, Validation Loss: 0.1474\n",
            "k-fold 2  Train Loss: 0.1036, Validation Loss: 0.1475\n",
            "k-fold 2  Train Loss: 0.1027, Validation Loss: 0.1462\n",
            "k-fold 2  Train Loss: 0.1020, Validation Loss: 0.1452\n",
            "k-fold 2  Train Loss: 0.1015, Validation Loss: 0.1444\n",
            "k-fold 2  Train Loss: 0.1009, Validation Loss: 0.1435\n",
            "k-fold 2  Train Loss: 0.1003, Validation Loss: 0.1425\n",
            "k-fold 2  Train Loss: 0.0999, Validation Loss: 0.1419\n",
            "k-fold 2  Train Loss: 0.0997, Validation Loss: 0.1418\n",
            "k-fold 2  Train Loss: 0.0994, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0992, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0989, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1415\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.0991, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0994, Validation Loss: 0.1449\n",
            "k-fold 2  Train Loss: 0.0996, Validation Loss: 0.1459\n",
            "k-fold 2  Train Loss: 0.1000, Validation Loss: 0.1470\n",
            "k-fold 2  Train Loss: 0.1006, Validation Loss: 0.1484\n",
            "k-fold 2  Train Loss: 0.1011, Validation Loss: 0.1495\n",
            "k-fold 2  Train Loss: 0.1014, Validation Loss: 0.1501\n",
            "k-fold 2  Train Loss: 0.1008, Validation Loss: 0.1491\n",
            "k-fold 2  Train Loss: 0.1002, Validation Loss: 0.1483\n",
            "k-fold 2  Train Loss: 0.0998, Validation Loss: 0.1476\n",
            "k-fold 2  Train Loss: 0.0997, Validation Loss: 0.1475\n",
            "k-fold 2  Train Loss: 0.0993, Validation Loss: 0.1467\n",
            "k-fold 2  Train Loss: 0.0992, Validation Loss: 0.1463\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1459\n",
            "k-fold 2  Train Loss: 0.0987, Validation Loss: 0.1452\n",
            "k-fold 2  Train Loss: 0.0984, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0984, Validation Loss: 0.1428\n",
            "k-fold 2  Train Loss: 0.0987, Validation Loss: 0.1419\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1416\n",
            "k-fold 2  Train Loss: 0.0997, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0998, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0994, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0991, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0989, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0983, Validation Loss: 0.1415\n",
            "k-fold 2  Train Loss: 0.0980, Validation Loss: 0.1422\n",
            "k-fold 2  Train Loss: 0.0978, Validation Loss: 0.1433\n",
            "k-fold 2  Train Loss: 0.0979, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0982, Validation Loss: 0.1451\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1464\n",
            "k-fold 2  Train Loss: 0.0991, Validation Loss: 0.1471\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.0991, Validation Loss: 0.1469\n",
            "k-fold 2  Train Loss: 0.0991, Validation Loss: 0.1467\n",
            "k-fold 2  Train Loss: 0.0992, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1463\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1459\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1461\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1461\n",
            "k-fold 2  Train Loss: 0.0990, Validation Loss: 0.1460\n",
            "k-fold 2  Train Loss: 0.0986, Validation Loss: 0.1453\n",
            "k-fold 2  Train Loss: 0.0984, Validation Loss: 0.1448\n",
            "k-fold 2  Train Loss: 0.0979, Validation Loss: 0.1437\n",
            "k-fold 2  Train Loss: 0.0975, Validation Loss: 0.1428\n",
            "k-fold 2  Train Loss: 0.0973, Validation Loss: 0.1422\n",
            "k-fold 2  Train Loss: 0.0973, Validation Loss: 0.1423\n",
            "k-fold 2  Train Loss: 0.0977, Validation Loss: 0.1434\n",
            "k-fold 2  Train Loss: 0.0983, Validation Loss: 0.1446\n",
            "k-fold 2  Train Loss: 0.0989, Validation Loss: 0.1454\n",
            "k-fold 2  Train Loss: 0.0997, Validation Loss: 0.1465\n",
            "k-fold 2  Train Loss: 0.1005, Validation Loss: 0.1475\n",
            "k-fold 2  Train Loss: 0.1016, Validation Loss: 0.1488\n",
            "k-fold 2  Train Loss: 0.1020, Validation Loss: 0.1490\n",
            "k-fold 2  Train Loss: 0.1021, Validation Loss: 0.1491\n",
            "k-fold 2  Train Loss: 0.1036, Validation Loss: 0.1506\n",
            "k-fold 2  Train Loss: 0.1043, Validation Loss: 0.1513\n",
            "k-fold 2  Train Loss: 0.1044, Validation Loss: 0.1511\n",
            "k-fold 2  Train Loss: 0.1040, Validation Loss: 0.1505\n",
            "k-fold 2  Train Loss: 0.1043, Validation Loss: 0.1504\n",
            "k-fold 2  Train Loss: 0.1039, Validation Loss: 0.1497\n",
            "k-fold 2  Train Loss: 0.1030, Validation Loss: 0.1483\n",
            "k-fold 2  Train Loss: 0.1021, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.1012, Validation Loss: 0.1453\n",
            "k-fold 2  Train Loss: 0.1008, Validation Loss: 0.1446\n",
            "k-fold 2  Train Loss: 0.1000, Validation Loss: 0.1435\n",
            "k-fold 2  Train Loss: 0.0996, Validation Loss: 0.1427\n",
            "k-fold 2  Train Loss: 0.0989, Validation Loss: 0.1414\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0987, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0985, Validation Loss: 0.1407\n",
            "k-fold 2  Train Loss: 0.0982, Validation Loss: 0.1404\n",
            "k-fold 2  Train Loss: 0.0979, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0976, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0974, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0972, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0970, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0968, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0968, Validation Loss: 0.1401\n",
            "k-fold 2  Train Loss: 0.0969, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0970, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0973, Validation Loss: 0.1431\n",
            "k-fold 2  Train Loss: 0.0977, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0979, Validation Loss: 0.1449\n",
            "k-fold 2  Train Loss: 0.0981, Validation Loss: 0.1455\n",
            "k-fold 2  Train Loss: 0.0979, Validation Loss: 0.1454\n",
            "k-fold 2  Train Loss: 0.0975, Validation Loss: 0.1451\n",
            "k-fold 2  Train Loss: 0.0969, Validation Loss: 0.1442\n",
            "k-fold 2  Train Loss: 0.0965, Validation Loss: 0.1435\n",
            "k-fold 2  Train Loss: 0.0961, Validation Loss: 0.1429\n",
            "k-fold 2  Train Loss: 0.0958, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0956, Validation Loss: 0.1414\n",
            "k-fold 2  Train Loss: 0.0955, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0956, Validation Loss: 0.1415\n",
            "k-fold 2  Train Loss: 0.0956, Validation Loss: 0.1418\n",
            "k-fold 2  Train Loss: 0.0957, Validation Loss: 0.1424\n",
            "k-fold 2  Train Loss: 0.0960, Validation Loss: 0.1432\n",
            "k-fold 2  Train Loss: 0.0964, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0967, Validation Loss: 0.1444\n",
            "k-fold 2  Train Loss: 0.0966, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0967, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0968, Validation Loss: 0.1440\n",
            "k-fold 2  Train Loss: 0.0970, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0972, Validation Loss: 0.1443\n",
            "k-fold 2  Train Loss: 0.0973, Validation Loss: 0.1443\n",
            "k-fold 2  Train Loss: 0.0974, Validation Loss: 0.1445\n",
            "k-fold 2  Train Loss: 0.0976, Validation Loss: 0.1447\n",
            "k-fold 2  Train Loss: 0.0979, Validation Loss: 0.1451\n",
            "k-fold 2  Train Loss: 0.0981, Validation Loss: 0.1454\n",
            "k-fold 2  Train Loss: 0.0981, Validation Loss: 0.1455\n",
            "k-fold 2  Train Loss: 0.0978, Validation Loss: 0.1451\n",
            "k-fold 2  Train Loss: 0.0974, Validation Loss: 0.1445\n",
            "k-fold 2  Train Loss: 0.0971, Validation Loss: 0.1440\n",
            "k-fold 2  Train Loss: 0.0972, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0970, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0960, Validation Loss: 0.1422\n",
            "k-fold 2  Train Loss: 0.0956, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0954, Validation Loss: 0.1407\n",
            "k-fold 2  Train Loss: 0.0952, Validation Loss: 0.1403\n",
            "k-fold 2  Train Loss: 0.0953, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0958, Validation Loss: 0.1385\n",
            "k-fold 2  Train Loss: 0.0959, Validation Loss: 0.1385\n",
            "k-fold 2  Train Loss: 0.0959, Validation Loss: 0.1385\n",
            "k-fold 2  Train Loss: 0.0954, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0950, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0947, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0945, Validation Loss: 0.1403\n",
            "k-fold 2  Train Loss: 0.0945, Validation Loss: 0.1417\n",
            "k-fold 2  Train Loss: 0.0949, Validation Loss: 0.1431\n",
            "k-fold 2  Train Loss: 0.0955, Validation Loss: 0.1448\n",
            "k-fold 2  Train Loss: 0.0962, Validation Loss: 0.1463\n",
            "k-fold 2  Train Loss: 0.0968, Validation Loss: 0.1474\n",
            "k-fold 2  Train Loss: 0.0973, Validation Loss: 0.1481\n",
            "k-fold 2  Train Loss: 0.0975, Validation Loss: 0.1483\n",
            "k-fold 2  Train Loss: 0.0974, Validation Loss: 0.1481\n",
            "k-fold 2  Train Loss: 0.0975, Validation Loss: 0.1482\n",
            "k-fold 2  Train Loss: 0.0974, Validation Loss: 0.1480\n",
            "k-fold 2  Train Loss: 0.0972, Validation Loss: 0.1476\n",
            "k-fold 2  Train Loss: 0.0969, Validation Loss: 0.1470\n",
            "k-fold 2  Train Loss: 0.0963, Validation Loss: 0.1458\n",
            "k-fold 2  Train Loss: 0.0955, Validation Loss: 0.1443\n",
            "k-fold 2  Train Loss: 0.0954, Validation Loss: 0.1439\n",
            "k-fold 2  Train Loss: 0.0950, Validation Loss: 0.1430\n",
            "k-fold 2  Train Loss: 0.0954, Validation Loss: 0.1433\n",
            "k-fold 2  Train Loss: 0.0957, Validation Loss: 0.1431\n",
            "k-fold 2  Train Loss: 0.0960, Validation Loss: 0.1432\n",
            "k-fold 2  Train Loss: 0.0961, Validation Loss: 0.1430\n",
            "k-fold 2  Train Loss: 0.0959, Validation Loss: 0.1422\n",
            "k-fold 2  Train Loss: 0.0954, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0953, Validation Loss: 0.1408\n",
            "k-fold 2  Train Loss: 0.0951, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0946, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0943, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0940, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0938, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0937, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0936, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0938, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0938, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0940, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0943, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0943, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0938, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0935, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1404\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0937, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.0942, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0947, Validation Loss: 0.1445\n",
            "k-fold 2  Train Loss: 0.0957, Validation Loss: 0.1458\n",
            "k-fold 2  Train Loss: 0.0963, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.0972, Validation Loss: 0.1479\n",
            "k-fold 2  Train Loss: 0.0983, Validation Loss: 0.1492\n",
            "k-fold 2  Train Loss: 0.0992, Validation Loss: 0.1503\n",
            "k-fold 2  Train Loss: 0.0989, Validation Loss: 0.1501\n",
            "k-fold 2  Train Loss: 0.0988, Validation Loss: 0.1498\n",
            "k-fold 2  Train Loss: 0.0985, Validation Loss: 0.1495\n",
            "k-fold 2  Train Loss: 0.0976, Validation Loss: 0.1484\n",
            "k-fold 2  Train Loss: 0.0973, Validation Loss: 0.1478\n",
            "k-fold 2  Train Loss: 0.0972, Validation Loss: 0.1475\n",
            "k-fold 2  Train Loss: 0.0966, Validation Loss: 0.1465\n",
            "k-fold 2  Train Loss: 0.0958, Validation Loss: 0.1453\n",
            "k-fold 2  Train Loss: 0.0949, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0941, Validation Loss: 0.1423\n",
            "k-fold 2  Train Loss: 0.0937, Validation Loss: 0.1414\n",
            "k-fold 2  Train Loss: 0.0934, Validation Loss: 0.1403\n",
            "k-fold 2  Train Loss: 0.0933, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0931, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0930, Validation Loss: 0.1375\n",
            "k-fold 2  Train Loss: 0.0928, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0925, Validation Loss: 0.1375\n",
            "k-fold 2  Train Loss: 0.0922, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0919, Validation Loss: 0.1381\n",
            "k-fold 2  Train Loss: 0.0918, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0918, Validation Loss: 0.1395\n",
            "k-fold 2  Train Loss: 0.0924, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0935, Validation Loss: 0.1428\n",
            "k-fold 2  Train Loss: 0.0950, Validation Loss: 0.1445\n",
            "k-fold 2  Train Loss: 0.0959, Validation Loss: 0.1454\n",
            "k-fold 2  Train Loss: 0.0966, Validation Loss: 0.1464\n",
            "k-fold 2  Train Loss: 0.0968, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.0976, Validation Loss: 0.1476\n",
            "k-fold 2  Train Loss: 0.0978, Validation Loss: 0.1477\n",
            "k-fold 2  Train Loss: 0.0978, Validation Loss: 0.1475\n",
            "k-fold 2  Train Loss: 0.0978, Validation Loss: 0.1475\n",
            "k-fold 2  Train Loss: 0.0978, Validation Loss: 0.1472\n",
            "k-fold 2  Train Loss: 0.0971, Validation Loss: 0.1465\n",
            "k-fold 2  Train Loss: 0.0951, Validation Loss: 0.1441\n",
            "k-fold 2  Train Loss: 0.0935, Validation Loss: 0.1418\n",
            "k-fold 2  Train Loss: 0.0923, Validation Loss: 0.1397\n",
            "k-fold 2  Train Loss: 0.0918, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0923, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0928, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0935, Validation Loss: 0.1413\n",
            "k-fold 2  Train Loss: 0.0944, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.0957, Validation Loss: 0.1442\n",
            "k-fold 2  Train Loss: 0.0970, Validation Loss: 0.1456\n",
            "k-fold 2  Train Loss: 0.0982, Validation Loss: 0.1468\n",
            "k-fold 2  Train Loss: 0.0994, Validation Loss: 0.1479\n",
            "k-fold 2  Train Loss: 0.0976, Validation Loss: 0.1464\n",
            "k-fold 2  Train Loss: 0.0955, Validation Loss: 0.1445\n",
            "k-fold 2  Train Loss: 0.0937, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.0926, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0917, Validation Loss: 0.1399\n",
            "k-fold 2  Train Loss: 0.0912, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0909, Validation Loss: 0.1379\n",
            "k-fold 2  Train Loss: 0.0908, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0906, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0903, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0901, Validation Loss: 0.1383\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0901, Validation Loss: 0.1400\n",
            "k-fold 2  Train Loss: 0.0902, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0904, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0908, Validation Loss: 0.1406\n",
            "k-fold 2  Train Loss: 0.0913, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0914, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0916, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0920, Validation Loss: 0.1413\n",
            "k-fold 2  Train Loss: 0.0922, Validation Loss: 0.1415\n",
            "k-fold 2  Train Loss: 0.0924, Validation Loss: 0.1416\n",
            "k-fold 2  Train Loss: 0.0928, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0930, Validation Loss: 0.1422\n",
            "k-fold 2  Train Loss: 0.0927, Validation Loss: 0.1416\n",
            "k-fold 2  Train Loss: 0.0921, Validation Loss: 0.1406\n",
            "k-fold 2  Train Loss: 0.0913, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0910, Validation Loss: 0.1387\n",
            "k-fold 2  Train Loss: 0.0909, Validation Loss: 0.1381\n",
            "k-fold 2  Train Loss: 0.0907, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0908, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0910, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0910, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0911, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0911, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0910, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0909, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0908, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0909, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0913, Validation Loss: 0.1406\n",
            "k-fold 2  Train Loss: 0.0918, Validation Loss: 0.1417\n",
            "k-fold 2  Train Loss: 0.0911, Validation Loss: 0.1408\n",
            "k-fold 2  Train Loss: 0.0906, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0903, Validation Loss: 0.1399\n",
            "k-fold 2  Train Loss: 0.0900, Validation Loss: 0.1395\n",
            "k-fold 2  Train Loss: 0.0905, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0912, Validation Loss: 0.1424\n",
            "k-fold 2  Train Loss: 0.0921, Validation Loss: 0.1435\n",
            "k-fold 2  Train Loss: 0.0930, Validation Loss: 0.1447\n",
            "k-fold 2  Train Loss: 0.0923, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0917, Validation Loss: 0.1430\n",
            "k-fold 2  Train Loss: 0.0906, Validation Loss: 0.1416\n",
            "k-fold 2  Train Loss: 0.0897, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0891, Validation Loss: 0.1390\n",
            "k-fold 2  Train Loss: 0.0885, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0883, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0882, Validation Loss: 0.1364\n",
            "k-fold 2  Train Loss: 0.0882, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0888, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0897, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0906, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0913, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0914, Validation Loss: 0.1321\n",
            "k-fold 2  Train Loss: 0.0911, Validation Loss: 0.1318\n",
            "k-fold 2  Train Loss: 0.0903, Validation Loss: 0.1318\n",
            "k-fold 2  Train Loss: 0.0894, Validation Loss: 0.1319\n",
            "k-fold 2  Train Loss: 0.0885, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0879, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0875, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0875, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0879, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0887, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0895, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0905, Validation Loss: 0.1404\n",
            "k-fold 2  Train Loss: 0.0918, Validation Loss: 0.1417\n",
            "k-fold 2  Train Loss: 0.0929, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.0933, Validation Loss: 0.1425\n",
            "k-fold 2  Train Loss: 0.0934, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0936, Validation Loss: 0.1416\n",
            "k-fold 2  Train Loss: 0.0932, Validation Loss: 0.1408\n",
            "k-fold 2  Train Loss: 0.0926, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0916, Validation Loss: 0.1383\n",
            "k-fold 2  Train Loss: 0.0913, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0909, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0907, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0901, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0902, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0903, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0894, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0885, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0879, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0878, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0875, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0874, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0875, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0877, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0880, Validation Loss: 0.1390\n",
            "k-fold 2  Train Loss: 0.0883, Validation Loss: 0.1403\n",
            "k-fold 2  Train Loss: 0.0887, Validation Loss: 0.1414\n",
            "k-fold 2  Train Loss: 0.0894, Validation Loss: 0.1427\n",
            "k-fold 2  Train Loss: 0.0887, Validation Loss: 0.1422\n",
            "k-fold 2  Train Loss: 0.0880, Validation Loss: 0.1413\n",
            "k-fold 2  Train Loss: 0.0874, Validation Loss: 0.1405\n",
            "k-fold 2  Train Loss: 0.0867, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0862, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0860, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0861, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0863, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0870, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0878, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0885, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0882, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0880, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0877, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0872, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0869, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0871, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0885, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0904, Validation Loss: 0.1419\n",
            "k-fold 2  Train Loss: 0.0927, Validation Loss: 0.1446\n",
            "k-fold 2  Train Loss: 0.0937, Validation Loss: 0.1454\n",
            "k-fold 2  Train Loss: 0.0942, Validation Loss: 0.1457\n",
            "k-fold 2  Train Loss: 0.0944, Validation Loss: 0.1457\n",
            "k-fold 2  Train Loss: 0.0941, Validation Loss: 0.1453\n",
            "k-fold 2  Train Loss: 0.0937, Validation Loss: 0.1442\n",
            "k-fold 2  Train Loss: 0.0930, Validation Loss: 0.1429\n",
            "k-fold 2  Train Loss: 0.0925, Validation Loss: 0.1418\n",
            "k-fold 2  Train Loss: 0.0921, Validation Loss: 0.1409\n",
            "k-fold 2  Train Loss: 0.0917, Validation Loss: 0.1399\n",
            "k-fold 2  Train Loss: 0.0914, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0912, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0907, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0907, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0906, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0904, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0902, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0902, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0901, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0900, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0900, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0897, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0895, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0896, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0898, Validation Loss: 0.1390\n",
            "k-fold 2  Train Loss: 0.0898, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0898, Validation Loss: 0.1397\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1401\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1405\n",
            "k-fold 2  Train Loss: 0.0899, Validation Loss: 0.1407\n",
            "k-fold 2  Train Loss: 0.0894, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0891, Validation Loss: 0.1400\n",
            "k-fold 2  Train Loss: 0.0887, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0882, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0876, Validation Loss: 0.1385\n",
            "k-fold 2  Train Loss: 0.0869, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0861, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0853, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0851, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0848, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0850, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0854, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0862, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0870, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0876, Validation Loss: 0.1323\n",
            "k-fold 2  Train Loss: 0.0876, Validation Loss: 0.1322\n",
            "k-fold 2  Train Loss: 0.0865, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0857, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0853, Validation Loss: 0.1328\n",
            "k-fold 2  Train Loss: 0.0846, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0844, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0846, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1364\n",
            "k-fold 2  Train Loss: 0.0855, Validation Loss: 0.1375\n",
            "k-fold 2  Train Loss: 0.0864, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0872, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0877, Validation Loss: 0.1404\n",
            "k-fold 2  Train Loss: 0.0885, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0893, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0895, Validation Loss: 0.1423\n",
            "k-fold 2  Train Loss: 0.0894, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0883, Validation Loss: 0.1409\n",
            "k-fold 2  Train Loss: 0.0873, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0862, Validation Loss: 0.1383\n",
            "k-fold 2  Train Loss: 0.0857, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0847, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0844, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0846, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0853, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0854, Validation Loss: 0.1312\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1315\n",
            "k-fold 2  Train Loss: 0.0845, Validation Loss: 0.1321\n",
            "k-fold 2  Train Loss: 0.0841, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0839, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0845, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0853, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0860, Validation Loss: 0.1381\n",
            "k-fold 2  Train Loss: 0.0868, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0868, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0868, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0859, Validation Loss: 0.1375\n",
            "k-fold 2  Train Loss: 0.0855, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0854, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0855, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0855, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0857, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0859, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0861, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0863, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0865, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0866, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0865, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0864, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0862, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0859, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0855, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0853, Validation Loss: 0.1320\n",
            "k-fold 2  Train Loss: 0.0852, Validation Loss: 0.1314\n",
            "k-fold 2  Train Loss: 0.0851, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0848, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1314\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1316\n",
            "k-fold 2  Train Loss: 0.0838, Validation Loss: 0.1318\n",
            "k-fold 2  Train Loss: 0.0836, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0835, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0836, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0839, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0839, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0838, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0837, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0836, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0830, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0835, Validation Loss: 0.1312\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1303\n",
            "k-fold 2  Train Loss: 0.0848, Validation Loss: 0.1299\n",
            "k-fold 2  Train Loss: 0.0858, Validation Loss: 0.1294\n",
            "k-fold 2  Train Loss: 0.0864, Validation Loss: 0.1293\n",
            "k-fold 2  Train Loss: 0.0863, Validation Loss: 0.1293\n",
            "k-fold 2  Train Loss: 0.0863, Validation Loss: 0.1294\n",
            "k-fold 2  Train Loss: 0.0871, Validation Loss: 0.1293\n",
            "k-fold 2  Train Loss: 0.0876, Validation Loss: 0.1292\n",
            "k-fold 2  Train Loss: 0.0873, Validation Loss: 0.1293\n",
            "k-fold 2  Train Loss: 0.0862, Validation Loss: 0.1298\n",
            "k-fold 2  Train Loss: 0.0858, Validation Loss: 0.1300\n",
            "k-fold 2  Train Loss: 0.0851, Validation Loss: 0.1305\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0837, Validation Loss: 0.1322\n",
            "k-fold 2  Train Loss: 0.0833, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0834, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1396\n",
            "k-fold 2  Train Loss: 0.0851, Validation Loss: 0.1399\n",
            "k-fold 2  Train Loss: 0.0850, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0849, Validation Loss: 0.1396\n",
            "k-fold 2  Train Loss: 0.0846, Validation Loss: 0.1391\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0837, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0833, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0829, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0827, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0828, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0828, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0830, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0834, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0836, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0838, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0841, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0841, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1344\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0844, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0841, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0837, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0839, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0834, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0828, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0822, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0817, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0815, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0814, Validation Loss: 0.1318\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1317\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1315\n",
            "k-fold 2  Train Loss: 0.0811, Validation Loss: 0.1322\n",
            "k-fold 2  Train Loss: 0.0812, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0815, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0821, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0826, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0831, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0839, Validation Loss: 0.1364\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0843, Validation Loss: 0.1364\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0844, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0844, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0845, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0845, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0847, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0847, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0847, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0845, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0842, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0836, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0830, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0826, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0825, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0825, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0826, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0829, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0834, Validation Loss: 0.1399\n",
            "k-fold 2  Train Loss: 0.0836, Validation Loss: 0.1409\n",
            "k-fold 2  Train Loss: 0.0841, Validation Loss: 0.1419\n",
            "k-fold 2  Train Loss: 0.0844, Validation Loss: 0.1426\n",
            "k-fold 2  Train Loss: 0.0848, Validation Loss: 0.1433\n",
            "k-fold 2  Train Loss: 0.0847, Validation Loss: 0.1435\n",
            "k-fold 2  Train Loss: 0.0847, Validation Loss: 0.1438\n",
            "k-fold 2  Train Loss: 0.0841, Validation Loss: 0.1434\n",
            "k-fold 2  Train Loss: 0.0833, Validation Loss: 0.1423\n",
            "k-fold 2  Train Loss: 0.0825, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0820, Validation Loss: 0.1400\n",
            "k-fold 2  Train Loss: 0.0819, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0810, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0807, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0804, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0801, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0800, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0801, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0802, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0803, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0804, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0806, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0809, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0814, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0817, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0820, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0828, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0834, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0839, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0840, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0835, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0830, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0824, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0819, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0816, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0815, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0814, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0813, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0811, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0811, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0811, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0808, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0808, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0807, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0808, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0810, Validation Loss: 0.1325\n",
            "k-fold 2  Train Loss: 0.0814, Validation Loss: 0.1322\n",
            "k-fold 2  Train Loss: 0.0819, Validation Loss: 0.1321\n",
            "k-fold 2  Train Loss: 0.0824, Validation Loss: 0.1322\n",
            "k-fold 2  Train Loss: 0.0827, Validation Loss: 0.1323\n",
            "k-fold 2  Train Loss: 0.0829, Validation Loss: 0.1325\n",
            "k-fold 2  Train Loss: 0.0829, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0828, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0825, Validation Loss: 0.1323\n",
            "k-fold 2  Train Loss: 0.0823, Validation Loss: 0.1321\n",
            "k-fold 2  Train Loss: 0.0819, Validation Loss: 0.1322\n",
            "k-fold 2  Train Loss: 0.0814, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0811, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0808, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0802, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0797, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0794, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0792, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0792, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0788, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0785, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1323\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1320\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1319\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0782, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0785, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1323\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1315\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1309\n",
            "k-fold 2  Train Loss: 0.0782, Validation Loss: 0.1306\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1304\n",
            "k-fold 2  Train Loss: 0.0785, Validation Loss: 0.1298\n",
            "k-fold 2  Train Loss: 0.0786, Validation Loss: 0.1295\n",
            "k-fold 2  Train Loss: 0.0787, Validation Loss: 0.1295\n",
            "k-fold 2  Train Loss: 0.0787, Validation Loss: 0.1292\n",
            "k-fold 2  Train Loss: 0.0784, Validation Loss: 0.1296\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1306\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1317\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0786, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0788, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0789, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0791, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0790, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0789, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0789, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0792, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0794, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0797, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0794, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0790, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0788, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0785, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0785, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1344\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0777, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0775, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0774, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0771, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0771, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0774, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0776, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0775, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0774, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0773, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0771, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0771, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0771, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0773, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0775, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0775, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0777, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0777, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0782, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0777, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0774, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0773, Validation Loss: 0.1344\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0773, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0776, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0793, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0803, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0811, Validation Loss: 0.1364\n",
            "k-fold 2  Train Loss: 0.0816, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0824, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0824, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0828, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0822, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0812, Validation Loss: 0.1379\n",
            "k-fold 2  Train Loss: 0.0805, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0795, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0764, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0761, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0763, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1307\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1308\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1308\n",
            "k-fold 2  Train Loss: 0.0775, Validation Loss: 0.1304\n",
            "k-fold 2  Train Loss: 0.0782, Validation Loss: 0.1301\n",
            "k-fold 2  Train Loss: 0.0783, Validation Loss: 0.1300\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1304\n",
            "k-fold 2  Train Loss: 0.0758, Validation Loss: 0.1319\n",
            "k-fold 2  Train Loss: 0.0750, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0755, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1394\n",
            "k-fold 2  Train Loss: 0.0789, Validation Loss: 0.1403\n",
            "k-fold 2  Train Loss: 0.0800, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0805, Validation Loss: 0.1413\n",
            "k-fold 2  Train Loss: 0.0806, Validation Loss: 0.1412\n",
            "k-fold 2  Train Loss: 0.0802, Validation Loss: 0.1407\n",
            "k-fold 2  Train Loss: 0.0795, Validation Loss: 0.1396\n",
            "k-fold 2  Train Loss: 0.0791, Validation Loss: 0.1390\n",
            "k-fold 2  Train Loss: 0.0787, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0777, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0778, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0781, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0782, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0780, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0774, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0766, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0762, Validation Loss: 0.1318\n",
            "k-fold 2  Train Loss: 0.0760, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0758, Validation Loss: 0.1308\n",
            "k-fold 2  Train Loss: 0.0756, Validation Loss: 0.1306\n",
            "k-fold 2  Train Loss: 0.0756, Validation Loss: 0.1304\n",
            "k-fold 2  Train Loss: 0.0757, Validation Loss: 0.1301\n",
            "k-fold 2  Train Loss: 0.0757, Validation Loss: 0.1302\n",
            "k-fold 2  Train Loss: 0.0756, Validation Loss: 0.1302\n",
            "k-fold 2  Train Loss: 0.0755, Validation Loss: 0.1306\n",
            "k-fold 2  Train Loss: 0.0757, Validation Loss: 0.1313\n",
            "k-fold 2  Train Loss: 0.0760, Validation Loss: 0.1319\n",
            "k-fold 2  Train Loss: 0.0764, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0767, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0767, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0764, Validation Loss: 0.1334\n",
            "k-fold 2  Train Loss: 0.0763, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0760, Validation Loss: 0.1335\n",
            "k-fold 2  Train Loss: 0.0758, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0760, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0763, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0765, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0771, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0764, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0773, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0774, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0776, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0786, Validation Loss: 0.1381\n",
            "k-fold 2  Train Loss: 0.0793, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0799, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0796, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0793, Validation Loss: 0.1379\n",
            "k-fold 2  Train Loss: 0.0787, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0782, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0777, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0761, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0752, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0750, Validation Loss: 0.1330\n",
            "k-fold 2  Train Loss: 0.0748, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0749, Validation Loss: 0.1327\n",
            "k-fold 2  Train Loss: 0.0747, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0740, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0732, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0729, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0736, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0747, Validation Loss: 0.1387\n",
            "k-fold 2  Train Loss: 0.0762, Validation Loss: 0.1401\n",
            "k-fold 2  Train Loss: 0.0776, Validation Loss: 0.1411\n",
            "k-fold 2  Train Loss: 0.0785, Validation Loss: 0.1415\n",
            "k-fold 2  Train Loss: 0.0796, Validation Loss: 0.1420\n",
            "k-fold 2  Train Loss: 0.0794, Validation Loss: 0.1415\n",
            "k-fold 2  Train Loss: 0.0793, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0786, Validation Loss: 0.1401\n",
            "k-fold 2  Train Loss: 0.0779, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0775, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0772, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0770, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0759, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0750, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0742, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0736, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0733, Validation Loss: 0.1344\n",
            "k-fold 2  Train Loss: 0.0730, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0728, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0726, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0725, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0727, Validation Loss: 0.1363\n",
            "k-fold 2  Train Loss: 0.0734, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0740, Validation Loss: 0.1384\n",
            "k-fold 2  Train Loss: 0.0738, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0739, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0738, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0736, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0741, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0743, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0747, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0751, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0755, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0758, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0760, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0759, Validation Loss: 0.1379\n",
            "k-fold 2  Train Loss: 0.0764, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1385\n",
            "k-fold 2  Train Loss: 0.0767, Validation Loss: 0.1385\n",
            "k-fold 2  Train Loss: 0.0760, Validation Loss: 0.1381\n",
            "k-fold 2  Train Loss: 0.0754, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0747, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0743, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0741, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0729, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0727, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0727, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0727, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0726, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0717, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0718, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0720, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0721, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0721, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0720, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0717, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0716, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0715, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0710, Validation Loss: 0.1364\n",
            "k-fold 2  Train Loss: 0.0709, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0710, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0711, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0710, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0709, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0707, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0706, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0706, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0706, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0709, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0709, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0718, Validation Loss: 0.1344\n",
            "k-fold 2  Train Loss: 0.0732, Validation Loss: 0.1339\n",
            "k-fold 2  Train Loss: 0.0744, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0754, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0762, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0768, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0769, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0767, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0762, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0756, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0749, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0742, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0735, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0728, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0734, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0745, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0751, Validation Loss: 0.1383\n",
            "k-fold 2  Train Loss: 0.0751, Validation Loss: 0.1382\n",
            "k-fold 2  Train Loss: 0.0753, Validation Loss: 0.1383\n",
            "k-fold 2  Train Loss: 0.0750, Validation Loss: 0.1379\n",
            "k-fold 2  Train Loss: 0.0747, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0740, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0711, Validation Loss: 0.1332\n",
            "k-fold 2  Train Loss: 0.0711, Validation Loss: 0.1328\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1324\n",
            "k-fold 2  Train Loss: 0.0717, Validation Loss: 0.1321\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1320\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1319\n",
            "k-fold 2  Train Loss: 0.0714, Validation Loss: 0.1321\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0704, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0704, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0714, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0722, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0732, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0741, Validation Loss: 0.1398\n",
            "k-fold 2  Train Loss: 0.0741, Validation Loss: 0.1400\n",
            "k-fold 2  Train Loss: 0.0738, Validation Loss: 0.1397\n",
            "k-fold 2  Train Loss: 0.0733, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0730, Validation Loss: 0.1390\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1380\n",
            "k-fold 2  Train Loss: 0.0716, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0707, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0704, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0702, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0702, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0704, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0707, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0709, Validation Loss: 0.1349\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1354\n",
            "k-fold 2  Train Loss: 0.0716, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0718, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0718, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0716, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0714, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0715, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0714, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0715, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0717, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0715, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1371\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0715, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0716, Validation Loss: 0.1367\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1369\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1374\n",
            "k-fold 2  Train Loss: 0.0724, Validation Loss: 0.1375\n",
            "k-fold 2  Train Loss: 0.0725, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0724, Validation Loss: 0.1375\n",
            "k-fold 2  Train Loss: 0.0725, Validation Loss: 0.1376\n",
            "k-fold 2  Train Loss: 0.0728, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0727, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0724, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0716, Validation Loss: 0.1373\n",
            "k-fold 2  Train Loss: 0.0711, Validation Loss: 0.1368\n",
            "k-fold 2  Train Loss: 0.0706, Validation Loss: 0.1363\n",
            "k-fold 2  Train Loss: 0.0701, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0697, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0696, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0695, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0694, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0693, Validation Loss: 0.1345\n",
            "k-fold 2  Train Loss: 0.0693, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0693, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0696, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0698, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1366\n",
            "k-fold 2  Train Loss: 0.0701, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0695, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0690, Validation Loss: 0.1352\n",
            "k-fold 2  Train Loss: 0.0685, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0685, Validation Loss: 0.1337\n",
            "k-fold 2  Train Loss: 0.0690, Validation Loss: 0.1333\n",
            "k-fold 2  Train Loss: 0.0693, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0694, Validation Loss: 0.1331\n",
            "k-fold 2  Train Loss: 0.0696, Validation Loss: 0.1328\n",
            "k-fold 2  Train Loss: 0.0698, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0696, Validation Loss: 0.1326\n",
            "k-fold 2  Train Loss: 0.0688, Validation Loss: 0.1329\n",
            "k-fold 2  Train Loss: 0.0686, Validation Loss: 0.1336\n",
            "k-fold 2  Train Loss: 0.0684, Validation Loss: 0.1342\n",
            "k-fold 2  Train Loss: 0.0685, Validation Loss: 0.1348\n",
            "k-fold 2  Train Loss: 0.0689, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0697, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0709, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0723, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0736, Validation Loss: 0.1399\n",
            "k-fold 2  Train Loss: 0.0748, Validation Loss: 0.1405\n",
            "k-fold 2  Train Loss: 0.0757, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0758, Validation Loss: 0.1410\n",
            "k-fold 2  Train Loss: 0.0756, Validation Loss: 0.1407\n",
            "k-fold 2  Train Loss: 0.0751, Validation Loss: 0.1402\n",
            "k-fold 2  Train Loss: 0.0746, Validation Loss: 0.1397\n",
            "k-fold 2  Train Loss: 0.0743, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0736, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0728, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1370\n",
            "k-fold 2  Train Loss: 0.0711, Validation Loss: 0.1363\n",
            "k-fold 2  Train Loss: 0.0707, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0701, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0697, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0695, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0693, Validation Loss: 0.1356\n",
            "k-fold 2  Train Loss: 0.0690, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0686, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0681, Validation Loss: 0.1353\n",
            "k-fold 2  Train Loss: 0.0682, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0681, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0680, Validation Loss: 0.1358\n",
            "k-fold 2  Train Loss: 0.0680, Validation Loss: 0.1359\n",
            "k-fold 2  Train Loss: 0.0680, Validation Loss: 0.1360\n",
            "k-fold 2  Train Loss: 0.0677, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0675, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0674, Validation Loss: 0.1355\n",
            "k-fold 2  Train Loss: 0.0674, Validation Loss: 0.1350\n",
            "k-fold 2  Train Loss: 0.0673, Validation Loss: 0.1346\n",
            "k-fold 2  Train Loss: 0.0672, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0671, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0677, Validation Loss: 0.1338\n",
            "k-fold 2  Train Loss: 0.0686, Validation Loss: 0.1340\n",
            "k-fold 2  Train Loss: 0.0691, Validation Loss: 0.1341\n",
            "k-fold 2  Train Loss: 0.0695, Validation Loss: 0.1343\n",
            "k-fold 2  Train Loss: 0.0701, Validation Loss: 0.1347\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1351\n",
            "k-fold 2  Train Loss: 0.0711, Validation Loss: 0.1357\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0710, Validation Loss: 0.1361\n",
            "k-fold 2  Train Loss: 0.0705, Validation Loss: 0.1362\n",
            "k-fold 2  Train Loss: 0.0703, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0699, Validation Loss: 0.1365\n",
            "k-fold 2  Train Loss: 0.0704, Validation Loss: 0.1372\n",
            "k-fold 2  Train Loss: 0.0708, Validation Loss: 0.1378\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1383\n",
            "k-fold 2  Train Loss: 0.0719, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0722, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0722, Validation Loss: 0.1393\n",
            "k-fold 2  Train Loss: 0.0720, Validation Loss: 0.1392\n",
            "k-fold 2  Train Loss: 0.0713, Validation Loss: 0.1389\n",
            "k-fold 2  Train Loss: 0.0712, Validation Loss: 0.1388\n",
            "k-fold 2  Train Loss: 0.0710, Validation Loss: 0.1386\n",
            "k-fold 2  Train Loss: 0.0704, Validation Loss: 0.1381\n",
            "k-fold 2  Train Loss: 0.0698, Validation Loss: 0.1377\n",
            "k-fold 2  Train Loss: 0.0694, Validation Loss: 0.1374\n",
            "Validation Score: 0.1381, ± 0.0275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hqEzQ37pi01l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}